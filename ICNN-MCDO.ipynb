{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.utils import get_custom_objects\n",
    "from keras.layers import Layer, Input, Conv2D, Conv3D, Reshape, BatchNormalization, Activation, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "import statsmodels.api as sm\n",
    "from math import sqrt\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5498903038364008141\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6966896648802742306\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 10148974707430616801\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:1\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 630268649388154450\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:2\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 644755265186958750\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:3\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 10018984676128151756\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def division(data):\n",
    "    '''\n",
    "    train:validation:test=6:2:2 데이터 분할\n",
    "    '''\n",
    "    train_size = int(len(data)*0.6)\n",
    "    val_size = int(len(data)*0.8)\n",
    "    \n",
    "    data_train = data[0:train_size]\n",
    "    data_val = data[train_size:val_size]\n",
    "    data_test = data[val_size:len(data)]\n",
    "    \n",
    "    return data_train, data_val, data_test\n",
    "\n",
    "\n",
    "def look_back(X, Y, a):\n",
    "    '''\n",
    "    시계열 분석을 위해 timestep과 lag를 적용하여 데이터셋을 생성\n",
    "    '''\n",
    "    X_lb = np.zeros((len(X) - a, a, 40, 28, 11))\n",
    "    for l in range(len(X) - a): \n",
    "        for r in range(a):\n",
    "            X_lb[l, r] = X[l + r, 0]\n",
    "    Y = Y[a:]\n",
    "    \n",
    "    return X_lb, Y\n",
    "\n",
    "\n",
    "def get_dropout(input_tensor, p=0.5, mc=False):\n",
    "    '''\n",
    "    몬테카를로 드롭아웃\n",
    "    기존 학습 과정에서만 적용되었던 드롭아웃을 테스트 과정에서도 적용되도록 한다.\n",
    "    '''\n",
    "    if mc:\n",
    "        return Dropout(p)(input_tensor, training=True)\n",
    "    else:\n",
    "        return Dropout(p)(input_tensor)\n",
    "\n",
    "    \n",
    "# 커스텀 객체 선언\n",
    "# 참고 : https://hwiyong.tistory.com/375\n",
    "def mish(x):\n",
    "    return x * K.tanh(K.softplus(x))\n",
    "\n",
    "get_custom_objects().update({'mish': mish})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICNN_MCDO(time_st):\n",
    "    '''\n",
    "    기존 ICNN모델에 Monte Carlo Dropout과 L2 regularization을 적용하여 베이지안 뉴럴 네트워크에 근사한 값을 출력한다.\n",
    "    또한 테스트 단계에서 Monte Carlo Sampling을 통해 예측값의 불확실성을 추정한다.\n",
    "    \n",
    "    \n",
    "    적용 순서\n",
    "    Convolution - Batch Normalization - Activation - Dropout - Pooling\n",
    "    \n",
    "    참고 : https://gaussian37.github.io/dl-concept-order_of_regularization_term/\n",
    "    '''\n",
    "    \n",
    "    input_tensor = Input(shape=(time_st, 40, 28, 11))\n",
    "    x = Conv3D(64, (time_st, 1, 1), padding='valid', kernel_regularizer=l2(0.01))(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = get_dropout(x, p=0.25, mc=True)\n",
    "    \n",
    "    x = Reshape((40, 28, 64))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = get_dropout(x, p=0.25, mc=True)\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), padding='same', kernel_regularizer=l2(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = get_dropout(x, p=0.25, mc=True)\n",
    "    \n",
    "    x = Conv2D(16, (3, 3), padding='same', kernel_regularizer=l2(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = get_dropout(x, p=0.25, mc=True)\n",
    "    \n",
    "    x = Conv2D(8, (3, 3), padding='same', kernel_regularizer=l2(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = get_dropout(x, p=0.25, mc=True)\n",
    "    \n",
    "    output_tensor = Conv2D(1, (3, 3), padding='same')(x)\n",
    "\n",
    "    model = Model(input_tensor, output_tensor)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICNN_Dropout(time_st):\n",
    "    input_tensor = Input(shape=(time_st, 40, 28, 11))\n",
    "    x = Conv3D(64, (time_st, 1, 1), padding='valid')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = get_dropout(x, p=0.25)\n",
    "    \n",
    "    x = Reshape((40, 28, 64))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = get_dropout(x, p=0.25)\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = get_dropout(x, p=0.25)\n",
    "    \n",
    "    x = Conv2D(16, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = get_dropout(x, p=0.25)\n",
    "    \n",
    "    x = Conv2D(8, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = get_dropout(x, p=0.25)\n",
    "    \n",
    "    output_tensor = Conv2D(1, (3, 3), padding='same')(x)\n",
    "\n",
    "    model = Model(input_tensor, output_tensor)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICNN(time_st):\n",
    "    input_tensor = Input(shape=(time_st, 40, 28, 11))\n",
    "    x = Conv3D(64, (time_st, 1, 1), padding='valid')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(mish)(x)\n",
    "    \n",
    "    x = Reshape((40, 28, 64))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(mish)(x)\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(mish)(x)\n",
    "    \n",
    "    x = Conv2D(16, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(mish)(x)\n",
    "    \n",
    "    x = Conv2D(8, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(mish)(x)\n",
    "    \n",
    "    output_tensor = Conv2D(1, (3, 3), padding='same')(x)\n",
    "\n",
    "    model = Model(input_tensor, output_tensor)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "디렉토리에서 데이터를 불러온후 Reshape를 통해 이미지 형식으로 변환\n",
    "'''\n",
    "\n",
    "path_dir = \"./IDWDATA_40_28size/\"\n",
    "file_list = ['CO_sizereset.csv','NO2_sizereset.csv', 'O3_sizereset.csv', 'PM10_sizereset.csv','PM25_sizereset.csv', 'SO2_sizereset.csv', 'temp_sizereset.csv', 'wind_x_sizereset.csv','wind_y_sizereset.csv','windspeed_sizereset.csv','rainfall_sizereset.csv']\n",
    "\n",
    "concat = []\n",
    "dataset = []\n",
    "flg = 0\n",
    "i = 0\n",
    "for f in file_list:\n",
    "    df = pd.read_csv(path_dir + f, engine='python')\n",
    "    del df['Unnamed: 0']\n",
    "\n",
    "    df_flat = df.values.flatten()\n",
    "    df_flat_rs = df_flat.reshape(len(df_flat), 1)\n",
    "\n",
    "    if (i == 0):\n",
    "        concat = df_flat_rs\n",
    "    else:\n",
    "        concat = np.concatenate((concat, df_flat_rs), axis=1)\n",
    "    i = i+1\n",
    "\n",
    "if(flg==0):\n",
    "    dataset = concat\n",
    "else:\n",
    "    dataset = np.concatenate((dataset, concat), axis=0)\n",
    "flg = flg+1\n",
    "\n",
    "# reshape here\n",
    "data = np.reshape(dataset, ((8760*2), 1, 40, 28, 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) ICNN-MCDO : PM10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Currnent time step:24, lag:24 ##########\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-468400874953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mY_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8760\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlook_back\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlag\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mX_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlag\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e017931083c3>\u001b[0m in \u001b[0;36mlook_back\u001b[0;34m(X, Y, a)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mX_lb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "for time_step in [24]:\n",
    "    for lag in [24]:\n",
    "        print('########## Currnent time step:{0}, lag:{1} ##########'.format(str(time_step), str(lag)))\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=50, mode='min')\n",
    "        mc = ModelCheckpoint('./Result/PM10/ICNN-MCDO/Model/timestep{0}_lag{1}.h5'.format(str(time_step), str(lag)), monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    " \n",
    "        \n",
    "        #model = ICNN_MCDO(time_step)\n",
    "        model = load_model('./Result/PM10/ICNN-MCDO/Model/timestep{0}_lag{1}.h5'.format(str(time_step), str(lag)))\n",
    "    \n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        model.fit(X_train, Y_train, epochs=300, batch_size=512, validation_data=(X_val, Y_val), callbacks=[early_stopping, mc])\n",
    "        \n",
    "        del model, X_data, Y_data, X_train, X_val, X_test, Y_train, Y_val, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3809f0f5d5427788ff6b52abe580b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b407315183642c9adf22da5e0c238e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8f0f5e704947b188358044f4ab0b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2fb740b1b94b109375d0f1cc34d9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "for time_step in tqdm([1, 12, 24]):\n",
    "    for lag in tqdm([1, 2, 4, 6, 12, 24]):\n",
    "        model_src = './Result/PM10/ICNN-MCDO/Model/timestep{0}_lag{1}.h5'.format(time_step, lag)\n",
    "        model = load_model(model_src, custom_objects={'mish':mish})\n",
    "        \n",
    "        X_data = data\n",
    "        Y_data = data[:,:,:,:,3] # 3 = PM10, 4 = PM2.5\n",
    "        Y_data = Y_data.reshape((8760*2),1,40,28,1)\n",
    "\n",
    "        X_data, Y_data = look_back(X_data, Y_data, time_step)\n",
    "        if lag > 1:\n",
    "            X_data = X_data[0:len(X_data)-(lag-1)]\n",
    "            Y_data = Y_data[(lag-1):]\n",
    "\n",
    "        X_train, X_val, X_test = division(X_data)\n",
    "        Y_train, Y_val, Y_test = division(Y_data)\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], 40, 28, 1).astype('float32')\n",
    "        Y_val = Y_val.reshape(Y_val.shape[0],  40, 28, 1).astype('float32')\n",
    "        Y_test = Y_test.reshape(Y_test.shape[0], 40, 28, 1).astype('float32')\n",
    "        \n",
    "        Y_pred_pm10 = np.zeros((100, Y_test.shape[0],40, 28))\n",
    "        \n",
    "        for i in range(100):\n",
    "            print(i)\n",
    "            tmp = model.predict(X_test)\n",
    "            tmp = tmp.reshape(-1, 40, 28)\n",
    "            Y_pred_pm10[i, :, :, :] = tmp\n",
    "        \n",
    "        np.save('./Result/PM10/ICNN-MCDO/Predict/timestep{0}_lag{1}'.format(time_step, lag), Y_pred_pm10)\n",
    "        \n",
    "        del model, X_data, Y_data, X_train, X_val, X_test, Y_train, Y_val, Y_test, Y_pred_pm10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) ICNN-Dropout : PM10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_step in [1, 12, 24]:\n",
    "    for lag in [1, 2, 4, 6, 12, 24]:\n",
    "        print('Currnent time step:{0}, lag:{1}'.format(str(time_step), str(lag)))\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=50, mode='min')\n",
    "        mc = ModelCheckpoint('./Result/PM10/ICNN-Dropout/Model/timestep{0}_lag{1}.h5'.format(str(time_step), str(lag)), monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "        X_data = data\n",
    "        Y_data = data[:,:,:,:,3] # 3 = PM10, 4 = PM2.5\n",
    "        Y_data = Y_data.reshape((8760*2),1,40,28,1)\n",
    "\n",
    "        X_data, Y_data = look_back(X_data, Y_data, time_step)\n",
    "        if lag > 1:\n",
    "            X_data = X_data[0:len(X_data)-(lag-1)]\n",
    "            Y_data = Y_data[(lag-1):]\n",
    "\n",
    "        X_train, X_val, X_test = division(X_data)\n",
    "        Y_train, Y_val, Y_test = division(Y_data)\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], 40, 28, 1).astype('float32')\n",
    "        Y_val = Y_val.reshape(Y_val.shape[0],  40, 28, 1).astype('float32')\n",
    "        Y_test = Y_test.reshape(Y_test.shape[0], 40, 28, 1).astype('float32')\n",
    "        \n",
    "        #model = ICNN_Dropout(time_step)\n",
    "        model = load_model('./Result/PM10/ICNN-Dropout/Model/timestep{0}_lag{1}.h5'.format(str(time_step), str(lag)))\n",
    "\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        model.fit(X_train, Y_train, epochs=300, batch_size=512, validation_data=(X_val, Y_val), callbacks=[early_stopping, mc])\n",
    "\n",
    "        del model, X_data, Y_data, X_train, X_val, X_test, Y_train, Y_val, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_step in [1, 12, 24]:\n",
    "    for lag in [1, 2, 4, 6, 12, 24]:\n",
    "        model_src = './Result/PM10/ICNN-Dropout/Model/timestep{0}_lag{1}.h5'.format(time_step, lag)\n",
    "        model = load_model(model_src, custom_objects={'mish':mish})\n",
    "        \n",
    "        X_data = data\n",
    "        Y_data = data[:,:,:,:,3] # 3 = PM10, 4 = PM2.5\n",
    "        Y_data = Y_data.reshape((8760*2),1,40,28,1)\n",
    "\n",
    "        X_data, Y_data = look_back(X_data, Y_data, time_step)\n",
    "        if lag > 1:\n",
    "            X_data = X_data[0:len(X_data)-(lag-1)]\n",
    "            Y_data = Y_data[(lag-1):]\n",
    "\n",
    "        X_train, X_val, X_test = division(X_data)\n",
    "        Y_train, Y_val, Y_test = division(Y_data)\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], 40, 28, 1).astype('float32')\n",
    "        Y_val = Y_val.reshape(Y_val.shape[0],  40, 28, 1).astype('float32')\n",
    "        Y_test = Y_test.reshape(Y_test.shape[0], 40, 28, 1).astype('float32')\n",
    "        \n",
    "        Y_pred_pm10 = model.predict(X_test).reshape(-1, 40, 28)\n",
    "        \n",
    "        np.save('./Result/PM10/ICNN-Dropout/Predict/timestep{0}_lag{1}'.format(time_step, lag), Y_pred_pm10)\n",
    "        \n",
    "        del model, X_data, Y_data, X_train, X_val, X_test, Y_train, Y_val, Y_test, Y_pred_pm10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) ICNN : PM10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currnent time step:24, lag:6\n",
      "WARNING:tensorflow:From /home/dsl001/anaconda3/envs/tf115/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/dsl001/anaconda3/envs/tf115/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 10494 samples, validate on 3498 samples\n",
      "Epoch 1/2000\n",
      "10494/10494 [==============================] - 91s 9ms/step - loss: 2578.0025 - val_loss: 558.4317\n",
      "Epoch 2/2000\n",
      "10494/10494 [==============================] - 91s 9ms/step - loss: 2493.8127 - val_loss: 917.8928\n",
      "Epoch 3/2000\n",
      "10494/10494 [==============================] - 93s 9ms/step - loss: 2429.2595 - val_loss: 1094.3077\n",
      "Epoch 4/2000\n",
      "10494/10494 [==============================] - 92s 9ms/step - loss: 2359.4482 - val_loss: 1183.8533\n",
      "Epoch 5/2000\n",
      "10494/10494 [==============================] - 93s 9ms/step - loss: 2286.4050 - val_loss: 1259.6314\n",
      "Epoch 6/2000\n",
      "10494/10494 [==============================] - 94s 9ms/step - loss: 2206.9321 - val_loss: 1361.2951\n",
      "Epoch 7/2000\n",
      "10494/10494 [==============================] - 94s 9ms/step - loss: 2123.6694 - val_loss: 1281.3749\n",
      "Epoch 8/2000\n",
      "10494/10494 [==============================] - 95s 9ms/step - loss: 2037.2761 - val_loss: 1452.0329\n",
      "Epoch 9/2000\n",
      "10494/10494 [==============================] - 93s 9ms/step - loss: 1946.6804 - val_loss: 1263.5842\n",
      "Epoch 10/2000\n",
      "10494/10494 [==============================] - 96s 9ms/step - loss: 1856.6003 - val_loss: 1224.6770\n",
      "Epoch 11/2000\n",
      "10494/10494 [==============================] - 94s 9ms/step - loss: 1758.6537 - val_loss: 1372.6909\n",
      "Epoch 12/2000\n",
      "10494/10494 [==============================] - 94s 9ms/step - loss: 1661.8656 - val_loss: 1442.6604\n",
      "Epoch 13/2000\n",
      "10494/10494 [==============================] - 93s 9ms/step - loss: 1562.9577 - val_loss: 1190.3620\n",
      "Epoch 14/2000\n",
      "10494/10494 [==============================] - 96s 9ms/step - loss: 1466.1683 - val_loss: 1113.3748\n",
      "Epoch 15/2000\n",
      "10494/10494 [==============================] - 94s 9ms/step - loss: 1364.5771 - val_loss: 1448.4672\n",
      "Epoch 16/2000\n",
      "10494/10494 [==============================] - 92s 9ms/step - loss: 1262.4517 - val_loss: 1295.2062\n",
      "Epoch 17/2000\n",
      "10494/10494 [==============================] - 93s 9ms/step - loss: 1156.4862 - val_loss: 981.8880\n",
      "Epoch 18/2000\n",
      "10494/10494 [==============================] - 93s 9ms/step - loss: 1049.0174 - val_loss: 1562.2379\n",
      "Epoch 19/2000\n",
      "10494/10494 [==============================] - 92s 9ms/step - loss: 943.5716 - val_loss: 898.6262\n",
      "Epoch 20/2000\n",
      "10494/10494 [==============================] - 94s 9ms/step - loss: 845.3148 - val_loss: 688.4767\n",
      "Epoch 21/2000\n",
      "10494/10494 [==============================] - 92s 9ms/step - loss: 756.4429 - val_loss: 794.5107\n",
      "Currnent time step:24, lag:12\n",
      "Train on 10491 samples, validate on 3497 samples\n",
      "Epoch 1/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 2722.2742 - val_loss: 1746.1171\n",
      "Epoch 2/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 2678.7431 - val_loss: 1770.5789\n",
      "Epoch 3/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 2635.0466 - val_loss: 1784.2858\n",
      "Epoch 4/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 2558.5708 - val_loss: 1669.1277\n",
      "Epoch 5/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 2463.7574 - val_loss: 1470.7952\n",
      "Epoch 6/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 2365.3053 - val_loss: 1510.8551\n",
      "Epoch 7/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 2260.6722 - val_loss: 1661.3118\n",
      "Epoch 8/2000\n",
      "10491/10491 [==============================] - 92s 9ms/step - loss: 2154.3418 - val_loss: 1517.4308\n",
      "Epoch 9/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 2041.0998 - val_loss: 1258.2984\n",
      "Epoch 10/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 1924.5112 - val_loss: 1496.6111\n",
      "Epoch 11/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 1809.3454 - val_loss: 1414.2808\n",
      "Epoch 12/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 1692.7594 - val_loss: 1281.2954\n",
      "Epoch 13/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 1570.9600 - val_loss: 1733.5153\n",
      "Epoch 14/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 1460.3075 - val_loss: 1337.8081\n",
      "Epoch 15/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 1345.2742 - val_loss: 1330.4166\n",
      "Epoch 16/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 1234.0161 - val_loss: 1101.8235\n",
      "Epoch 17/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 1127.4142 - val_loss: 964.8563\n",
      "Epoch 18/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 1030.1651 - val_loss: 995.6622\n",
      "Epoch 19/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 936.4541 - val_loss: 941.8805\n",
      "Epoch 20/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 851.1836 - val_loss: 784.4891\n",
      "Epoch 21/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 773.4562 - val_loss: 846.1215\n",
      "Epoch 22/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 696.9427 - val_loss: 768.1432\n",
      "Epoch 23/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 626.8961 - val_loss: 720.4938\n",
      "Epoch 24/2000\n",
      "10491/10491 [==============================] - 96s 9ms/step - loss: 574.0133 - val_loss: 594.9750\n",
      "Epoch 25/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 527.5679 - val_loss: 609.5351\n",
      "Epoch 26/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 484.8227 - val_loss: 597.8736\n",
      "Epoch 27/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 435.4775 - val_loss: 602.9615\n",
      "Epoch 28/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 391.8085 - val_loss: 499.5150\n",
      "Epoch 29/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 364.5467 - val_loss: 526.9907\n",
      "Epoch 30/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 340.7098 - val_loss: 381.9943\n",
      "Epoch 31/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 300.9103 - val_loss: 259.5802\n",
      "Epoch 32/2000\n",
      "10491/10491 [==============================] - 97s 9ms/step - loss: 277.1710 - val_loss: 265.8751\n",
      "Epoch 33/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 262.5827 - val_loss: 254.0087\n",
      "Epoch 34/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 238.7132 - val_loss: 268.6747\n",
      "Epoch 35/2000\n",
      "10491/10491 [==============================] - 92s 9ms/step - loss: 231.9284 - val_loss: 247.7974\n",
      "Epoch 36/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 214.7706 - val_loss: 247.2041\n",
      "Epoch 37/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 206.8025 - val_loss: 242.9595\n",
      "Epoch 38/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 196.9696 - val_loss: 256.5425\n",
      "Epoch 39/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 186.4398 - val_loss: 264.1469\n",
      "Epoch 40/2000\n",
      "10491/10491 [==============================] - 96s 9ms/step - loss: 183.3176 - val_loss: 234.6519\n",
      "Epoch 41/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 180.3017 - val_loss: 212.4526\n",
      "Epoch 42/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 179.6714 - val_loss: 221.3812\n",
      "Epoch 43/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 174.8698 - val_loss: 246.7280\n",
      "Epoch 44/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 169.0075 - val_loss: 203.2494\n",
      "Epoch 45/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 163.5507 - val_loss: 206.7541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/2000\n",
      "10491/10491 [==============================] - 92s 9ms/step - loss: 167.5642 - val_loss: 215.2747\n",
      "Epoch 47/2000\n",
      "10491/10491 [==============================] - 96s 9ms/step - loss: 164.9965 - val_loss: 223.7073\n",
      "Epoch 48/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 160.1261 - val_loss: 202.6296\n",
      "Epoch 49/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 154.3854 - val_loss: 203.5759\n",
      "Epoch 50/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 154.5538 - val_loss: 214.3516\n",
      "Epoch 51/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 153.4172 - val_loss: 212.2157\n",
      "Epoch 52/2000\n",
      "10491/10491 [==============================] - 92s 9ms/step - loss: 149.3138 - val_loss: 198.9901\n",
      "Epoch 53/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 148.3130 - val_loss: 200.6796\n",
      "Epoch 54/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 146.9849 - val_loss: 217.5220\n",
      "Epoch 55/2000\n",
      "10491/10491 [==============================] - 96s 9ms/step - loss: 149.0773 - val_loss: 203.7871\n",
      "Epoch 56/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 151.5554 - val_loss: 228.5034\n",
      "Epoch 57/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 148.9028 - val_loss: 215.2050\n",
      "Epoch 58/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 145.3597 - val_loss: 223.8776\n",
      "Epoch 59/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 146.6172 - val_loss: 221.4632\n",
      "Epoch 60/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 139.6844 - val_loss: 195.4338\n",
      "Epoch 61/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 138.0210 - val_loss: 209.6584\n",
      "Epoch 62/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 136.0556 - val_loss: 206.7228\n",
      "Epoch 63/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 141.8006 - val_loss: 203.4831\n",
      "Epoch 64/2000\n",
      "10491/10491 [==============================] - 92s 9ms/step - loss: 131.1734 - val_loss: 215.9846\n",
      "Epoch 65/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 132.3958 - val_loss: 222.9520\n",
      "Epoch 66/2000\n",
      "10491/10491 [==============================] - 97s 9ms/step - loss: 136.9399 - val_loss: 198.1071\n",
      "Epoch 67/2000\n",
      "10491/10491 [==============================] - 96s 9ms/step - loss: 130.6725 - val_loss: 214.2245\n",
      "Epoch 68/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 129.0773 - val_loss: 202.9262\n",
      "Epoch 69/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 136.1650 - val_loss: 231.4337\n",
      "Epoch 70/2000\n",
      "10491/10491 [==============================] - 96s 9ms/step - loss: 129.1834 - val_loss: 209.7050\n",
      "Epoch 71/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 128.5782 - val_loss: 210.8769\n",
      "Epoch 72/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 125.3804 - val_loss: 216.9603\n",
      "Epoch 73/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 127.5547 - val_loss: 209.5793\n",
      "Epoch 74/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 127.3122 - val_loss: 208.8493\n",
      "Epoch 75/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 133.1312 - val_loss: 234.6223\n",
      "Epoch 76/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 127.8492 - val_loss: 226.0915\n",
      "Epoch 77/2000\n",
      "10491/10491 [==============================] - 96s 9ms/step - loss: 122.2748 - val_loss: 210.3504\n",
      "Epoch 78/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 122.6305 - val_loss: 203.8537\n",
      "Epoch 79/2000\n",
      "10491/10491 [==============================] - 93s 9ms/step - loss: 123.4041 - val_loss: 213.8075\n",
      "Epoch 80/2000\n",
      "10491/10491 [==============================] - 94s 9ms/step - loss: 123.9646 - val_loss: 224.7746\n",
      "Currnent time step:24, lag:24\n",
      "Train on 10483 samples, validate on 3495 samples\n",
      "Epoch 1/2000\n",
      "10483/10483 [==============================] - 139s 13ms/step - loss: 2636.9729 - val_loss: 1579.0831\n",
      "Epoch 2/2000\n",
      "10483/10483 [==============================] - 137s 13ms/step - loss: 2564.6184 - val_loss: 1618.6483\n",
      "Epoch 3/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 2497.8392 - val_loss: 1679.9155\n",
      "Epoch 4/2000\n",
      "10483/10483 [==============================] - 135s 13ms/step - loss: 2423.9141 - val_loss: 1604.8216\n",
      "Epoch 5/2000\n",
      "10483/10483 [==============================] - 134s 13ms/step - loss: 2347.1020 - val_loss: 1685.4599\n",
      "Epoch 6/2000\n",
      "10483/10483 [==============================] - 133s 13ms/step - loss: 2269.5315 - val_loss: 1560.1648\n",
      "Epoch 7/2000\n",
      "10483/10483 [==============================] - 135s 13ms/step - loss: 2182.0538 - val_loss: 1858.5568\n",
      "Epoch 8/2000\n",
      "10483/10483 [==============================] - 134s 13ms/step - loss: 2093.7212 - val_loss: 1804.4625\n",
      "Epoch 9/2000\n",
      "10483/10483 [==============================] - 133s 13ms/step - loss: 2000.8746 - val_loss: 1781.0943\n",
      "Epoch 10/2000\n",
      "10483/10483 [==============================] - 132s 13ms/step - loss: 1902.8572 - val_loss: 1736.2727\n",
      "Epoch 11/2000\n",
      "10483/10483 [==============================] - 134s 13ms/step - loss: 1806.3577 - val_loss: 1848.1617\n",
      "Epoch 12/2000\n",
      "10483/10483 [==============================] - 134s 13ms/step - loss: 1701.6294 - val_loss: 1685.9266\n",
      "Epoch 13/2000\n",
      "10483/10483 [==============================] - 133s 13ms/step - loss: 1589.7245 - val_loss: 1490.2605\n",
      "Epoch 14/2000\n",
      "10483/10483 [==============================] - 134s 13ms/step - loss: 1465.0765 - val_loss: 1157.4284\n",
      "Epoch 15/2000\n",
      "10483/10483 [==============================] - 134s 13ms/step - loss: 1341.0899 - val_loss: 1548.9097\n",
      "Epoch 16/2000\n",
      "10483/10483 [==============================] - 134s 13ms/step - loss: 1232.2059 - val_loss: 1344.5068\n",
      "Epoch 17/2000\n",
      "10483/10483 [==============================] - 135s 13ms/step - loss: 1117.4715 - val_loss: 1337.2345\n",
      "Epoch 18/2000\n",
      "10483/10483 [==============================] - 135s 13ms/step - loss: 1006.4959 - val_loss: 1399.0547\n",
      "Epoch 19/2000\n",
      "10483/10483 [==============================] - 134s 13ms/step - loss: 909.9295 - val_loss: 1084.4139\n",
      "Epoch 20/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 835.8198 - val_loss: 832.8025\n",
      "Epoch 21/2000\n",
      "10483/10483 [==============================] - 135s 13ms/step - loss: 751.5744 - val_loss: 948.9508\n",
      "Epoch 22/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 695.5490 - val_loss: 667.5929\n",
      "Epoch 23/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 620.5988 - val_loss: 692.8912\n",
      "Epoch 24/2000\n",
      "10483/10483 [==============================] - 135s 13ms/step - loss: 543.0751 - val_loss: 544.3182\n",
      "Epoch 25/2000\n",
      "10483/10483 [==============================] - 138s 13ms/step - loss: 482.9070 - val_loss: 428.4949\n",
      "Epoch 26/2000\n",
      "10483/10483 [==============================] - 138s 13ms/step - loss: 441.1730 - val_loss: 374.1171\n",
      "Epoch 27/2000\n",
      "10483/10483 [==============================] - 139s 13ms/step - loss: 406.8594 - val_loss: 400.5164\n",
      "Epoch 28/2000\n",
      "10483/10483 [==============================] - 139s 13ms/step - loss: 376.4393 - val_loss: 442.1579\n",
      "Epoch 29/2000\n",
      "10483/10483 [==============================] - 138s 13ms/step - loss: 352.9166 - val_loss: 373.6504\n",
      "Epoch 30/2000\n",
      "10483/10483 [==============================] - 137s 13ms/step - loss: 328.1412 - val_loss: 337.3439\n",
      "Epoch 31/2000\n",
      "10483/10483 [==============================] - 137s 13ms/step - loss: 316.4379 - val_loss: 381.7642\n",
      "Epoch 32/2000\n",
      "10483/10483 [==============================] - 139s 13ms/step - loss: 310.7109 - val_loss: 339.8262\n",
      "Epoch 33/2000\n",
      "10483/10483 [==============================] - 138s 13ms/step - loss: 298.3189 - val_loss: 351.8615\n",
      "Epoch 34/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 283.1632 - val_loss: 342.7129\n",
      "Epoch 35/2000\n",
      "10483/10483 [==============================] - 138s 13ms/step - loss: 274.2867 - val_loss: 333.4426\n",
      "Epoch 36/2000\n",
      "10483/10483 [==============================] - 140s 13ms/step - loss: 265.9834 - val_loss: 333.3489\n",
      "Epoch 37/2000\n",
      "10483/10483 [==============================] - 139s 13ms/step - loss: 256.1977 - val_loss: 337.7703\n",
      "Epoch 38/2000\n",
      "10483/10483 [==============================] - 139s 13ms/step - loss: 264.6657 - val_loss: 327.6791\n",
      "Epoch 39/2000\n",
      "10483/10483 [==============================] - 137s 13ms/step - loss: 251.9603 - val_loss: 386.7888\n",
      "Epoch 40/2000\n",
      "10483/10483 [==============================] - 138s 13ms/step - loss: 246.3776 - val_loss: 316.6439\n",
      "Epoch 41/2000\n",
      "10483/10483 [==============================] - 137s 13ms/step - loss: 238.1653 - val_loss: 327.2039\n",
      "Epoch 42/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 228.6854 - val_loss: 327.9866\n",
      "Epoch 43/2000\n",
      "10483/10483 [==============================] - 137s 13ms/step - loss: 233.0690 - val_loss: 335.3284\n",
      "Epoch 44/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 236.1007 - val_loss: 313.1402\n",
      "Epoch 45/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 232.4523 - val_loss: 321.8047\n",
      "Epoch 46/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 219.7368 - val_loss: 321.4427\n",
      "Epoch 47/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 225.3498 - val_loss: 353.5165\n",
      "Epoch 48/2000\n",
      "10483/10483 [==============================] - 137s 13ms/step - loss: 213.9976 - val_loss: 329.3327\n",
      "Epoch 49/2000\n",
      "10483/10483 [==============================] - 137s 13ms/step - loss: 216.6399 - val_loss: 321.8799\n",
      "Epoch 50/2000\n",
      "10483/10483 [==============================] - 138s 13ms/step - loss: 214.6638 - val_loss: 304.6413\n",
      "Epoch 51/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 212.3156 - val_loss: 318.0979\n",
      "Epoch 52/2000\n",
      "10483/10483 [==============================] - 137s 13ms/step - loss: 214.8738 - val_loss: 321.7377\n",
      "Epoch 53/2000\n",
      "10483/10483 [==============================] - 137s 13ms/step - loss: 208.1380 - val_loss: 327.2706\n",
      "Epoch 54/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 197.1326 - val_loss: 345.5515\n",
      "Epoch 55/2000\n",
      "10483/10483 [==============================] - 135s 13ms/step - loss: 200.6216 - val_loss: 327.4232\n",
      "Epoch 56/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 194.8775 - val_loss: 362.1219\n",
      "Epoch 57/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 194.7452 - val_loss: 327.2728\n",
      "Epoch 58/2000\n",
      "10483/10483 [==============================] - 137s 13ms/step - loss: 191.1957 - val_loss: 319.7493\n",
      "Epoch 59/2000\n",
      "10483/10483 [==============================] - 137s 13ms/step - loss: 189.3968 - val_loss: 312.2438\n",
      "Epoch 60/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 184.8024 - val_loss: 324.1737\n",
      "Epoch 61/2000\n",
      "10483/10483 [==============================] - 135s 13ms/step - loss: 191.5280 - val_loss: 336.8725\n",
      "Epoch 62/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 184.1615 - val_loss: 325.4632\n",
      "Epoch 63/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 180.7934 - val_loss: 337.7621\n",
      "Epoch 64/2000\n",
      "10483/10483 [==============================] - 137s 13ms/step - loss: 177.9867 - val_loss: 330.7105\n",
      "Epoch 65/2000\n",
      "10483/10483 [==============================] - 137s 13ms/step - loss: 178.4024 - val_loss: 306.4975\n",
      "Epoch 66/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 175.2603 - val_loss: 338.1562\n",
      "Epoch 67/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 171.9103 - val_loss: 337.6858\n",
      "Epoch 68/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 174.6627 - val_loss: 349.7050\n",
      "Epoch 69/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 176.2760 - val_loss: 330.3348\n",
      "Epoch 70/2000\n",
      "10483/10483 [==============================] - 136s 13ms/step - loss: 167.5334 - val_loss: 334.3085\n"
     ]
    }
   ],
   "source": [
    "for time_step in [1, 12, 24]:\n",
    "    for lag in [1, 2, 4, 6, 12, 24]:\n",
    "        print('Currnent time step:{0}, lag:{1}'.format(str(time_step), str(lag)))\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, mode='min')\n",
    "        mc = ModelCheckpoint('./Result/PM10/ICNN/Model/timestep{0}_lag{1}.h5'.format(str(time_step), str(lag)), monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "        X_data = data\n",
    "        Y_data = data[:,:,:,:,3] # 3 = PM10, 4 = PM2.5\n",
    "        Y_data = Y_data.reshape((8760*2),1,40,28,1)\n",
    "\n",
    "        X_data, Y_data = look_back(X_data, Y_data, time_step)\n",
    "        if lag > 1:\n",
    "            X_data = X_data[0:len(X_data)-(lag-1)]\n",
    "            Y_data = Y_data[(lag-1):]\n",
    "\n",
    "        X_train, X_val, X_test = division(X_data)\n",
    "        Y_train, Y_val, Y_test = division(Y_data)\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], 40, 28, 1).astype('float32')\n",
    "        Y_val = Y_val.reshape(Y_val.shape[0],  40, 28, 1).astype('float32')\n",
    "        Y_test = Y_test.reshape(Y_test.shape[0], 40, 28, 1).astype('float32')\n",
    "        \n",
    "        model = ICNN(time_step)\n",
    "\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        model.fit(X_train, Y_train, epochs=2000, batch_size=512, validation_data=(X_val, Y_val), callbacks=[early_stopping, mc])\n",
    "\n",
    "        del model, X_data, Y_data, X_train, X_val, X_test, Y_train, Y_val, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_step in [1, 12, 24]:\n",
    "    for lag in [1, 2, 4, 6, 12, 24]:\n",
    "        model_src = './Result/PM10/ICNN/Model/timestep{0}_lag{1}.h5'.format(time_step, lag)\n",
    "        model = load_model(model_src, custom_objects={'mish':mish})\n",
    "        \n",
    "        X_data = data\n",
    "        Y_data = data[:,:,:,:,3] # 3 = PM10, 4 = PM2.5\n",
    "        Y_data = Y_data.reshape((8760*2),1,40,28,1)\n",
    "\n",
    "        X_data, Y_data = look_back(X_data, Y_data, time_step)\n",
    "        if lag > 1:\n",
    "            X_data = X_data[0:len(X_data)-(lag-1)]\n",
    "            Y_data = Y_data[(lag-1):]\n",
    "\n",
    "        X_train, X_val, X_test = division(X_data)\n",
    "        Y_train, Y_val, Y_test = division(Y_data)\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], 40, 28, 1).astype('float32')\n",
    "        Y_val = Y_val.reshape(Y_val.shape[0],  40, 28, 1).astype('float32')\n",
    "        Y_test = Y_test.reshape(Y_test.shape[0], 40, 28, 1).astype('float32')\n",
    "        \n",
    "        Y_pred_pm10 = model.predict(X_test).reshape(-1, 40, 28)\n",
    "        \n",
    "        np.save('./Result/PM10/ICNN/Predict/timestep{0}_lag{1}'.format(time_step, lag), Y_pred_pm10)\n",
    "        \n",
    "        del model, X_data, Y_data, X_train, X_val, X_test, Y_train, Y_val, Y_test, Y_pred_pm10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) ICNN-MCDO : PM2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currnent time step:1, lag:1\n",
      "Train on 10511 samples, validate on 3504 samples\n",
      "Epoch 1/2000\n",
      "10511/10511 [==============================] - 64s 6ms/step - loss: 861.5862 - val_loss: 1336.6135\n",
      "Epoch 2/2000\n",
      "10511/10511 [==============================] - 61s 6ms/step - loss: 814.1716 - val_loss: 490.4784\n",
      "Epoch 3/2000\n",
      "10511/10511 [==============================] - 61s 6ms/step - loss: 769.9417 - val_loss: 90.8664\n",
      "Epoch 4/2000\n",
      "10511/10511 [==============================] - 59s 6ms/step - loss: 715.3002 - val_loss: 94.4531\n",
      "Epoch 5/2000\n",
      "10511/10511 [==============================] - 62s 6ms/step - loss: 658.8618 - val_loss: 142.0728\n",
      "Epoch 6/2000\n",
      "10511/10511 [==============================] - 61s 6ms/step - loss: 604.0180 - val_loss: 187.3913\n",
      "Epoch 7/2000\n",
      "10511/10511 [==============================] - 61s 6ms/step - loss: 549.9644 - val_loss: 213.0637\n",
      "Epoch 8/2000\n",
      "10511/10511 [==============================] - 61s 6ms/step - loss: 497.9778 - val_loss: 210.4455\n",
      "Epoch 9/2000\n",
      "10511/10511 [==============================] - 61s 6ms/step - loss: 447.4605 - val_loss: 244.1383\n",
      "Epoch 10/2000\n",
      "10511/10511 [==============================] - 61s 6ms/step - loss: 398.4589 - val_loss: 238.1418\n",
      "Epoch 11/2000\n",
      "10511/10511 [==============================] - 61s 6ms/step - loss: 352.0876 - val_loss: 251.6987\n",
      "Epoch 12/2000\n",
      "10511/10511 [==============================] - 61s 6ms/step - loss: 308.6352 - val_loss: 237.6651\n",
      "Epoch 13/2000\n",
      "10511/10511 [==============================] - 60s 6ms/step - loss: 268.4834 - val_loss: 218.9032\n",
      "Epoch 14/2000\n",
      "10511/10511 [==============================] - 61s 6ms/step - loss: 231.3188 - val_loss: 232.7851\n",
      "Epoch 15/2000\n",
      "10511/10511 [==============================] - 61s 6ms/step - loss: 199.4474 - val_loss: 218.2562\n",
      "Epoch 16/2000\n",
      "10511/10511 [==============================] - 60s 6ms/step - loss: 169.9308 - val_loss: 215.4548\n",
      "Epoch 17/2000\n",
      "10511/10511 [==============================] - 61s 6ms/step - loss: 146.0812 - val_loss: 207.9095\n",
      "Epoch 18/2000\n",
      "10511/10511 [==============================] - 63s 6ms/step - loss: 124.3331 - val_loss: 184.7676\n",
      "Epoch 19/2000\n",
      "10511/10511 [==============================] - 61s 6ms/step - loss: 106.0075 - val_loss: 172.4753\n",
      "Epoch 20/2000\n",
      "10511/10511 [==============================] - 61s 6ms/step - loss: 90.3945 - val_loss: 164.7715\n",
      "Epoch 21/2000\n",
      "10511/10511 [==============================] - 61s 6ms/step - loss: 78.8731 - val_loss: 154.7866\n",
      "Epoch 22/2000\n",
      "10511/10511 [==============================] - 61s 6ms/step - loss: 69.2205 - val_loss: 126.2145\n",
      "Epoch 23/2000\n",
      "10511/10511 [==============================] - 60s 6ms/step - loss: 60.6316 - val_loss: 106.2403\n",
      "Currnent time step:1, lag:2\n",
      "Train on 10510 samples, validate on 3504 samples\n",
      "Epoch 1/2000\n",
      "10510/10510 [==============================] - 65s 6ms/step - loss: 880.2512 - val_loss: 169.8638\n",
      "Epoch 2/2000\n",
      "10510/10510 [==============================] - 61s 6ms/step - loss: 836.4966 - val_loss: 92.2173\n",
      "Epoch 3/2000\n",
      "10510/10510 [==============================] - 61s 6ms/step - loss: 800.5640 - val_loss: 107.8158\n",
      "Epoch 4/2000\n",
      "10510/10510 [==============================] - 61s 6ms/step - loss: 763.5010 - val_loss: 191.3463\n",
      "Epoch 5/2000\n",
      "10510/10510 [==============================] - 62s 6ms/step - loss: 724.4298 - val_loss: 257.3650\n",
      "Epoch 6/2000\n",
      "10510/10510 [==============================] - 62s 6ms/step - loss: 684.3455 - val_loss: 273.8843\n",
      "Epoch 7/2000\n",
      "10510/10510 [==============================] - 61s 6ms/step - loss: 641.6287 - val_loss: 294.5906\n",
      "Epoch 8/2000\n",
      "10510/10510 [==============================] - 62s 6ms/step - loss: 598.2435 - val_loss: 314.4419\n",
      "Epoch 9/2000\n",
      "10510/10510 [==============================] - 62s 6ms/step - loss: 553.2753 - val_loss: 293.4288\n",
      "Epoch 10/2000\n",
      "10510/10510 [==============================] - 61s 6ms/step - loss: 505.0147 - val_loss: 286.5256\n",
      "Epoch 11/2000\n",
      "10510/10510 [==============================] - 61s 6ms/step - loss: 457.4760 - val_loss: 274.1910\n",
      "Epoch 12/2000\n",
      "10510/10510 [==============================] - 62s 6ms/step - loss: 410.8645 - val_loss: 271.5039\n",
      "Epoch 13/2000\n",
      "10510/10510 [==============================] - 62s 6ms/step - loss: 366.4474 - val_loss: 261.0221\n",
      "Epoch 14/2000\n",
      "10510/10510 [==============================] - 61s 6ms/step - loss: 323.9083 - val_loss: 236.7471\n",
      "Epoch 15/2000\n",
      "10510/10510 [==============================] - 60s 6ms/step - loss: 284.6763 - val_loss: 219.9907\n",
      "Epoch 16/2000\n",
      "10510/10510 [==============================] - 61s 6ms/step - loss: 249.1082 - val_loss: 193.9079\n",
      "Epoch 17/2000\n",
      "10510/10510 [==============================] - 60s 6ms/step - loss: 216.5486 - val_loss: 176.6248\n",
      "Epoch 18/2000\n",
      "10510/10510 [==============================] - 60s 6ms/step - loss: 187.0717 - val_loss: 162.2462\n",
      "Epoch 19/2000\n",
      "10510/10510 [==============================] - 60s 6ms/step - loss: 161.3584 - val_loss: 138.3875\n",
      "Epoch 20/2000\n",
      "10510/10510 [==============================] - 61s 6ms/step - loss: 139.0326 - val_loss: 123.3090\n",
      "Epoch 21/2000\n",
      "10510/10510 [==============================] - 60s 6ms/step - loss: 119.1979 - val_loss: 119.2471\n",
      "Epoch 22/2000\n",
      "10510/10510 [==============================] - 61s 6ms/step - loss: 103.8001 - val_loss: 99.9184\n",
      "Currnent time step:1, lag:4\n",
      "Train on 10509 samples, validate on 3503 samples\n",
      "Epoch 1/2000\n",
      "10509/10509 [==============================] - 65s 6ms/step - loss: 872.3321 - val_loss: 336.0308\n",
      "Epoch 2/2000\n",
      "10509/10509 [==============================] - 61s 6ms/step - loss: 832.7469 - val_loss: 188.7787\n",
      "Epoch 3/2000\n",
      "10509/10509 [==============================] - 60s 6ms/step - loss: 792.6091 - val_loss: 105.7325\n",
      "Epoch 4/2000\n",
      "10509/10509 [==============================] - 60s 6ms/step - loss: 744.8733 - val_loss: 147.6805\n",
      "Epoch 5/2000\n",
      "10509/10509 [==============================] - 61s 6ms/step - loss: 670.5523 - val_loss: 211.0636\n",
      "Epoch 6/2000\n",
      "10509/10509 [==============================] - 61s 6ms/step - loss: 594.8289 - val_loss: 212.5500\n",
      "Epoch 7/2000\n",
      "10509/10509 [==============================] - 62s 6ms/step - loss: 527.8880 - val_loss: 191.6047\n",
      "Epoch 8/2000\n",
      "10509/10509 [==============================] - 61s 6ms/step - loss: 467.6412 - val_loss: 197.7274\n",
      "Epoch 9/2000\n",
      "10509/10509 [==============================] - 60s 6ms/step - loss: 412.3344 - val_loss: 216.3832\n",
      "Epoch 10/2000\n",
      "10509/10509 [==============================] - 60s 6ms/step - loss: 362.1027 - val_loss: 223.6877\n",
      "Epoch 11/2000\n",
      "10509/10509 [==============================] - 60s 6ms/step - loss: 317.6570 - val_loss: 194.7246\n",
      "Epoch 12/2000\n",
      "10509/10509 [==============================] - 60s 6ms/step - loss: 279.7090 - val_loss: 191.5584\n",
      "Epoch 13/2000\n",
      "10509/10509 [==============================] - 60s 6ms/step - loss: 244.8107 - val_loss: 179.0088\n",
      "Epoch 14/2000\n",
      "10509/10509 [==============================] - 60s 6ms/step - loss: 215.0505 - val_loss: 185.7976\n",
      "Epoch 15/2000\n",
      "10509/10509 [==============================] - 61s 6ms/step - loss: 190.2462 - val_loss: 185.3508\n",
      "Epoch 16/2000\n",
      "10509/10509 [==============================] - 60s 6ms/step - loss: 167.0751 - val_loss: 164.9784\n",
      "Epoch 17/2000\n",
      "10509/10509 [==============================] - 60s 6ms/step - loss: 149.9229 - val_loss: 160.2824\n",
      "Epoch 18/2000\n",
      "10509/10509 [==============================] - 60s 6ms/step - loss: 136.8129 - val_loss: 145.4505\n",
      "Epoch 19/2000\n",
      "10509/10509 [==============================] - 61s 6ms/step - loss: 123.4803 - val_loss: 135.5273\n",
      "Epoch 20/2000\n",
      "10509/10509 [==============================] - 60s 6ms/step - loss: 112.4883 - val_loss: 129.8616\n",
      "Epoch 21/2000\n",
      "10509/10509 [==============================] - 62s 6ms/step - loss: 104.1794 - val_loss: 120.5195\n",
      "Epoch 22/2000\n",
      "10509/10509 [==============================] - 61s 6ms/step - loss: 96.8603 - val_loss: 133.4910\n",
      "Epoch 23/2000\n",
      "10509/10509 [==============================] - 60s 6ms/step - loss: 91.9393 - val_loss: 131.9475\n",
      "Currnent time step:1, lag:6\n",
      "Train on 10508 samples, validate on 3503 samples\n",
      "Epoch 1/2000\n",
      "10508/10508 [==============================] - 64s 6ms/step - loss: 831.3608 - val_loss: 2624.3712\n",
      "Epoch 2/2000\n",
      "10508/10508 [==============================] - 59s 6ms/step - loss: 770.7017 - val_loss: 1313.3771\n",
      "Epoch 3/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 714.4071 - val_loss: 300.4567\n",
      "Epoch 4/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 658.0898 - val_loss: 148.0095\n",
      "Epoch 5/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 603.2621 - val_loss: 164.2145\n",
      "Epoch 6/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 549.2739 - val_loss: 185.6875\n",
      "Epoch 7/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 497.4592 - val_loss: 192.5521\n",
      "Epoch 8/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 448.4230 - val_loss: 191.8921\n",
      "Epoch 9/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 402.0921 - val_loss: 224.2902\n",
      "Epoch 10/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 359.1511 - val_loss: 212.7957\n",
      "Epoch 11/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 321.3968 - val_loss: 195.7047\n",
      "Epoch 12/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 288.1674 - val_loss: 195.8097\n",
      "Epoch 13/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 256.0132 - val_loss: 198.5049\n",
      "Epoch 14/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 228.9502 - val_loss: 197.8157\n",
      "Epoch 15/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 206.8358 - val_loss: 216.2815\n",
      "Epoch 16/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 185.1288 - val_loss: 185.5911\n",
      "Epoch 17/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 167.0553 - val_loss: 207.4659\n",
      "Epoch 18/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 153.9143 - val_loss: 191.0677\n",
      "Epoch 19/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 140.3240 - val_loss: 166.5617\n",
      "Epoch 20/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 128.5981 - val_loss: 185.1317\n",
      "Epoch 21/2000\n",
      "10508/10508 [==============================] - 62s 6ms/step - loss: 120.6502 - val_loss: 164.3489\n",
      "Epoch 22/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 107.2299 - val_loss: 111.4474\n",
      "Epoch 23/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 96.2572 - val_loss: 88.6855\n",
      "Epoch 24/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 89.1931 - val_loss: 90.2910\n",
      "Epoch 25/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 86.4232 - val_loss: 86.4773\n",
      "Epoch 26/2000\n",
      "10508/10508 [==============================] - 62s 6ms/step - loss: 82.7045 - val_loss: 85.5471\n",
      "Epoch 27/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 82.0835 - val_loss: 68.5953\n",
      "Epoch 28/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 79.3400 - val_loss: 73.8522\n",
      "Epoch 29/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 77.0542 - val_loss: 65.8444\n",
      "Epoch 30/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 77.1507 - val_loss: 71.6629\n",
      "Epoch 31/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 76.5297 - val_loss: 66.4908\n",
      "Epoch 32/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 76.3240 - val_loss: 64.3070\n",
      "Epoch 33/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 73.7344 - val_loss: 63.1222\n",
      "Epoch 34/2000\n",
      "10508/10508 [==============================] - 62s 6ms/step - loss: 73.8273 - val_loss: 58.0843\n",
      "Epoch 35/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 73.0169 - val_loss: 57.4926\n",
      "Epoch 36/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 72.2926 - val_loss: 59.7642\n",
      "Epoch 37/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 73.5081 - val_loss: 55.7328\n",
      "Epoch 38/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 72.8549 - val_loss: 63.5420\n",
      "Epoch 39/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 73.6475 - val_loss: 55.1311\n",
      "Epoch 40/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 72.2468 - val_loss: 57.0445\n",
      "Epoch 41/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 71.0914 - val_loss: 56.4142\n",
      "Epoch 42/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 70.5392 - val_loss: 51.4165\n",
      "Epoch 43/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 70.1972 - val_loss: 52.9204\n",
      "Epoch 44/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 69.1658 - val_loss: 55.1539\n",
      "Epoch 45/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 70.5021 - val_loss: 52.8922\n",
      "Epoch 46/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 69.6984 - val_loss: 52.9630\n",
      "Epoch 47/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 69.5466 - val_loss: 52.6730\n",
      "Epoch 48/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 70.1121 - val_loss: 53.2101\n",
      "Epoch 49/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 69.7708 - val_loss: 53.3450\n",
      "Epoch 50/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 68.4659 - val_loss: 50.3981\n",
      "Epoch 51/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 68.3659 - val_loss: 50.5929\n",
      "Epoch 52/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 69.7285 - val_loss: 55.7970\n",
      "Epoch 53/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 68.7023 - val_loss: 52.5352\n",
      "Epoch 54/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 68.8468 - val_loss: 50.9513\n",
      "Epoch 55/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 68.3377 - val_loss: 53.9823\n",
      "Epoch 56/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 67.1648 - val_loss: 51.8960\n",
      "Epoch 57/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 66.7309 - val_loss: 51.0525\n",
      "Epoch 58/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 66.9656 - val_loss: 50.1577\n",
      "Epoch 59/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 67.5219 - val_loss: 49.8280\n",
      "Epoch 60/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 68.8592 - val_loss: 50.5747\n",
      "Epoch 61/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 68.4906 - val_loss: 50.6920\n",
      "Epoch 62/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 67.8884 - val_loss: 50.0748\n",
      "Epoch 63/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 68.8407 - val_loss: 49.4561\n",
      "Epoch 64/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 68.0823 - val_loss: 48.5391\n",
      "Epoch 65/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 67.2639 - val_loss: 49.6900\n",
      "Epoch 66/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 68.0115 - val_loss: 49.6062\n",
      "Epoch 67/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 67.5596 - val_loss: 49.8292\n",
      "Epoch 68/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 66.8026 - val_loss: 48.2005\n",
      "Epoch 69/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 67.2694 - val_loss: 49.8447\n",
      "Epoch 70/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 66.0619 - val_loss: 49.4676\n",
      "Epoch 71/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 66.1968 - val_loss: 55.7968\n",
      "Epoch 72/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 66.9191 - val_loss: 49.2556\n",
      "Epoch 73/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 67.1641 - val_loss: 50.5326\n",
      "Epoch 74/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 65.3626 - val_loss: 48.8121\n",
      "Epoch 75/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 66.3660 - val_loss: 50.1069\n",
      "Epoch 76/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 66.8015 - val_loss: 48.5830\n",
      "Epoch 77/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10508/10508 [==============================] - 60s 6ms/step - loss: 66.6705 - val_loss: 51.1610\n",
      "Epoch 78/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 66.6100 - val_loss: 48.7250\n",
      "Epoch 79/2000\n",
      "10508/10508 [==============================] - 62s 6ms/step - loss: 66.6444 - val_loss: 51.2825\n",
      "Epoch 80/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 66.3629 - val_loss: 51.6094\n",
      "Epoch 81/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 65.2832 - val_loss: 49.3588\n",
      "Epoch 82/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 65.0016 - val_loss: 48.1962\n",
      "Epoch 83/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 64.7997 - val_loss: 49.4947\n",
      "Epoch 84/2000\n",
      "10508/10508 [==============================] - 62s 6ms/step - loss: 65.6699 - val_loss: 49.5069\n",
      "Epoch 85/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 65.0816 - val_loss: 48.4492\n",
      "Epoch 86/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 65.0352 - val_loss: 48.3535\n",
      "Epoch 87/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 65.4292 - val_loss: 49.6564\n",
      "Epoch 88/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 66.2218 - val_loss: 50.7814\n",
      "Epoch 89/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 64.8505 - val_loss: 47.9300\n",
      "Epoch 90/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 64.5843 - val_loss: 48.7717\n",
      "Epoch 91/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 63.9191 - val_loss: 47.7214\n",
      "Epoch 92/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 66.0138 - val_loss: 51.4569\n",
      "Epoch 93/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 64.3462 - val_loss: 48.0102\n",
      "Epoch 94/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 64.7717 - val_loss: 48.3360\n",
      "Epoch 95/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 63.4728 - val_loss: 49.9275\n",
      "Epoch 96/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 64.2833 - val_loss: 49.9793\n",
      "Epoch 97/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 65.9847 - val_loss: 47.9408\n",
      "Epoch 98/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 64.0852 - val_loss: 47.7271\n",
      "Epoch 99/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 64.1233 - val_loss: 47.2828\n",
      "Epoch 100/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 64.0117 - val_loss: 48.4811\n",
      "Epoch 101/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 64.7325 - val_loss: 50.4975\n",
      "Epoch 102/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 63.7207 - val_loss: 49.5047\n",
      "Epoch 103/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 64.5769 - val_loss: 50.6469\n",
      "Epoch 104/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 65.1164 - val_loss: 48.4087\n",
      "Epoch 105/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 65.4618 - val_loss: 47.7061\n",
      "Epoch 106/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 64.5754 - val_loss: 49.4626\n",
      "Epoch 107/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 64.4807 - val_loss: 52.2809\n",
      "Epoch 108/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 64.3719 - val_loss: 47.9550\n",
      "Epoch 109/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 64.2998 - val_loss: 47.2433\n",
      "Epoch 110/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 64.1440 - val_loss: 49.5230\n",
      "Epoch 111/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 64.2019 - val_loss: 48.5413\n",
      "Epoch 112/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 63.3615 - val_loss: 49.8543\n",
      "Epoch 113/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 64.0312 - val_loss: 50.1439\n",
      "Epoch 114/2000\n",
      "10508/10508 [==============================] - 62s 6ms/step - loss: 63.6226 - val_loss: 48.9831\n",
      "Epoch 115/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 63.3518 - val_loss: 47.3889\n",
      "Epoch 116/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 63.9779 - val_loss: 48.1267\n",
      "Epoch 117/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 63.8101 - val_loss: 51.6763\n",
      "Epoch 118/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 64.0916 - val_loss: 48.3619\n",
      "Epoch 119/2000\n",
      "10508/10508 [==============================] - 62s 6ms/step - loss: 63.4230 - val_loss: 50.6094\n",
      "Epoch 120/2000\n",
      "10508/10508 [==============================] - 62s 6ms/step - loss: 63.8215 - val_loss: 52.6634\n",
      "Epoch 121/2000\n",
      "10508/10508 [==============================] - 62s 6ms/step - loss: 64.2091 - val_loss: 48.3128\n",
      "Epoch 122/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 63.4627 - val_loss: 46.7949\n",
      "Epoch 123/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 63.2089 - val_loss: 51.1028\n",
      "Epoch 124/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 64.7764 - val_loss: 49.3234\n",
      "Epoch 125/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 62.7909 - val_loss: 47.6651\n",
      "Epoch 126/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 62.4626 - val_loss: 47.6112\n",
      "Epoch 127/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 63.4562 - val_loss: 48.1126\n",
      "Epoch 128/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 63.2468 - val_loss: 46.9171\n",
      "Epoch 129/2000\n",
      "10508/10508 [==============================] - 63s 6ms/step - loss: 63.3802 - val_loss: 48.3010\n",
      "Epoch 130/2000\n",
      "10508/10508 [==============================] - 63s 6ms/step - loss: 62.5316 - val_loss: 49.1280\n",
      "Epoch 131/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 63.3760 - val_loss: 47.5469\n",
      "Epoch 132/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 62.4409 - val_loss: 47.1620\n",
      "Epoch 133/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 63.1714 - val_loss: 49.8975\n",
      "Epoch 134/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 63.9696 - val_loss: 47.8414\n",
      "Epoch 135/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 64.3862 - val_loss: 52.0521\n",
      "Epoch 136/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 62.8388 - val_loss: 47.3262\n",
      "Epoch 137/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 62.6393 - val_loss: 48.2414\n",
      "Epoch 138/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 62.4177 - val_loss: 47.8619\n",
      "Epoch 139/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 62.7716 - val_loss: 46.9962\n",
      "Epoch 140/2000\n",
      "10508/10508 [==============================] - 59s 6ms/step - loss: 62.4567 - val_loss: 46.7037\n",
      "Epoch 141/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 61.5062 - val_loss: 47.0616\n",
      "Epoch 142/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 62.0893 - val_loss: 50.6239\n",
      "Epoch 143/2000\n",
      "10508/10508 [==============================] - 62s 6ms/step - loss: 62.4137 - val_loss: 47.6240\n",
      "Epoch 144/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 63.3343 - val_loss: 48.0771\n",
      "Epoch 145/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 62.3193 - val_loss: 50.6355\n",
      "Epoch 146/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 62.0544 - val_loss: 47.4924\n",
      "Epoch 147/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 62.4552 - val_loss: 47.7576\n",
      "Epoch 148/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 61.5581 - val_loss: 48.0874\n",
      "Epoch 149/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 62.8240 - val_loss: 48.5795\n",
      "Epoch 150/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 63.2885 - val_loss: 47.9183\n",
      "Epoch 151/2000\n",
      "10508/10508 [==============================] - 62s 6ms/step - loss: 62.7353 - val_loss: 47.5227\n",
      "Epoch 152/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 62.2834 - val_loss: 48.1616\n",
      "Epoch 153/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 61.5014 - val_loss: 51.4851\n",
      "Epoch 154/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 61.8792 - val_loss: 47.1310\n",
      "Epoch 155/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 61.8624 - val_loss: 47.4408\n",
      "Epoch 156/2000\n",
      "10508/10508 [==============================] - 62s 6ms/step - loss: 61.1916 - val_loss: 48.9136\n",
      "Epoch 157/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 61.7215 - val_loss: 47.9292\n",
      "Epoch 158/2000\n",
      "10508/10508 [==============================] - 60s 6ms/step - loss: 62.9554 - val_loss: 48.1260\n",
      "Epoch 159/2000\n",
      "10508/10508 [==============================] - 62s 6ms/step - loss: 62.2218 - val_loss: 47.6159\n",
      "Epoch 160/2000\n",
      "10508/10508 [==============================] - 61s 6ms/step - loss: 62.2411 - val_loss: 48.6315\n",
      "Currnent time step:1, lag:12\n",
      "Train on 10504 samples, validate on 3502 samples\n",
      "Epoch 1/2000\n",
      "10504/10504 [==============================] - 66s 6ms/step - loss: 904.1879 - val_loss: 186.7559\n",
      "Epoch 2/2000\n",
      "10504/10504 [==============================] - 60s 6ms/step - loss: 879.0069 - val_loss: 173.2750\n",
      "Epoch 3/2000\n",
      "10504/10504 [==============================] - 60s 6ms/step - loss: 858.8571 - val_loss: 245.6791\n",
      "Epoch 4/2000\n",
      "10504/10504 [==============================] - 60s 6ms/step - loss: 830.1495 - val_loss: 251.8954\n",
      "Epoch 5/2000\n",
      "10504/10504 [==============================] - 60s 6ms/step - loss: 798.2757 - val_loss: 297.2681\n",
      "Epoch 6/2000\n",
      "10504/10504 [==============================] - 59s 6ms/step - loss: 765.6397 - val_loss: 345.4690\n",
      "Epoch 7/2000\n",
      "10504/10504 [==============================] - 59s 6ms/step - loss: 731.4940 - val_loss: 416.8233\n",
      "Epoch 8/2000\n",
      "10504/10504 [==============================] - 59s 6ms/step - loss: 695.1675 - val_loss: 397.2724\n",
      "Epoch 9/2000\n",
      "10504/10504 [==============================] - 60s 6ms/step - loss: 655.3881 - val_loss: 388.8988\n",
      "Epoch 10/2000\n",
      "10504/10504 [==============================] - 60s 6ms/step - loss: 610.9832 - val_loss: 401.6388\n",
      "Epoch 11/2000\n",
      "10504/10504 [==============================] - 59s 6ms/step - loss: 566.2923 - val_loss: 382.3504\n",
      "Epoch 12/2000\n",
      "10504/10504 [==============================] - 59s 6ms/step - loss: 521.9961 - val_loss: 355.9330\n",
      "Epoch 13/2000\n",
      "10504/10504 [==============================] - 59s 6ms/step - loss: 480.6099 - val_loss: 346.7800\n",
      "Epoch 14/2000\n",
      "10504/10504 [==============================] - 60s 6ms/step - loss: 439.1988 - val_loss: 361.6599\n",
      "Epoch 15/2000\n",
      "10504/10504 [==============================] - 60s 6ms/step - loss: 398.8600 - val_loss: 328.8821\n",
      "Epoch 16/2000\n",
      "10504/10504 [==============================] - 60s 6ms/step - loss: 361.8391 - val_loss: 305.6380\n",
      "Epoch 17/2000\n",
      "10504/10504 [==============================] - 60s 6ms/step - loss: 328.1962 - val_loss: 263.0611\n",
      "Epoch 18/2000\n",
      "10504/10504 [==============================] - 60s 6ms/step - loss: 298.8803 - val_loss: 247.0335\n",
      "Epoch 19/2000\n",
      "10504/10504 [==============================] - 59s 6ms/step - loss: 268.6669 - val_loss: 235.3453\n",
      "Epoch 20/2000\n",
      "10504/10504 [==============================] - 60s 6ms/step - loss: 242.6214 - val_loss: 237.3946\n",
      "Epoch 21/2000\n",
      "10504/10504 [==============================] - 59s 6ms/step - loss: 224.4162 - val_loss: 223.5968\n",
      "Epoch 22/2000\n",
      "10504/10504 [==============================] - 60s 6ms/step - loss: 203.3295 - val_loss: 179.9517\n",
      "Currnent time step:1, lag:24\n",
      "Train on 10497 samples, validate on 3499 samples\n",
      "Epoch 1/2000\n",
      "10497/10497 [==============================] - 64s 6ms/step - loss: 835.0381 - val_loss: 1332.8506\n",
      "Epoch 2/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 783.8209 - val_loss: 410.2988\n",
      "Epoch 3/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 740.4347 - val_loss: 373.3339\n",
      "Epoch 4/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 696.8474 - val_loss: 274.4283\n",
      "Epoch 5/2000\n",
      "10497/10497 [==============================] - 60s 6ms/step - loss: 649.8055 - val_loss: 279.7805\n",
      "Epoch 6/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 601.8151 - val_loss: 362.3486\n",
      "Epoch 7/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 554.5717 - val_loss: 330.1126\n",
      "Epoch 8/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 510.5061 - val_loss: 348.2985\n",
      "Epoch 9/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 469.1240 - val_loss: 346.4567\n",
      "Epoch 10/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 434.6847 - val_loss: 365.5079\n",
      "Epoch 11/2000\n",
      "10497/10497 [==============================] - 60s 6ms/step - loss: 398.5956 - val_loss: 328.7031\n",
      "Epoch 12/2000\n",
      "10497/10497 [==============================] - 60s 6ms/step - loss: 367.2837 - val_loss: 326.9806\n",
      "Epoch 13/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 339.6933 - val_loss: 292.9810\n",
      "Epoch 14/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 315.3602 - val_loss: 304.0529\n",
      "Epoch 15/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 298.2708 - val_loss: 328.8908\n",
      "Epoch 16/2000\n",
      "10497/10497 [==============================] - 60s 6ms/step - loss: 278.3662 - val_loss: 271.1568\n",
      "Epoch 17/2000\n",
      "10497/10497 [==============================] - 61s 6ms/step - loss: 258.2121 - val_loss: 264.3930\n",
      "Epoch 18/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 237.4402 - val_loss: 189.5225\n",
      "Epoch 19/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 213.7793 - val_loss: 200.0374\n",
      "Epoch 20/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 200.7459 - val_loss: 194.0858\n",
      "Epoch 21/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 189.5724 - val_loss: 164.0120\n",
      "Epoch 22/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 181.4547 - val_loss: 155.9651\n",
      "Epoch 23/2000\n",
      "10497/10497 [==============================] - 60s 6ms/step - loss: 172.7930 - val_loss: 154.5685\n",
      "Epoch 24/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 175.7580 - val_loss: 157.5467\n",
      "Epoch 25/2000\n",
      "10497/10497 [==============================] - 60s 6ms/step - loss: 170.3390 - val_loss: 141.5859\n",
      "Epoch 26/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 165.3736 - val_loss: 136.5625\n",
      "Epoch 27/2000\n",
      "10497/10497 [==============================] - 61s 6ms/step - loss: 166.6235 - val_loss: 134.7316\n",
      "Epoch 28/2000\n",
      "10497/10497 [==============================] - 60s 6ms/step - loss: 166.0261 - val_loss: 131.2242\n",
      "Epoch 29/2000\n",
      "10497/10497 [==============================] - 61s 6ms/step - loss: 164.8768 - val_loss: 121.2055\n",
      "Epoch 30/2000\n",
      "10497/10497 [==============================] - 60s 6ms/step - loss: 163.3027 - val_loss: 125.1036\n",
      "Epoch 31/2000\n",
      "10497/10497 [==============================] - 61s 6ms/step - loss: 162.5911 - val_loss: 116.7262\n",
      "Epoch 32/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 161.3255 - val_loss: 115.4965\n",
      "Epoch 33/2000\n",
      "10497/10497 [==============================] - 60s 6ms/step - loss: 161.8345 - val_loss: 114.3090\n",
      "Epoch 34/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 160.1803 - val_loss: 116.6066\n",
      "Epoch 35/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 163.0318 - val_loss: 119.8205\n",
      "Epoch 36/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 162.8844 - val_loss: 114.7482\n",
      "Epoch 37/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 161.1261 - val_loss: 116.6133\n",
      "Epoch 38/2000\n",
      "10497/10497 [==============================] - 60s 6ms/step - loss: 160.0945 - val_loss: 114.9727\n",
      "Epoch 39/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 160.6722 - val_loss: 114.3135\n",
      "Epoch 40/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 159.4848 - val_loss: 118.7630\n",
      "Epoch 41/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10497/10497 [==============================] - 59s 6ms/step - loss: 156.8581 - val_loss: 114.7490\n",
      "Epoch 42/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 157.8583 - val_loss: 112.1866\n",
      "Epoch 43/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 157.5832 - val_loss: 117.2430\n",
      "Epoch 44/2000\n",
      "10497/10497 [==============================] - 60s 6ms/step - loss: 156.7311 - val_loss: 111.4892\n",
      "Epoch 45/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 159.1367 - val_loss: 116.5031\n",
      "Epoch 46/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 158.9600 - val_loss: 118.0008\n",
      "Epoch 47/2000\n",
      "10497/10497 [==============================] - 60s 6ms/step - loss: 155.5195 - val_loss: 114.7882\n",
      "Epoch 48/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 157.2328 - val_loss: 125.7619\n",
      "Epoch 49/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 156.2715 - val_loss: 114.1389\n",
      "Epoch 50/2000\n",
      "10497/10497 [==============================] - 61s 6ms/step - loss: 156.6018 - val_loss: 119.1612\n",
      "Epoch 51/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 154.2748 - val_loss: 113.4513\n",
      "Epoch 52/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 153.0475 - val_loss: 115.8863\n",
      "Epoch 53/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 154.5230 - val_loss: 112.8379\n",
      "Epoch 54/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 153.5193 - val_loss: 111.9912\n",
      "Epoch 55/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 153.3627 - val_loss: 119.1598\n",
      "Epoch 56/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 152.4034 - val_loss: 114.8559\n",
      "Epoch 57/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 153.6785 - val_loss: 111.9234\n",
      "Epoch 58/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 151.9163 - val_loss: 119.3711\n",
      "Epoch 59/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 153.9598 - val_loss: 115.8617\n",
      "Epoch 60/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 151.6535 - val_loss: 114.2409\n",
      "Epoch 61/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 151.8872 - val_loss: 113.3004\n",
      "Epoch 62/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 153.1066 - val_loss: 115.3888\n",
      "Epoch 63/2000\n",
      "10497/10497 [==============================] - 59s 6ms/step - loss: 151.9479 - val_loss: 111.8777\n",
      "Epoch 64/2000\n",
      "10497/10497 [==============================] - 60s 6ms/step - loss: 150.4094 - val_loss: 118.4488\n",
      "Currnent time step:12, lag:1\n",
      "Train on 10504 samples, validate on 3502 samples\n",
      "Epoch 1/2000\n",
      "10504/10504 [==============================] - 87s 8ms/step - loss: 841.0771 - val_loss: 2243.3677\n",
      "Epoch 2/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 773.1524 - val_loss: 1011.0292\n",
      "Epoch 3/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 715.8361 - val_loss: 195.6729\n",
      "Epoch 4/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 653.6410 - val_loss: 156.5588\n",
      "Epoch 5/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 591.1207 - val_loss: 130.9670\n",
      "Epoch 6/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 530.3977 - val_loss: 127.6123\n",
      "Epoch 7/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 471.9845 - val_loss: 130.2038\n",
      "Epoch 8/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 418.3469 - val_loss: 142.1286\n",
      "Epoch 9/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 367.8593 - val_loss: 154.3506\n",
      "Epoch 10/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 321.8168 - val_loss: 159.3702\n",
      "Epoch 11/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 279.6530 - val_loss: 177.7971\n",
      "Epoch 12/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 242.9112 - val_loss: 216.3198\n",
      "Epoch 13/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 209.5775 - val_loss: 250.9025\n",
      "Epoch 14/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 182.4275 - val_loss: 188.6804\n",
      "Epoch 15/2000\n",
      "10504/10504 [==============================] - 81s 8ms/step - loss: 156.3422 - val_loss: 220.2079\n",
      "Epoch 16/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 134.2007 - val_loss: 207.6712\n",
      "Epoch 17/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 115.7300 - val_loss: 212.3264\n",
      "Epoch 18/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 100.0303 - val_loss: 225.2605\n",
      "Epoch 19/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 86.6165 - val_loss: 224.2067\n",
      "Epoch 20/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 75.3430 - val_loss: 209.6488\n",
      "Epoch 21/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 64.4589 - val_loss: 197.0468\n",
      "Epoch 22/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 55.3549 - val_loss: 135.7199\n",
      "Epoch 23/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 44.1479 - val_loss: 101.8275\n",
      "Epoch 24/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 38.4169 - val_loss: 88.2709\n",
      "Epoch 25/2000\n",
      "10504/10504 [==============================] - 84s 8ms/step - loss: 33.1244 - val_loss: 62.3825\n",
      "Epoch 26/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 31.3538 - val_loss: 54.1932\n",
      "Epoch 27/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 28.2738 - val_loss: 49.0573\n",
      "Epoch 28/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 26.3181 - val_loss: 45.5725\n",
      "Epoch 29/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 25.3471 - val_loss: 33.0963\n",
      "Epoch 30/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 24.5223 - val_loss: 32.4045\n",
      "Epoch 31/2000\n",
      "10504/10504 [==============================] - 84s 8ms/step - loss: 23.2898 - val_loss: 23.5149\n",
      "Epoch 32/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 22.9217 - val_loss: 23.5359\n",
      "Epoch 33/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 22.0678 - val_loss: 24.8149\n",
      "Epoch 34/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 22.4130 - val_loss: 20.0470\n",
      "Epoch 35/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 21.3374 - val_loss: 17.8608\n",
      "Epoch 36/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 21.0501 - val_loss: 17.1111\n",
      "Epoch 37/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 20.5772 - val_loss: 17.1388\n",
      "Epoch 38/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 20.4025 - val_loss: 14.3192\n",
      "Epoch 39/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 19.5565 - val_loss: 16.7520\n",
      "Epoch 40/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 19.6702 - val_loss: 17.6297\n",
      "Epoch 41/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 19.2051 - val_loss: 14.9139\n",
      "Epoch 42/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 19.6510 - val_loss: 13.7696\n",
      "Epoch 43/2000\n",
      "10504/10504 [==============================] - 84s 8ms/step - loss: 18.8180 - val_loss: 14.7756\n",
      "Epoch 44/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 18.6502 - val_loss: 13.3696\n",
      "Epoch 45/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 19.1566 - val_loss: 18.9412\n",
      "Epoch 46/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 18.8034 - val_loss: 15.9539\n",
      "Epoch 47/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 18.3480 - val_loss: 14.3345\n",
      "Epoch 48/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 18.5561 - val_loss: 14.5138\n",
      "Epoch 49/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 17.9732 - val_loss: 12.8389\n",
      "Epoch 50/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 18.2053 - val_loss: 13.0090\n",
      "Epoch 51/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 17.8879 - val_loss: 12.9534\n",
      "Epoch 52/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 17.9078 - val_loss: 14.3891\n",
      "Epoch 53/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 18.8804 - val_loss: 15.0889\n",
      "Epoch 54/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 17.9668 - val_loss: 12.3220\n",
      "Epoch 55/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 18.4832 - val_loss: 15.3790\n",
      "Epoch 56/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 18.4650 - val_loss: 12.3142\n",
      "Epoch 57/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 18.5675 - val_loss: 14.6449\n",
      "Epoch 58/2000\n",
      "10504/10504 [==============================] - 84s 8ms/step - loss: 17.3156 - val_loss: 13.0728\n",
      "Epoch 59/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 17.8599 - val_loss: 12.7604\n",
      "Epoch 60/2000\n",
      "10504/10504 [==============================] - 84s 8ms/step - loss: 17.9122 - val_loss: 13.3249\n",
      "Epoch 61/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 18.1402 - val_loss: 13.6373\n",
      "Epoch 62/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 18.1021 - val_loss: 12.8699\n",
      "Epoch 63/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 17.7247 - val_loss: 13.4838\n",
      "Epoch 64/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 17.7376 - val_loss: 12.0668\n",
      "Epoch 65/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 17.7710 - val_loss: 12.4337\n",
      "Epoch 66/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 18.0970 - val_loss: 11.9269\n",
      "Epoch 67/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 17.5049 - val_loss: 14.3354\n",
      "Epoch 68/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 17.3134 - val_loss: 12.6406\n",
      "Epoch 69/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 17.6457 - val_loss: 13.3785\n",
      "Epoch 70/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 17.0169 - val_loss: 11.2439\n",
      "Epoch 71/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 17.4853 - val_loss: 11.9402\n",
      "Epoch 72/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 16.7138 - val_loss: 11.5286\n",
      "Epoch 73/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 17.1858 - val_loss: 11.2877\n",
      "Epoch 74/2000\n",
      "10504/10504 [==============================] - 81s 8ms/step - loss: 16.6601 - val_loss: 12.1802\n",
      "Epoch 75/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 16.9592 - val_loss: 12.1197\n",
      "Epoch 76/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 16.7268 - val_loss: 12.5838\n",
      "Epoch 77/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 16.5284 - val_loss: 10.9361\n",
      "Epoch 78/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 16.9039 - val_loss: 11.6017\n",
      "Epoch 79/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 17.6716 - val_loss: 10.8402\n",
      "Epoch 80/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 17.1763 - val_loss: 12.5937\n",
      "Epoch 81/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 16.9132 - val_loss: 11.1962\n",
      "Epoch 82/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 16.6443 - val_loss: 10.5193\n",
      "Epoch 83/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 16.3391 - val_loss: 10.8638\n",
      "Epoch 84/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 16.8282 - val_loss: 11.5014\n",
      "Epoch 85/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 17.5475 - val_loss: 13.3603\n",
      "Epoch 86/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 16.2622 - val_loss: 10.6936\n",
      "Epoch 87/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 16.1606 - val_loss: 10.4980\n",
      "Epoch 88/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 15.9786 - val_loss: 10.4806\n",
      "Epoch 89/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 16.1873 - val_loss: 10.6129\n",
      "Epoch 90/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 16.4032 - val_loss: 11.1551\n",
      "Epoch 91/2000\n",
      "10504/10504 [==============================] - 84s 8ms/step - loss: 16.3251 - val_loss: 11.3665\n",
      "Epoch 92/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 16.4865 - val_loss: 11.7040\n",
      "Epoch 93/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 16.5074 - val_loss: 11.4246\n",
      "Epoch 94/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 16.0448 - val_loss: 11.0657\n",
      "Epoch 95/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 16.7595 - val_loss: 11.0484\n",
      "Epoch 96/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 17.1753 - val_loss: 11.1944\n",
      "Epoch 97/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 16.2596 - val_loss: 13.7940\n",
      "Epoch 98/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 15.9977 - val_loss: 12.5531\n",
      "Epoch 99/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 16.2901 - val_loss: 11.9291\n",
      "Epoch 100/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 16.6029 - val_loss: 10.7927\n",
      "Epoch 101/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 16.2780 - val_loss: 10.8118\n",
      "Epoch 102/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 15.2727 - val_loss: 10.3279\n",
      "Epoch 103/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 15.4542 - val_loss: 10.7953\n",
      "Epoch 104/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 15.4885 - val_loss: 10.6250\n",
      "Epoch 105/2000\n",
      "10504/10504 [==============================] - 81s 8ms/step - loss: 15.3319 - val_loss: 10.2047\n",
      "Epoch 106/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 15.7497 - val_loss: 9.9077\n",
      "Epoch 107/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 15.6481 - val_loss: 11.4378\n",
      "Epoch 108/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 15.6586 - val_loss: 10.8634\n",
      "Epoch 109/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 16.2569 - val_loss: 10.8701\n",
      "Epoch 110/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 16.1546 - val_loss: 10.1152\n",
      "Epoch 111/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 16.6999 - val_loss: 10.4884\n",
      "Epoch 112/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 15.7045 - val_loss: 10.4761\n",
      "Epoch 113/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 15.2123 - val_loss: 10.6212\n",
      "Epoch 114/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 15.8091 - val_loss: 10.1589\n",
      "Epoch 115/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 15.2584 - val_loss: 11.4106\n",
      "Epoch 116/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 15.2078 - val_loss: 10.8005\n",
      "Epoch 117/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 15.4374 - val_loss: 10.3486\n",
      "Epoch 118/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 15.2068 - val_loss: 10.0268\n",
      "Epoch 119/2000\n",
      "10504/10504 [==============================] - 81s 8ms/step - loss: 15.5217 - val_loss: 10.6703\n",
      "Epoch 120/2000\n",
      "10504/10504 [==============================] - 81s 8ms/step - loss: 15.7264 - val_loss: 12.6265\n",
      "Epoch 121/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 16.6879 - val_loss: 10.6383\n",
      "Epoch 122/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 15.2031 - val_loss: 10.3041\n",
      "Epoch 123/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 15.4504 - val_loss: 10.9671\n",
      "Epoch 124/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10504/10504 [==============================] - 83s 8ms/step - loss: 15.4791 - val_loss: 11.5501\n",
      "Epoch 125/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 15.3290 - val_loss: 10.6963\n",
      "Epoch 126/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 14.7362 - val_loss: 10.8188\n",
      "Currnent time step:12, lag:2\n",
      "Train on 10504 samples, validate on 3501 samples\n",
      "Epoch 1/2000\n",
      "10504/10504 [==============================] - 86s 8ms/step - loss: 885.5298 - val_loss: 144.2300\n",
      "Epoch 2/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 853.7586 - val_loss: 93.6290\n",
      "Epoch 3/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 829.7831 - val_loss: 143.2816\n",
      "Epoch 4/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 793.8055 - val_loss: 173.2243\n",
      "Epoch 5/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 756.5093 - val_loss: 188.6690\n",
      "Epoch 6/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 716.5009 - val_loss: 255.7617\n",
      "Epoch 7/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 664.0383 - val_loss: 217.9212\n",
      "Epoch 8/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 611.3408 - val_loss: 339.0175\n",
      "Epoch 9/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 560.4633 - val_loss: 291.6624\n",
      "Epoch 10/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 510.2312 - val_loss: 281.0958\n",
      "Epoch 11/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 461.5101 - val_loss: 284.6838\n",
      "Epoch 12/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 413.0201 - val_loss: 288.0106\n",
      "Epoch 13/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 368.1130 - val_loss: 226.3705\n",
      "Epoch 14/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 323.5752 - val_loss: 214.2504\n",
      "Epoch 15/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 282.2129 - val_loss: 245.9173\n",
      "Epoch 16/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 243.0613 - val_loss: 284.8863\n",
      "Epoch 17/2000\n",
      "10504/10504 [==============================] - 84s 8ms/step - loss: 209.9533 - val_loss: 160.3665\n",
      "Epoch 18/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 179.5120 - val_loss: 161.7973\n",
      "Epoch 19/2000\n",
      "10504/10504 [==============================] - 82s 8ms/step - loss: 153.2818 - val_loss: 182.7447\n",
      "Epoch 20/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 130.9098 - val_loss: 162.9423\n",
      "Epoch 21/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 110.7986 - val_loss: 126.2201\n",
      "Epoch 22/2000\n",
      "10504/10504 [==============================] - 83s 8ms/step - loss: 94.6115 - val_loss: 122.5728\n",
      "Currnent time step:12, lag:4\n",
      "Train on 10503 samples, validate on 3501 samples\n",
      "Epoch 1/2000\n",
      "10503/10503 [==============================] - 87s 8ms/step - loss: 892.9687 - val_loss: 171.7013\n",
      "Epoch 2/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 847.2832 - val_loss: 146.9246\n",
      "Epoch 3/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 810.0011 - val_loss: 185.1256\n",
      "Epoch 4/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 772.1235 - val_loss: 272.1657\n",
      "Epoch 5/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 732.3403 - val_loss: 349.5631\n",
      "Epoch 6/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 691.4191 - val_loss: 392.6583\n",
      "Epoch 7/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 648.8960 - val_loss: 323.2830\n",
      "Epoch 8/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 605.0559 - val_loss: 333.5462\n",
      "Epoch 9/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 560.4188 - val_loss: 360.7622\n",
      "Epoch 10/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 514.9670 - val_loss: 368.5969\n",
      "Epoch 11/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 464.3404 - val_loss: 276.7918\n",
      "Epoch 12/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 417.3678 - val_loss: 277.7939\n",
      "Epoch 13/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 371.5380 - val_loss: 242.7163\n",
      "Epoch 14/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 331.5021 - val_loss: 171.0912\n",
      "Epoch 15/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 292.9967 - val_loss: 195.7221\n",
      "Epoch 16/2000\n",
      "10503/10503 [==============================] - 85s 8ms/step - loss: 256.7619 - val_loss: 221.1400\n",
      "Epoch 17/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 224.2171 - val_loss: 180.0926\n",
      "Epoch 18/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 195.6754 - val_loss: 186.3573\n",
      "Epoch 19/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 169.8458 - val_loss: 165.6654\n",
      "Epoch 20/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 148.3978 - val_loss: 107.3025\n",
      "Epoch 21/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 129.1970 - val_loss: 137.7174\n",
      "Epoch 22/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 113.9325 - val_loss: 179.4214\n",
      "Epoch 23/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 101.1147 - val_loss: 96.2613\n",
      "Epoch 24/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 90.6696 - val_loss: 146.7921\n",
      "Epoch 25/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 80.4934 - val_loss: 84.7137\n",
      "Epoch 26/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 72.9382 - val_loss: 59.5694\n",
      "Epoch 27/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 68.4018 - val_loss: 43.7088\n",
      "Epoch 28/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 62.4569 - val_loss: 58.7291\n",
      "Epoch 29/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 59.4309 - val_loss: 45.4122\n",
      "Epoch 30/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 55.7844 - val_loss: 48.8452\n",
      "Epoch 31/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 51.5621 - val_loss: 45.4488\n",
      "Epoch 32/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 48.5625 - val_loss: 48.7717\n",
      "Epoch 33/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 48.5215 - val_loss: 48.9604\n",
      "Epoch 34/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 47.1191 - val_loss: 48.3375\n",
      "Epoch 35/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 45.8154 - val_loss: 48.1767\n",
      "Epoch 36/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 43.9275 - val_loss: 50.8443\n",
      "Epoch 37/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 43.9202 - val_loss: 46.4121\n",
      "Epoch 38/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 43.2426 - val_loss: 45.4743\n",
      "Epoch 39/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 42.0182 - val_loss: 47.3357\n",
      "Epoch 40/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 41.5068 - val_loss: 45.9038\n",
      "Epoch 41/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 41.1637 - val_loss: 44.6294\n",
      "Epoch 42/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 41.3847 - val_loss: 37.5024\n",
      "Epoch 43/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 41.4855 - val_loss: 33.2750\n",
      "Epoch 44/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 40.4181 - val_loss: 41.0268\n",
      "Epoch 45/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 40.7633 - val_loss: 36.5898\n",
      "Epoch 46/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 40.0396 - val_loss: 30.7287\n",
      "Epoch 47/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 40.4111 - val_loss: 33.2299\n",
      "Epoch 48/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 40.1426 - val_loss: 34.7394\n",
      "Epoch 49/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 39.9979 - val_loss: 34.1309\n",
      "Epoch 50/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 39.9219 - val_loss: 31.8562\n",
      "Epoch 51/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 39.8226 - val_loss: 33.2505\n",
      "Epoch 52/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 40.7694 - val_loss: 31.4139\n",
      "Epoch 53/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 39.6290 - val_loss: 30.5809\n",
      "Epoch 54/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 39.5066 - val_loss: 29.6443\n",
      "Epoch 55/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 38.7282 - val_loss: 34.3310\n",
      "Epoch 56/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 39.5517 - val_loss: 36.8466\n",
      "Epoch 57/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 38.9509 - val_loss: 35.0775\n",
      "Epoch 58/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 38.1538 - val_loss: 32.0538\n",
      "Epoch 59/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 38.5794 - val_loss: 30.6249\n",
      "Epoch 60/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 38.9085 - val_loss: 30.7153\n",
      "Epoch 61/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 37.4855 - val_loss: 29.9262\n",
      "Epoch 62/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 38.4530 - val_loss: 29.8926\n",
      "Epoch 63/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 38.9307 - val_loss: 30.0008\n",
      "Epoch 64/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 38.0760 - val_loss: 35.6573\n",
      "Epoch 65/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 38.7793 - val_loss: 29.2077\n",
      "Epoch 66/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 37.7833 - val_loss: 31.9101\n",
      "Epoch 67/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 37.5525 - val_loss: 28.6267\n",
      "Epoch 68/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 37.3633 - val_loss: 31.6743\n",
      "Epoch 69/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 40.0016 - val_loss: 33.0931\n",
      "Epoch 70/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 37.8273 - val_loss: 29.4355\n",
      "Epoch 71/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 37.2221 - val_loss: 30.9264\n",
      "Epoch 72/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 36.7847 - val_loss: 29.4926\n",
      "Epoch 73/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 36.8171 - val_loss: 30.8132\n",
      "Epoch 74/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 37.7142 - val_loss: 32.9563\n",
      "Epoch 75/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 37.0186 - val_loss: 34.6794\n",
      "Epoch 76/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 36.7594 - val_loss: 30.2043\n",
      "Epoch 77/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 36.8268 - val_loss: 36.9603\n",
      "Epoch 78/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 37.1866 - val_loss: 28.0763\n",
      "Epoch 79/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 36.8543 - val_loss: 27.9619\n",
      "Epoch 80/2000\n",
      "10503/10503 [==============================] - 85s 8ms/step - loss: 36.8866 - val_loss: 27.8258\n",
      "Epoch 81/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 36.7473 - val_loss: 30.2111\n",
      "Epoch 82/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 37.3949 - val_loss: 27.8224\n",
      "Epoch 83/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 36.5122 - val_loss: 37.6264\n",
      "Epoch 84/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 36.0685 - val_loss: 28.8084\n",
      "Epoch 85/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 38.3895 - val_loss: 31.1233\n",
      "Epoch 86/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 36.3235 - val_loss: 30.9680\n",
      "Epoch 87/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 35.8103 - val_loss: 28.8667\n",
      "Epoch 88/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 35.2858 - val_loss: 28.0806\n",
      "Epoch 89/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 35.9405 - val_loss: 27.4718\n",
      "Epoch 90/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 35.5047 - val_loss: 30.0045\n",
      "Epoch 91/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 35.4142 - val_loss: 27.2312\n",
      "Epoch 92/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 36.7388 - val_loss: 28.6945\n",
      "Epoch 93/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 35.2245 - val_loss: 28.5596\n",
      "Epoch 94/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 35.5889 - val_loss: 29.2887\n",
      "Epoch 95/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 36.0931 - val_loss: 28.9183\n",
      "Epoch 96/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 35.1413 - val_loss: 28.2287\n",
      "Epoch 97/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 35.7919 - val_loss: 27.6218\n",
      "Epoch 98/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 34.7061 - val_loss: 29.4014\n",
      "Epoch 99/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 35.5477 - val_loss: 28.0495\n",
      "Epoch 100/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 35.9455 - val_loss: 28.9453\n",
      "Epoch 101/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 35.0806 - val_loss: 28.5063\n",
      "Epoch 102/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 35.7713 - val_loss: 27.1915\n",
      "Epoch 103/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 35.3549 - val_loss: 30.4856\n",
      "Epoch 104/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 34.3952 - val_loss: 28.0013\n",
      "Epoch 105/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 34.8688 - val_loss: 27.9725\n",
      "Epoch 106/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 36.0565 - val_loss: 30.2908\n",
      "Epoch 107/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 34.4803 - val_loss: 26.7790\n",
      "Epoch 108/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 34.3119 - val_loss: 28.1703\n",
      "Epoch 109/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 34.6740 - val_loss: 30.7953\n",
      "Epoch 110/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 35.8981 - val_loss: 34.0267\n",
      "Epoch 111/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 34.6166 - val_loss: 38.0517\n",
      "Epoch 112/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 34.3790 - val_loss: 34.7901\n",
      "Epoch 113/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 34.3056 - val_loss: 31.1308\n",
      "Epoch 114/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 34.0952 - val_loss: 27.6981\n",
      "Epoch 115/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 34.8921 - val_loss: 29.8450\n",
      "Epoch 116/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 34.7843 - val_loss: 28.4226\n",
      "Epoch 117/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 33.4416 - val_loss: 26.9103\n",
      "Epoch 118/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 34.3528 - val_loss: 27.1666\n",
      "Epoch 119/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 35.0038 - val_loss: 30.2958\n",
      "Epoch 120/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 34.4238 - val_loss: 29.1932\n",
      "Epoch 121/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 34.2313 - val_loss: 26.7972\n",
      "Epoch 122/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10503/10503 [==============================] - 82s 8ms/step - loss: 33.4673 - val_loss: 28.5385\n",
      "Epoch 123/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 33.4867 - val_loss: 27.0036\n",
      "Epoch 124/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 33.8704 - val_loss: 27.3909\n",
      "Epoch 125/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 33.4499 - val_loss: 28.2449\n",
      "Epoch 126/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 33.4525 - val_loss: 28.9709\n",
      "Epoch 127/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 34.9039 - val_loss: 26.7718\n",
      "Epoch 128/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 33.6879 - val_loss: 26.8367\n",
      "Epoch 129/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 34.1941 - val_loss: 38.9105\n",
      "Epoch 130/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 35.0103 - val_loss: 27.8373\n",
      "Epoch 131/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 33.7058 - val_loss: 26.5178\n",
      "Epoch 132/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 35.0516 - val_loss: 32.0643\n",
      "Epoch 133/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 33.3576 - val_loss: 28.0191\n",
      "Epoch 134/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 34.0515 - val_loss: 30.2922\n",
      "Epoch 135/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 33.9269 - val_loss: 33.7152\n",
      "Epoch 136/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 33.9697 - val_loss: 27.3013\n",
      "Epoch 137/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 34.1061 - val_loss: 27.5979\n",
      "Epoch 138/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 33.2707 - val_loss: 26.8966\n",
      "Epoch 139/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 32.4225 - val_loss: 26.5597\n",
      "Epoch 140/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 32.7970 - val_loss: 27.9734\n",
      "Epoch 141/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 32.7051 - val_loss: 28.5436\n",
      "Epoch 142/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 32.2336 - val_loss: 26.9980\n",
      "Epoch 143/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 33.3907 - val_loss: 28.5649\n",
      "Epoch 144/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 32.4969 - val_loss: 26.5519\n",
      "Epoch 145/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 32.7749 - val_loss: 27.3852\n",
      "Epoch 146/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 32.1862 - val_loss: 26.5593\n",
      "Epoch 147/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 32.8723 - val_loss: 26.2759\n",
      "Epoch 148/2000\n",
      "10503/10503 [==============================] - 85s 8ms/step - loss: 32.9929 - val_loss: 34.3179\n",
      "Epoch 149/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 33.9814 - val_loss: 33.1154\n",
      "Epoch 150/2000\n",
      "10503/10503 [==============================] - 81s 8ms/step - loss: 32.8192 - val_loss: 28.3036\n",
      "Epoch 151/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 32.2146 - val_loss: 27.0985\n",
      "Epoch 152/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 33.0677 - val_loss: 26.8224\n",
      "Epoch 153/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 33.4138 - val_loss: 28.8618\n",
      "Epoch 154/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 32.3727 - val_loss: 28.3232\n",
      "Epoch 155/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 32.8435 - val_loss: 27.7871\n",
      "Epoch 156/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 32.3018 - val_loss: 28.3349\n",
      "Epoch 157/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 33.0619 - val_loss: 28.7735\n",
      "Epoch 158/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 32.3365 - val_loss: 26.2479\n",
      "Epoch 159/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 31.7393 - val_loss: 26.0515\n",
      "Epoch 160/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 32.6741 - val_loss: 37.9675\n",
      "Epoch 161/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 32.7882 - val_loss: 27.5651\n",
      "Epoch 162/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 32.1134 - val_loss: 29.2673\n",
      "Epoch 163/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 31.2112 - val_loss: 26.5131\n",
      "Epoch 164/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 31.4627 - val_loss: 27.8464\n",
      "Epoch 165/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 31.8852 - val_loss: 26.7114\n",
      "Epoch 166/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 32.4744 - val_loss: 27.6159\n",
      "Epoch 167/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 32.6337 - val_loss: 27.1675\n",
      "Epoch 168/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 31.9446 - val_loss: 28.9988\n",
      "Epoch 169/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 31.6095 - val_loss: 26.2013\n",
      "Epoch 170/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 31.3902 - val_loss: 27.4481\n",
      "Epoch 171/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 32.1943 - val_loss: 26.9362\n",
      "Epoch 172/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 31.7708 - val_loss: 28.5631\n",
      "Epoch 173/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 33.2552 - val_loss: 27.8827\n",
      "Epoch 174/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 31.6854 - val_loss: 29.0570\n",
      "Epoch 175/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 31.9449 - val_loss: 26.4983\n",
      "Epoch 176/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 32.0875 - val_loss: 26.2563\n",
      "Epoch 177/2000\n",
      "10503/10503 [==============================] - 82s 8ms/step - loss: 31.3323 - val_loss: 27.8400\n",
      "Epoch 178/2000\n",
      "10503/10503 [==============================] - 84s 8ms/step - loss: 32.4482 - val_loss: 27.9743\n",
      "Epoch 179/2000\n",
      "10503/10503 [==============================] - 83s 8ms/step - loss: 31.9848 - val_loss: 27.8215\n",
      "Currnent time step:12, lag:6\n",
      "Train on 10501 samples, validate on 3501 samples\n",
      "Epoch 1/2000\n",
      "10501/10501 [==============================] - 88s 8ms/step - loss: 857.2333 - val_loss: 1315.0204\n",
      "Epoch 2/2000\n",
      "10501/10501 [==============================] - 81s 8ms/step - loss: 795.2758 - val_loss: 286.7263\n",
      "Epoch 3/2000\n",
      "10501/10501 [==============================] - 84s 8ms/step - loss: 733.2043 - val_loss: 155.7620\n",
      "Epoch 4/2000\n",
      "10501/10501 [==============================] - 84s 8ms/step - loss: 672.2809 - val_loss: 171.7869\n",
      "Epoch 5/2000\n",
      "10501/10501 [==============================] - 85s 8ms/step - loss: 613.1966 - val_loss: 323.5074\n",
      "Epoch 6/2000\n",
      "10501/10501 [==============================] - 85s 8ms/step - loss: 555.9668 - val_loss: 243.9315\n",
      "Epoch 7/2000\n",
      "10501/10501 [==============================] - 88s 8ms/step - loss: 500.1185 - val_loss: 269.2537\n",
      "Epoch 8/2000\n",
      "10501/10501 [==============================] - 88s 8ms/step - loss: 448.2545 - val_loss: 238.6856\n",
      "Epoch 9/2000\n",
      "10501/10501 [==============================] - 87s 8ms/step - loss: 399.7152 - val_loss: 237.2044\n",
      "Epoch 10/2000\n",
      "10501/10501 [==============================] - 88s 8ms/step - loss: 355.5863 - val_loss: 224.3119\n",
      "Epoch 11/2000\n",
      "10501/10501 [==============================] - 89s 8ms/step - loss: 314.3466 - val_loss: 281.9232\n",
      "Epoch 12/2000\n",
      "10501/10501 [==============================] - 89s 8ms/step - loss: 279.0969 - val_loss: 310.1009\n",
      "Epoch 13/2000\n",
      "10501/10501 [==============================] - 89s 9ms/step - loss: 245.1951 - val_loss: 189.2324\n",
      "Epoch 14/2000\n",
      "10501/10501 [==============================] - 90s 9ms/step - loss: 218.0355 - val_loss: 286.5097\n",
      "Epoch 15/2000\n",
      "10501/10501 [==============================] - 89s 8ms/step - loss: 191.9713 - val_loss: 243.1341\n",
      "Epoch 16/2000\n",
      "10501/10501 [==============================] - 89s 8ms/step - loss: 169.6450 - val_loss: 186.0277\n",
      "Epoch 17/2000\n",
      "10501/10501 [==============================] - 90s 9ms/step - loss: 151.4302 - val_loss: 246.6632\n",
      "Epoch 18/2000\n",
      "10501/10501 [==============================] - 90s 9ms/step - loss: 137.1318 - val_loss: 115.8456\n",
      "Epoch 19/2000\n",
      "10501/10501 [==============================] - 89s 8ms/step - loss: 115.4324 - val_loss: 144.7396\n",
      "Epoch 20/2000\n",
      "10501/10501 [==============================] - 89s 9ms/step - loss: 101.9227 - val_loss: 107.9002\n",
      "Epoch 21/2000\n",
      "10501/10501 [==============================] - 91s 9ms/step - loss: 90.3786 - val_loss: 96.9636\n",
      "Epoch 22/2000\n",
      "10501/10501 [==============================] - 88s 8ms/step - loss: 80.4769 - val_loss: 81.9201\n",
      "Epoch 23/2000\n",
      "10501/10501 [==============================] - 89s 8ms/step - loss: 76.5735 - val_loss: 78.6249\n",
      "Epoch 24/2000\n",
      "10501/10501 [==============================] - 88s 8ms/step - loss: 70.3914 - val_loss: 78.8356\n",
      "Epoch 25/2000\n",
      "10501/10501 [==============================] - 90s 9ms/step - loss: 67.9291 - val_loss: 87.7718\n",
      "Epoch 26/2000\n",
      "10501/10501 [==============================] - 89s 8ms/step - loss: 69.6713 - val_loss: 72.2803\n",
      "Epoch 27/2000\n",
      "10501/10501 [==============================] - 89s 8ms/step - loss: 65.1776 - val_loss: 59.3167\n",
      "Epoch 28/2000\n",
      "10501/10501 [==============================] - 88s 8ms/step - loss: 62.6634 - val_loss: 55.0966\n",
      "Epoch 29/2000\n",
      "10501/10501 [==============================] - 88s 8ms/step - loss: 60.8438 - val_loss: 59.6827\n",
      "Epoch 30/2000\n",
      "10501/10501 [==============================] - 88s 8ms/step - loss: 58.9891 - val_loss: 58.8795\n",
      "Epoch 31/2000\n",
      "10501/10501 [==============================] - 88s 8ms/step - loss: 59.8170 - val_loss: 53.4336\n",
      "Epoch 32/2000\n",
      "10501/10501 [==============================] - 90s 9ms/step - loss: 58.0354 - val_loss: 61.3498\n",
      "Epoch 33/2000\n",
      "10501/10501 [==============================] - 89s 8ms/step - loss: 58.3874 - val_loss: 50.3981\n",
      "Epoch 34/2000\n",
      "10501/10501 [==============================] - 91s 9ms/step - loss: 57.0190 - val_loss: 54.2213\n",
      "Epoch 35/2000\n",
      "10501/10501 [==============================] - 91s 9ms/step - loss: 56.8677 - val_loss: 46.9182\n",
      "Epoch 36/2000\n",
      "10501/10501 [==============================] - 90s 9ms/step - loss: 57.7445 - val_loss: 47.6251\n",
      "Epoch 37/2000\n",
      "10501/10501 [==============================] - 91s 9ms/step - loss: 57.0792 - val_loss: 47.1476\n",
      "Epoch 38/2000\n",
      "10501/10501 [==============================] - 89s 9ms/step - loss: 56.4993 - val_loss: 47.0778\n",
      "Epoch 39/2000\n",
      "10501/10501 [==============================] - 90s 9ms/step - loss: 55.0987 - val_loss: 46.3027\n",
      "Epoch 40/2000\n",
      "10501/10501 [==============================] - 90s 9ms/step - loss: 57.3264 - val_loss: 47.7494\n",
      "Epoch 41/2000\n",
      "10501/10501 [==============================] - 89s 8ms/step - loss: 56.8817 - val_loss: 45.2734\n",
      "Epoch 42/2000\n",
      "10501/10501 [==============================] - 89s 8ms/step - loss: 55.6238 - val_loss: 45.3571\n",
      "Epoch 43/2000\n",
      "10501/10501 [==============================] - 90s 9ms/step - loss: 54.1735 - val_loss: 49.8928\n",
      "Epoch 44/2000\n",
      "10501/10501 [==============================] - 89s 8ms/step - loss: 55.1276 - val_loss: 44.8839\n",
      "Epoch 45/2000\n",
      "10501/10501 [==============================] - 91s 9ms/step - loss: 53.6290 - val_loss: 43.8669\n",
      "Epoch 46/2000\n",
      "10501/10501 [==============================] - 90s 9ms/step - loss: 54.0355 - val_loss: 46.6651\n",
      "Epoch 47/2000\n",
      "10501/10501 [==============================] - 89s 9ms/step - loss: 52.4606 - val_loss: 43.0644\n",
      "Epoch 48/2000\n",
      "10501/10501 [==============================] - 89s 9ms/step - loss: 52.0718 - val_loss: 51.2809\n",
      "Epoch 49/2000\n",
      "10501/10501 [==============================] - 90s 9ms/step - loss: 52.8717 - val_loss: 43.1452\n",
      "Epoch 50/2000\n",
      "10501/10501 [==============================] - 90s 9ms/step - loss: 52.1102 - val_loss: 43.1016\n",
      "Epoch 51/2000\n",
      "10501/10501 [==============================] - 89s 8ms/step - loss: 52.3495 - val_loss: 47.6806\n",
      "Epoch 52/2000\n",
      "10501/10501 [==============================] - 89s 8ms/step - loss: 53.0779 - val_loss: 43.9418\n",
      "Epoch 53/2000\n",
      "10501/10501 [==============================] - 89s 8ms/step - loss: 52.1451 - val_loss: 46.2969\n",
      "Epoch 54/2000\n",
      "10501/10501 [==============================] - 91s 9ms/step - loss: 50.4934 - val_loss: 41.6778\n",
      "Epoch 55/2000\n",
      "10501/10501 [==============================] - 93s 9ms/step - loss: 52.8251 - val_loss: 48.0474\n",
      "Epoch 56/2000\n",
      "10501/10501 [==============================] - 91s 9ms/step - loss: 50.3935 - val_loss: 44.7617\n",
      "Epoch 57/2000\n",
      "10501/10501 [==============================] - 89s 9ms/step - loss: 50.5380 - val_loss: 43.6516\n",
      "Epoch 58/2000\n",
      "10501/10501 [==============================] - 89s 9ms/step - loss: 50.1089 - val_loss: 43.2607\n",
      "Epoch 59/2000\n",
      "10501/10501 [==============================] - 91s 9ms/step - loss: 52.1252 - val_loss: 53.6049\n",
      "Epoch 60/2000\n",
      "10501/10501 [==============================] - 89s 8ms/step - loss: 53.0363 - val_loss: 44.5708\n",
      "Epoch 61/2000\n",
      "10501/10501 [==============================] - 89s 8ms/step - loss: 50.4906 - val_loss: 42.3182\n",
      "Epoch 62/2000\n",
      "10501/10501 [==============================] - 90s 9ms/step - loss: 52.0334 - val_loss: 43.7998\n",
      "Epoch 63/2000\n",
      "10501/10501 [==============================] - 91s 9ms/step - loss: 51.3015 - val_loss: 48.1291\n",
      "Epoch 64/2000\n",
      "10501/10501 [==============================] - 93s 9ms/step - loss: 50.1644 - val_loss: 42.0130\n",
      "Epoch 65/2000\n",
      "10501/10501 [==============================] - 90s 9ms/step - loss: 49.7732 - val_loss: 43.2742\n",
      "Epoch 66/2000\n",
      "10501/10501 [==============================] - 92s 9ms/step - loss: 49.4909 - val_loss: 44.7911\n",
      "Epoch 67/2000\n",
      "10501/10501 [==============================] - 89s 9ms/step - loss: 50.5382 - val_loss: 42.0358\n",
      "Epoch 68/2000\n",
      "10501/10501 [==============================] - 90s 9ms/step - loss: 49.6939 - val_loss: 47.5354\n",
      "Epoch 69/2000\n",
      "10501/10501 [==============================] - 88s 8ms/step - loss: 51.1409 - val_loss: 44.7675\n",
      "Epoch 70/2000\n",
      "10501/10501 [==============================] - 89s 8ms/step - loss: 50.3622 - val_loss: 50.5028\n",
      "Epoch 71/2000\n",
      "10501/10501 [==============================] - 91s 9ms/step - loss: 50.5504 - val_loss: 51.1589\n",
      "Epoch 72/2000\n",
      "10501/10501 [==============================] - 91s 9ms/step - loss: 49.1845 - val_loss: 42.4067\n",
      "Epoch 73/2000\n",
      "10501/10501 [==============================] - 91s 9ms/step - loss: 49.1511 - val_loss: 51.7734\n",
      "Epoch 74/2000\n",
      "10501/10501 [==============================] - 89s 9ms/step - loss: 50.0528 - val_loss: 44.9592\n",
      "Currnent time step:12, lag:12\n",
      "Train on 10498 samples, validate on 3499 samples\n",
      "Epoch 1/2000\n",
      "10498/10498 [==============================] - 94s 9ms/step - loss: 872.8284 - val_loss: 238.2121\n",
      "Epoch 2/2000\n",
      "10498/10498 [==============================] - 89s 8ms/step - loss: 818.1770 - val_loss: 199.6006\n",
      "Epoch 3/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 773.1585 - val_loss: 322.3664\n",
      "Epoch 4/2000\n",
      "10498/10498 [==============================] - 89s 8ms/step - loss: 731.9193 - val_loss: 477.7481\n",
      "Epoch 5/2000\n",
      "10498/10498 [==============================] - 89s 8ms/step - loss: 690.7254 - val_loss: 397.3550\n",
      "Epoch 6/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 648.1416 - val_loss: 317.2617\n",
      "Epoch 7/2000\n",
      "10498/10498 [==============================] - 88s 8ms/step - loss: 605.8854 - val_loss: 474.1530\n",
      "Epoch 8/2000\n",
      "10498/10498 [==============================] - 89s 8ms/step - loss: 563.1810 - val_loss: 387.8701\n",
      "Epoch 9/2000\n",
      "10498/10498 [==============================] - 91s 9ms/step - loss: 517.3820 - val_loss: 293.8998\n",
      "Epoch 10/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 470.1706 - val_loss: 330.4019\n",
      "Epoch 11/2000\n",
      "10498/10498 [==============================] - 91s 9ms/step - loss: 423.9367 - val_loss: 407.7974\n",
      "Epoch 12/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 383.5367 - val_loss: 391.2444\n",
      "Epoch 13/2000\n",
      "10498/10498 [==============================] - 92s 9ms/step - loss: 346.9861 - val_loss: 386.8629\n",
      "Epoch 14/2000\n",
      "10498/10498 [==============================] - 92s 9ms/step - loss: 311.3676 - val_loss: 356.0644\n",
      "Epoch 15/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10498/10498 [==============================] - 91s 9ms/step - loss: 280.9284 - val_loss: 303.6107\n",
      "Epoch 16/2000\n",
      "10498/10498 [==============================] - 91s 9ms/step - loss: 252.8077 - val_loss: 240.4625\n",
      "Epoch 17/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 231.0406 - val_loss: 275.0296\n",
      "Epoch 18/2000\n",
      "10498/10498 [==============================] - 89s 9ms/step - loss: 210.4003 - val_loss: 181.5239\n",
      "Epoch 19/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 191.2251 - val_loss: 190.0142\n",
      "Epoch 20/2000\n",
      "10498/10498 [==============================] - 91s 9ms/step - loss: 174.3193 - val_loss: 211.5536\n",
      "Epoch 21/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 161.6896 - val_loss: 248.8452\n",
      "Epoch 22/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 153.6230 - val_loss: 152.5901\n",
      "Epoch 23/2000\n",
      "10498/10498 [==============================] - 92s 9ms/step - loss: 139.3278 - val_loss: 113.8932\n",
      "Epoch 24/2000\n",
      "10498/10498 [==============================] - 91s 9ms/step - loss: 124.9367 - val_loss: 103.6413\n",
      "Epoch 25/2000\n",
      "10498/10498 [==============================] - 89s 8ms/step - loss: 117.1736 - val_loss: 112.8186\n",
      "Epoch 26/2000\n",
      "10498/10498 [==============================] - 91s 9ms/step - loss: 111.7398 - val_loss: 105.1226\n",
      "Epoch 27/2000\n",
      "10498/10498 [==============================] - 92s 9ms/step - loss: 105.9798 - val_loss: 106.5897\n",
      "Epoch 28/2000\n",
      "10498/10498 [==============================] - 89s 8ms/step - loss: 101.1840 - val_loss: 109.1426\n",
      "Epoch 29/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 100.2433 - val_loss: 99.9982\n",
      "Epoch 30/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 101.0411 - val_loss: 97.9586\n",
      "Epoch 31/2000\n",
      "10498/10498 [==============================] - 92s 9ms/step - loss: 99.2403 - val_loss: 89.4507\n",
      "Epoch 32/2000\n",
      "10498/10498 [==============================] - 92s 9ms/step - loss: 97.2472 - val_loss: 94.3177\n",
      "Epoch 33/2000\n",
      "10498/10498 [==============================] - 92s 9ms/step - loss: 96.9860 - val_loss: 79.1776\n",
      "Epoch 34/2000\n",
      "10498/10498 [==============================] - 91s 9ms/step - loss: 95.5247 - val_loss: 83.5764\n",
      "Epoch 35/2000\n",
      "10498/10498 [==============================] - 92s 9ms/step - loss: 94.6251 - val_loss: 82.8373\n",
      "Epoch 36/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 96.6851 - val_loss: 83.1328\n",
      "Epoch 37/2000\n",
      "10498/10498 [==============================] - 91s 9ms/step - loss: 92.3682 - val_loss: 83.2922\n",
      "Epoch 38/2000\n",
      "10498/10498 [==============================] - 89s 8ms/step - loss: 92.6136 - val_loss: 93.7058\n",
      "Epoch 39/2000\n",
      "10498/10498 [==============================] - 89s 9ms/step - loss: 89.8404 - val_loss: 84.2041\n",
      "Epoch 40/2000\n",
      "10498/10498 [==============================] - 92s 9ms/step - loss: 92.4849 - val_loss: 78.3893\n",
      "Epoch 41/2000\n",
      "10498/10498 [==============================] - 91s 9ms/step - loss: 91.8513 - val_loss: 89.3690\n",
      "Epoch 42/2000\n",
      "10498/10498 [==============================] - 92s 9ms/step - loss: 90.4712 - val_loss: 78.5385\n",
      "Epoch 43/2000\n",
      "10498/10498 [==============================] - 88s 8ms/step - loss: 89.1292 - val_loss: 86.6540\n",
      "Epoch 44/2000\n",
      "10498/10498 [==============================] - 91s 9ms/step - loss: 89.0463 - val_loss: 76.8224\n",
      "Epoch 45/2000\n",
      "10498/10498 [==============================] - 89s 9ms/step - loss: 89.0204 - val_loss: 87.7821\n",
      "Epoch 46/2000\n",
      "10498/10498 [==============================] - 93s 9ms/step - loss: 89.0400 - val_loss: 77.3987\n",
      "Epoch 47/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 86.9881 - val_loss: 81.7805\n",
      "Epoch 48/2000\n",
      "10498/10498 [==============================] - 93s 9ms/step - loss: 89.1859 - val_loss: 85.5943\n",
      "Epoch 49/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 87.4407 - val_loss: 82.7766\n",
      "Epoch 50/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 86.7482 - val_loss: 81.3301\n",
      "Epoch 51/2000\n",
      "10498/10498 [==============================] - 91s 9ms/step - loss: 84.9141 - val_loss: 75.5588\n",
      "Epoch 52/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 85.9311 - val_loss: 78.1413\n",
      "Epoch 53/2000\n",
      "10498/10498 [==============================] - 92s 9ms/step - loss: 86.4933 - val_loss: 77.6863\n",
      "Epoch 54/2000\n",
      "10498/10498 [==============================] - 92s 9ms/step - loss: 85.7432 - val_loss: 79.2744\n",
      "Epoch 55/2000\n",
      "10498/10498 [==============================] - 91s 9ms/step - loss: 88.5084 - val_loss: 78.7430\n",
      "Epoch 56/2000\n",
      "10498/10498 [==============================] - 92s 9ms/step - loss: 86.5351 - val_loss: 84.4372\n",
      "Epoch 57/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 87.9782 - val_loss: 82.2022\n",
      "Epoch 58/2000\n",
      "10498/10498 [==============================] - 91s 9ms/step - loss: 85.0003 - val_loss: 79.8481\n",
      "Epoch 59/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 85.4170 - val_loss: 82.3809\n",
      "Epoch 60/2000\n",
      "10498/10498 [==============================] - 92s 9ms/step - loss: 84.0171 - val_loss: 85.2266\n",
      "Epoch 61/2000\n",
      "10498/10498 [==============================] - 91s 9ms/step - loss: 83.1607 - val_loss: 89.1099\n",
      "Epoch 62/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 84.3359 - val_loss: 81.7016\n",
      "Epoch 63/2000\n",
      "10498/10498 [==============================] - 91s 9ms/step - loss: 83.1242 - val_loss: 82.2591\n",
      "Epoch 64/2000\n",
      "10498/10498 [==============================] - 91s 9ms/step - loss: 82.6992 - val_loss: 79.1957\n",
      "Epoch 65/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 83.7912 - val_loss: 81.5355\n",
      "Epoch 66/2000\n",
      "10498/10498 [==============================] - 92s 9ms/step - loss: 82.9230 - val_loss: 80.6022\n",
      "Epoch 67/2000\n",
      "10498/10498 [==============================] - 92s 9ms/step - loss: 81.7264 - val_loss: 80.7881\n",
      "Epoch 68/2000\n",
      "10498/10498 [==============================] - 91s 9ms/step - loss: 79.9020 - val_loss: 78.2881\n",
      "Epoch 69/2000\n",
      "10498/10498 [==============================] - 90s 9ms/step - loss: 80.7805 - val_loss: 80.7221\n",
      "Epoch 70/2000\n",
      "10498/10498 [==============================] - 89s 8ms/step - loss: 80.3287 - val_loss: 80.2197\n",
      "Epoch 71/2000\n",
      "10498/10498 [==============================] - 89s 8ms/step - loss: 80.6459 - val_loss: 80.7423\n",
      "Currnent time step:12, lag:24\n",
      "Train on 10491 samples, validate on 3497 samples\n",
      "Epoch 1/2000\n",
      "10491/10491 [==============================] - 95s 9ms/step - loss: 870.9078 - val_loss: 292.2775\n",
      "Epoch 2/2000\n",
      "10491/10491 [==============================] - 89s 8ms/step - loss: 825.6547 - val_loss: 375.5275\n",
      "Epoch 3/2000\n",
      "10491/10491 [==============================] - 89s 8ms/step - loss: 777.9315 - val_loss: 352.0137\n",
      "Epoch 4/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 729.2778 - val_loss: 216.2765\n",
      "Epoch 5/2000\n",
      "10491/10491 [==============================] - 88s 8ms/step - loss: 680.2679 - val_loss: 373.7004\n",
      "Epoch 6/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 625.9109 - val_loss: 536.6015\n",
      "Epoch 7/2000\n",
      "10491/10491 [==============================] - 89s 9ms/step - loss: 575.0436 - val_loss: 401.7306\n",
      "Epoch 8/2000\n",
      "10491/10491 [==============================] - 91s 9ms/step - loss: 526.2214 - val_loss: 393.0717\n",
      "Epoch 9/2000\n",
      "10491/10491 [==============================] - 92s 9ms/step - loss: 479.2149 - val_loss: 434.7424\n",
      "Epoch 10/2000\n",
      "10491/10491 [==============================] - 89s 8ms/step - loss: 438.0867 - val_loss: 429.5909\n",
      "Epoch 11/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 401.0888 - val_loss: 325.4120\n",
      "Epoch 12/2000\n",
      "10491/10491 [==============================] - 91s 9ms/step - loss: 369.6196 - val_loss: 301.8515\n",
      "Epoch 13/2000\n",
      "10491/10491 [==============================] - 89s 9ms/step - loss: 337.7964 - val_loss: 282.6896\n",
      "Epoch 14/2000\n",
      "10491/10491 [==============================] - 92s 9ms/step - loss: 310.2552 - val_loss: 490.9829\n",
      "Epoch 15/2000\n",
      "10491/10491 [==============================] - 91s 9ms/step - loss: 282.6769 - val_loss: 376.5044\n",
      "Epoch 16/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 254.5080 - val_loss: 278.0487\n",
      "Epoch 17/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 230.8830 - val_loss: 244.8605\n",
      "Epoch 18/2000\n",
      "10491/10491 [==============================] - 91s 9ms/step - loss: 211.7576 - val_loss: 232.0548\n",
      "Epoch 19/2000\n",
      "10491/10491 [==============================] - 91s 9ms/step - loss: 193.9245 - val_loss: 216.5034\n",
      "Epoch 20/2000\n",
      "10491/10491 [==============================] - 89s 9ms/step - loss: 183.7273 - val_loss: 228.7595\n",
      "Epoch 21/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 173.5940 - val_loss: 160.9856\n",
      "Epoch 22/2000\n",
      "10491/10491 [==============================] - 91s 9ms/step - loss: 166.6428 - val_loss: 159.5759\n",
      "Epoch 23/2000\n",
      "10491/10491 [==============================] - 91s 9ms/step - loss: 161.7767 - val_loss: 172.7510\n",
      "Epoch 24/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 157.4768 - val_loss: 141.0624\n",
      "Epoch 25/2000\n",
      "10491/10491 [==============================] - 91s 9ms/step - loss: 152.5582 - val_loss: 143.6824\n",
      "Epoch 26/2000\n",
      "10491/10491 [==============================] - 89s 9ms/step - loss: 152.2736 - val_loss: 123.2990\n",
      "Epoch 27/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 151.8577 - val_loss: 127.5275\n",
      "Epoch 28/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 149.9616 - val_loss: 125.0676\n",
      "Epoch 29/2000\n",
      "10491/10491 [==============================] - 89s 9ms/step - loss: 147.4754 - val_loss: 127.1456\n",
      "Epoch 30/2000\n",
      "10491/10491 [==============================] - 89s 9ms/step - loss: 145.3382 - val_loss: 126.3289\n",
      "Epoch 31/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 144.7363 - val_loss: 114.9314\n",
      "Epoch 32/2000\n",
      "10491/10491 [==============================] - 91s 9ms/step - loss: 142.9499 - val_loss: 117.4312\n",
      "Epoch 33/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 144.5482 - val_loss: 115.5948\n",
      "Epoch 34/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 143.9109 - val_loss: 111.1893\n",
      "Epoch 35/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 139.8305 - val_loss: 115.4917\n",
      "Epoch 36/2000\n",
      "10491/10491 [==============================] - 89s 9ms/step - loss: 140.6379 - val_loss: 114.1517\n",
      "Epoch 37/2000\n",
      "10491/10491 [==============================] - 89s 8ms/step - loss: 138.9895 - val_loss: 114.2124\n",
      "Epoch 38/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 137.5986 - val_loss: 111.9899\n",
      "Epoch 39/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 137.5494 - val_loss: 115.1243\n",
      "Epoch 40/2000\n",
      "10491/10491 [==============================] - 91s 9ms/step - loss: 138.8104 - val_loss: 123.3579\n",
      "Epoch 41/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 138.7834 - val_loss: 119.5794\n",
      "Epoch 42/2000\n",
      "10491/10491 [==============================] - 91s 9ms/step - loss: 139.1332 - val_loss: 119.9734\n",
      "Epoch 43/2000\n",
      "10491/10491 [==============================] - 92s 9ms/step - loss: 138.0544 - val_loss: 116.3172\n",
      "Epoch 44/2000\n",
      "10491/10491 [==============================] - 91s 9ms/step - loss: 138.4821 - val_loss: 118.1859\n",
      "Epoch 45/2000\n",
      "10491/10491 [==============================] - 91s 9ms/step - loss: 135.2428 - val_loss: 119.5292\n",
      "Epoch 46/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 134.9021 - val_loss: 123.0257\n",
      "Epoch 47/2000\n",
      "10491/10491 [==============================] - 91s 9ms/step - loss: 133.5372 - val_loss: 114.0642\n",
      "Epoch 48/2000\n",
      "10491/10491 [==============================] - 91s 9ms/step - loss: 134.0290 - val_loss: 120.1428\n",
      "Epoch 49/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 132.3795 - val_loss: 113.0839\n",
      "Epoch 50/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 131.5137 - val_loss: 118.5696\n",
      "Epoch 51/2000\n",
      "10491/10491 [==============================] - 89s 8ms/step - loss: 132.4573 - val_loss: 117.3649\n",
      "Epoch 52/2000\n",
      "10491/10491 [==============================] - 91s 9ms/step - loss: 130.5497 - val_loss: 124.3030\n",
      "Epoch 53/2000\n",
      "10491/10491 [==============================] - 90s 9ms/step - loss: 129.7129 - val_loss: 120.9557\n",
      "Epoch 54/2000\n",
      "10491/10491 [==============================] - 89s 8ms/step - loss: 127.3804 - val_loss: 127.6708\n",
      "Currnent time step:24, lag:1\n",
      "Train on 10497 samples, validate on 3499 samples\n",
      "Epoch 1/2000\n",
      "10497/10497 [==============================] - 114s 11ms/step - loss: 889.8457 - val_loss: 157.9302\n",
      "Epoch 2/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 851.7430 - val_loss: 95.7948\n",
      "Epoch 3/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 820.6388 - val_loss: 145.2472\n",
      "Epoch 4/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 784.7891 - val_loss: 149.6805\n",
      "Epoch 5/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 738.4661 - val_loss: 223.2397\n",
      "Epoch 6/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 692.4956 - val_loss: 234.6709\n",
      "Epoch 7/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 646.5534 - val_loss: 324.5934\n",
      "Epoch 8/2000\n",
      "10497/10497 [==============================] - 110s 11ms/step - loss: 600.1176 - val_loss: 317.9646\n",
      "Epoch 9/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 553.8261 - val_loss: 288.1007\n",
      "Epoch 10/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 507.4691 - val_loss: 316.2903\n",
      "Epoch 11/2000\n",
      "10497/10497 [==============================] - 112s 11ms/step - loss: 461.3640 - val_loss: 284.0585\n",
      "Epoch 12/2000\n",
      "10497/10497 [==============================] - 113s 11ms/step - loss: 415.4703 - val_loss: 293.0555\n",
      "Epoch 13/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 371.9242 - val_loss: 237.5059\n",
      "Epoch 14/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 329.9486 - val_loss: 243.4129\n",
      "Epoch 15/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 289.1786 - val_loss: 205.0870\n",
      "Epoch 16/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 250.5537 - val_loss: 231.8900\n",
      "Epoch 17/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 215.6786 - val_loss: 197.4697\n",
      "Epoch 18/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 184.5652 - val_loss: 211.7871\n",
      "Epoch 19/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 157.0215 - val_loss: 186.8757\n",
      "Epoch 20/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 134.2648 - val_loss: 149.2592\n",
      "Epoch 21/2000\n",
      "10497/10497 [==============================] - 106s 10ms/step - loss: 113.6982 - val_loss: 161.1059\n",
      "Epoch 22/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 97.1317 - val_loss: 170.1071\n",
      "Currnent time step:24, lag:2\n",
      "Train on 10497 samples, validate on 3499 samples\n",
      "Epoch 1/2000\n",
      "10497/10497 [==============================] - 115s 11ms/step - loss: 852.6093 - val_loss: 3684.5562\n",
      "Epoch 2/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 797.9836 - val_loss: 1659.4133\n",
      "Epoch 3/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 743.7192 - val_loss: 226.2696\n",
      "Epoch 4/2000\n",
      "10497/10497 [==============================] - 110s 11ms/step - loss: 685.3465 - val_loss: 136.0223\n",
      "Epoch 5/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 622.9181 - val_loss: 209.1499\n",
      "Epoch 6/2000\n",
      "10497/10497 [==============================] - 110s 11ms/step - loss: 562.2348 - val_loss: 277.3295\n",
      "Epoch 7/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 505.7530 - val_loss: 257.1857\n",
      "Epoch 8/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 453.2920 - val_loss: 161.2023\n",
      "Epoch 9/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 401.8348 - val_loss: 185.9984\n",
      "Epoch 10/2000\n",
      "10497/10497 [==============================] - 107s 10ms/step - loss: 354.8422 - val_loss: 226.5413\n",
      "Epoch 11/2000\n",
      "10497/10497 [==============================] - 110s 11ms/step - loss: 311.5064 - val_loss: 227.2163\n",
      "Epoch 12/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10497/10497 [==============================] - 110s 10ms/step - loss: 272.0615 - val_loss: 169.8567\n",
      "Epoch 13/2000\n",
      "10497/10497 [==============================] - 112s 11ms/step - loss: 237.0556 - val_loss: 193.8523\n",
      "Epoch 14/2000\n",
      "10497/10497 [==============================] - 108s 10ms/step - loss: 205.0256 - val_loss: 176.2196\n",
      "Epoch 15/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 177.7011 - val_loss: 188.7412\n",
      "Epoch 16/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 154.9494 - val_loss: 230.8015\n",
      "Epoch 17/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 135.4540 - val_loss: 110.0435\n",
      "Epoch 18/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 116.6199 - val_loss: 163.9212\n",
      "Epoch 19/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 99.8432 - val_loss: 138.2038\n",
      "Epoch 20/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 82.8628 - val_loss: 136.5828\n",
      "Epoch 21/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 68.3860 - val_loss: 158.9449\n",
      "Epoch 22/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 58.3928 - val_loss: 133.3704\n",
      "Epoch 23/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 50.3436 - val_loss: 68.8457\n",
      "Epoch 24/2000\n",
      "10497/10497 [==============================] - 110s 11ms/step - loss: 45.5925 - val_loss: 70.8475\n",
      "Epoch 25/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 39.9399 - val_loss: 77.2245\n",
      "Epoch 26/2000\n",
      "10497/10497 [==============================] - 112s 11ms/step - loss: 36.0633 - val_loss: 57.6287\n",
      "Epoch 27/2000\n",
      "10497/10497 [==============================] - 110s 11ms/step - loss: 34.4407 - val_loss: 62.9991\n",
      "Epoch 28/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 32.6281 - val_loss: 50.6415\n",
      "Epoch 29/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 30.8882 - val_loss: 80.7763\n",
      "Epoch 30/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 29.8240 - val_loss: 45.8044\n",
      "Epoch 31/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 28.9294 - val_loss: 20.6680\n",
      "Epoch 32/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 28.3911 - val_loss: 34.7516\n",
      "Epoch 33/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 27.8490 - val_loss: 35.2833\n",
      "Epoch 34/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 26.5691 - val_loss: 35.1722\n",
      "Epoch 35/2000\n",
      "10497/10497 [==============================] - 110s 11ms/step - loss: 27.7323 - val_loss: 21.3104\n",
      "Epoch 36/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 27.5293 - val_loss: 29.9132\n",
      "Epoch 37/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 26.8930 - val_loss: 22.8504\n",
      "Epoch 38/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 26.1226 - val_loss: 31.7801\n",
      "Epoch 39/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 26.7588 - val_loss: 25.0119\n",
      "Epoch 40/2000\n",
      "10497/10497 [==============================] - 112s 11ms/step - loss: 26.3048 - val_loss: 19.0273\n",
      "Epoch 41/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 25.4928 - val_loss: 26.9381\n",
      "Epoch 42/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 25.0467 - val_loss: 27.0704\n",
      "Epoch 43/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 24.5706 - val_loss: 23.4048\n",
      "Epoch 44/2000\n",
      "10497/10497 [==============================] - 112s 11ms/step - loss: 25.0059 - val_loss: 19.2587\n",
      "Epoch 45/2000\n",
      "10497/10497 [==============================] - 108s 10ms/step - loss: 24.9871 - val_loss: 22.0552\n",
      "Epoch 46/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 24.5335 - val_loss: 19.2442\n",
      "Epoch 47/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 26.7905 - val_loss: 27.1036\n",
      "Epoch 48/2000\n",
      "10497/10497 [==============================] - 107s 10ms/step - loss: 25.6853 - val_loss: 24.6075\n",
      "Epoch 49/2000\n",
      "10497/10497 [==============================] - 107s 10ms/step - loss: 25.1776 - val_loss: 24.0910\n",
      "Epoch 50/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 24.3171 - val_loss: 21.9110\n",
      "Epoch 51/2000\n",
      "10497/10497 [==============================] - 112s 11ms/step - loss: 25.0982 - val_loss: 18.1004\n",
      "Epoch 52/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 23.7153 - val_loss: 19.6074\n",
      "Epoch 53/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 23.9577 - val_loss: 19.1813\n",
      "Epoch 54/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 23.6802 - val_loss: 17.5047\n",
      "Epoch 55/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 24.0097 - val_loss: 17.8863\n",
      "Epoch 56/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 23.8352 - val_loss: 16.8334\n",
      "Epoch 57/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 23.8517 - val_loss: 28.8340\n",
      "Epoch 58/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 23.7053 - val_loss: 16.9619\n",
      "Epoch 59/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 23.2227 - val_loss: 16.9658\n",
      "Epoch 60/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 23.6979 - val_loss: 19.1456\n",
      "Epoch 61/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 23.9256 - val_loss: 17.2736\n",
      "Epoch 62/2000\n",
      "10497/10497 [==============================] - 110s 11ms/step - loss: 23.0705 - val_loss: 21.5267\n",
      "Epoch 63/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 22.9152 - val_loss: 19.6960\n",
      "Epoch 64/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 22.8350 - val_loss: 16.5830\n",
      "Epoch 65/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 22.8453 - val_loss: 15.5610\n",
      "Epoch 66/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 22.7272 - val_loss: 19.8592\n",
      "Epoch 67/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 22.5177 - val_loss: 19.3113\n",
      "Epoch 68/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 23.3236 - val_loss: 17.2712\n",
      "Epoch 69/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 22.5453 - val_loss: 22.2755\n",
      "Epoch 70/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 22.6929 - val_loss: 18.3467\n",
      "Epoch 71/2000\n",
      "10497/10497 [==============================] - 108s 10ms/step - loss: 22.3740 - val_loss: 17.3553\n",
      "Epoch 72/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 21.7171 - val_loss: 16.0471\n",
      "Epoch 73/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 21.8841 - val_loss: 25.0108\n",
      "Epoch 74/2000\n",
      "10497/10497 [==============================] - 113s 11ms/step - loss: 23.2832 - val_loss: 15.5056\n",
      "Epoch 75/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 23.0731 - val_loss: 19.0863\n",
      "Epoch 76/2000\n",
      "10497/10497 [==============================] - 110s 11ms/step - loss: 22.5455 - val_loss: 15.8471\n",
      "Epoch 77/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 22.1140 - val_loss: 18.6174\n",
      "Epoch 78/2000\n",
      "10497/10497 [==============================] - 108s 10ms/step - loss: 21.8676 - val_loss: 19.4346\n",
      "Epoch 79/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 22.0466 - val_loss: 16.4748\n",
      "Epoch 80/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 22.3463 - val_loss: 16.4325\n",
      "Epoch 81/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 22.8230 - val_loss: 25.7744\n",
      "Epoch 82/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 22.9003 - val_loss: 21.6395\n",
      "Epoch 83/2000\n",
      "10497/10497 [==============================] - 108s 10ms/step - loss: 23.1937 - val_loss: 18.8512\n",
      "Epoch 84/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 21.9904 - val_loss: 16.6740\n",
      "Epoch 85/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10497/10497 [==============================] - 110s 10ms/step - loss: 21.3651 - val_loss: 15.0811\n",
      "Epoch 86/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 21.4836 - val_loss: 17.4337\n",
      "Epoch 87/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 21.7378 - val_loss: 15.4853\n",
      "Epoch 88/2000\n",
      "10497/10497 [==============================] - 107s 10ms/step - loss: 21.2606 - val_loss: 15.2410\n",
      "Epoch 89/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 21.6921 - val_loss: 16.5293\n",
      "Epoch 90/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 23.1279 - val_loss: 20.4574\n",
      "Epoch 91/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 21.8109 - val_loss: 18.2855\n",
      "Epoch 92/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 23.1937 - val_loss: 15.6505\n",
      "Epoch 93/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 21.0667 - val_loss: 15.2018\n",
      "Epoch 94/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 21.4614 - val_loss: 18.2315\n",
      "Epoch 95/2000\n",
      "10497/10497 [==============================] - 110s 11ms/step - loss: 21.5144 - val_loss: 16.0748\n",
      "Epoch 96/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 21.3171 - val_loss: 17.4353\n",
      "Epoch 97/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 21.1298 - val_loss: 17.7163\n",
      "Epoch 98/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 20.6034 - val_loss: 14.6086\n",
      "Epoch 99/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 20.6659 - val_loss: 15.5348\n",
      "Epoch 100/2000\n",
      "10497/10497 [==============================] - 110s 11ms/step - loss: 20.8361 - val_loss: 15.9084\n",
      "Epoch 101/2000\n",
      "10497/10497 [==============================] - 110s 11ms/step - loss: 21.5044 - val_loss: 17.2870\n",
      "Epoch 102/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 21.5094 - val_loss: 14.7274\n",
      "Epoch 103/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 20.7163 - val_loss: 16.4796\n",
      "Epoch 104/2000\n",
      "10497/10497 [==============================] - 112s 11ms/step - loss: 20.9923 - val_loss: 15.1892\n",
      "Epoch 105/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 20.5765 - val_loss: 16.5716\n",
      "Epoch 106/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 20.6712 - val_loss: 15.2239\n",
      "Epoch 107/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 21.1651 - val_loss: 18.2436\n",
      "Epoch 108/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 21.0590 - val_loss: 14.5635\n",
      "Epoch 109/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 20.8705 - val_loss: 20.3640\n",
      "Epoch 110/2000\n",
      "10497/10497 [==============================] - 108s 10ms/step - loss: 20.7707 - val_loss: 16.8941\n",
      "Epoch 111/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 20.3672 - val_loss: 15.7476\n",
      "Epoch 112/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 21.8587 - val_loss: 18.4997\n",
      "Epoch 113/2000\n",
      "10497/10497 [==============================] - 110s 11ms/step - loss: 20.9664 - val_loss: 14.8651\n",
      "Epoch 114/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 21.2559 - val_loss: 16.2670\n",
      "Epoch 115/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 21.5068 - val_loss: 18.4220\n",
      "Epoch 116/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 19.8730 - val_loss: 15.6463\n",
      "Epoch 117/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 19.7795 - val_loss: 14.6413\n",
      "Epoch 118/2000\n",
      "10497/10497 [==============================] - 112s 11ms/step - loss: 22.3840 - val_loss: 18.5457\n",
      "Epoch 119/2000\n",
      "10497/10497 [==============================] - 110s 11ms/step - loss: 22.0603 - val_loss: 20.3451\n",
      "Epoch 120/2000\n",
      "10497/10497 [==============================] - 112s 11ms/step - loss: 20.6900 - val_loss: 16.5272\n",
      "Epoch 121/2000\n",
      "10497/10497 [==============================] - 112s 11ms/step - loss: 20.7298 - val_loss: 18.1167\n",
      "Epoch 122/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 19.8114 - val_loss: 14.1955\n",
      "Epoch 123/2000\n",
      "10497/10497 [==============================] - 108s 10ms/step - loss: 20.1264 - val_loss: 14.7488\n",
      "Epoch 124/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 19.4150 - val_loss: 14.1010\n",
      "Epoch 125/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 20.3639 - val_loss: 17.8519\n",
      "Epoch 126/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 20.5703 - val_loss: 14.6964\n",
      "Epoch 127/2000\n",
      "10497/10497 [==============================] - 108s 10ms/step - loss: 20.3875 - val_loss: 15.2449\n",
      "Epoch 128/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 20.6505 - val_loss: 14.1316\n",
      "Epoch 129/2000\n",
      "10497/10497 [==============================] - 110s 11ms/step - loss: 20.1981 - val_loss: 15.1659\n",
      "Epoch 130/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 20.5071 - val_loss: 15.6882\n",
      "Epoch 131/2000\n",
      "10497/10497 [==============================] - 108s 10ms/step - loss: 20.0080 - val_loss: 14.8961\n",
      "Epoch 132/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 19.8315 - val_loss: 13.7762\n",
      "Epoch 133/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 19.9718 - val_loss: 14.5952\n",
      "Epoch 134/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 19.9499 - val_loss: 14.4550\n",
      "Epoch 135/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 20.4037 - val_loss: 14.9582\n",
      "Epoch 136/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 21.3892 - val_loss: 14.3675\n",
      "Epoch 137/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 19.8240 - val_loss: 14.9109\n",
      "Epoch 138/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 21.0701 - val_loss: 17.5900\n",
      "Epoch 139/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 21.2223 - val_loss: 17.9410\n",
      "Epoch 140/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 21.0777 - val_loss: 14.8818\n",
      "Epoch 141/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 19.8679 - val_loss: 15.0263\n",
      "Epoch 142/2000\n",
      "10497/10497 [==============================] - 110s 11ms/step - loss: 21.2032 - val_loss: 14.3029\n",
      "Epoch 143/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 19.8332 - val_loss: 14.5924\n",
      "Epoch 144/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 19.7829 - val_loss: 14.8232\n",
      "Epoch 145/2000\n",
      "10497/10497 [==============================] - 113s 11ms/step - loss: 19.7769 - val_loss: 14.5163\n",
      "Epoch 146/2000\n",
      "10497/10497 [==============================] - 111s 11ms/step - loss: 20.6037 - val_loss: 14.5530\n",
      "Epoch 147/2000\n",
      "10497/10497 [==============================] - 112s 11ms/step - loss: 20.2544 - val_loss: 15.2206\n",
      "Epoch 148/2000\n",
      "10497/10497 [==============================] - 112s 11ms/step - loss: 19.8690 - val_loss: 15.9398\n",
      "Epoch 149/2000\n",
      "10497/10497 [==============================] - 112s 11ms/step - loss: 20.4244 - val_loss: 17.3311\n",
      "Epoch 150/2000\n",
      "10497/10497 [==============================] - 112s 11ms/step - loss: 21.3469 - val_loss: 15.0690\n",
      "Epoch 151/2000\n",
      "10497/10497 [==============================] - 110s 10ms/step - loss: 19.8081 - val_loss: 16.7246\n",
      "Epoch 152/2000\n",
      "10497/10497 [==============================] - 109s 10ms/step - loss: 19.8567 - val_loss: 15.0079\n",
      "Currnent time step:24, lag:4\n",
      "Train on 10495 samples, validate on 3499 samples\n",
      "Epoch 1/2000\n",
      "10495/10495 [==============================] - 176s 17ms/step - loss: 894.9034 - val_loss: 182.9557\n",
      "Epoch 2/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 860.2091 - val_loss: 231.7953\n",
      "Epoch 3/2000\n",
      "10495/10495 [==============================] - 170s 16ms/step - loss: 815.1947 - val_loss: 322.0941\n",
      "Epoch 4/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 765.5476 - val_loss: 225.6726\n",
      "Epoch 5/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10495/10495 [==============================] - 168s 16ms/step - loss: 715.4779 - val_loss: 250.1819\n",
      "Epoch 6/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 665.6256 - val_loss: 343.1776\n",
      "Epoch 7/2000\n",
      "10495/10495 [==============================] - 168s 16ms/step - loss: 615.3586 - val_loss: 387.4507\n",
      "Epoch 8/2000\n",
      "10495/10495 [==============================] - 168s 16ms/step - loss: 565.3900 - val_loss: 259.3810\n",
      "Epoch 9/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 515.5478 - val_loss: 307.1666\n",
      "Epoch 10/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 465.1023 - val_loss: 290.7053\n",
      "Epoch 11/2000\n",
      "10495/10495 [==============================] - 165s 16ms/step - loss: 417.4309 - val_loss: 351.7048\n",
      "Epoch 12/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 371.8134 - val_loss: 243.4045\n",
      "Epoch 13/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 329.7925 - val_loss: 275.4598\n",
      "Epoch 14/2000\n",
      "10495/10495 [==============================] - 168s 16ms/step - loss: 288.1009 - val_loss: 229.7644\n",
      "Epoch 15/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 252.8122 - val_loss: 160.5767\n",
      "Epoch 16/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 219.5527 - val_loss: 235.4980\n",
      "Epoch 17/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 191.0039 - val_loss: 192.0479\n",
      "Epoch 18/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 164.6239 - val_loss: 240.0352\n",
      "Epoch 19/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 143.1426 - val_loss: 198.3919\n",
      "Epoch 20/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 124.0743 - val_loss: 148.7267\n",
      "Epoch 21/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 108.8408 - val_loss: 152.5440\n",
      "Epoch 22/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 95.9967 - val_loss: 164.5979\n",
      "Epoch 23/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 87.1240 - val_loss: 108.9747\n",
      "Epoch 24/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 77.8886 - val_loss: 76.5419\n",
      "Epoch 25/2000\n",
      "10495/10495 [==============================] - 168s 16ms/step - loss: 70.3320 - val_loss: 60.5851\n",
      "Epoch 26/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 64.5055 - val_loss: 45.3628\n",
      "Epoch 27/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 59.8504 - val_loss: 58.9850\n",
      "Epoch 28/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 53.2890 - val_loss: 45.4784\n",
      "Epoch 29/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 50.4314 - val_loss: 42.3507\n",
      "Epoch 30/2000\n",
      "10495/10495 [==============================] - 164s 16ms/step - loss: 49.9193 - val_loss: 70.1775\n",
      "Epoch 31/2000\n",
      "10495/10495 [==============================] - 165s 16ms/step - loss: 45.9654 - val_loss: 33.0765\n",
      "Epoch 32/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 44.2575 - val_loss: 36.3044\n",
      "Epoch 33/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 43.0083 - val_loss: 41.5548\n",
      "Epoch 34/2000\n",
      "10495/10495 [==============================] - 165s 16ms/step - loss: 42.2777 - val_loss: 33.4617\n",
      "Epoch 35/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 40.7436 - val_loss: 35.6268\n",
      "Epoch 36/2000\n",
      "10495/10495 [==============================] - 165s 16ms/step - loss: 39.8912 - val_loss: 39.6397\n",
      "Epoch 37/2000\n",
      "10495/10495 [==============================] - 168s 16ms/step - loss: 39.5592 - val_loss: 36.7563\n",
      "Epoch 38/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 41.3369 - val_loss: 35.9657\n",
      "Epoch 39/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 40.8523 - val_loss: 33.9058\n",
      "Epoch 40/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 38.9733 - val_loss: 38.7639\n",
      "Epoch 41/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 37.8513 - val_loss: 35.2249\n",
      "Epoch 42/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 37.5074 - val_loss: 34.3102\n",
      "Epoch 43/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 37.5282 - val_loss: 36.3452\n",
      "Epoch 44/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 37.6987 - val_loss: 33.5630\n",
      "Epoch 45/2000\n",
      "10495/10495 [==============================] - 168s 16ms/step - loss: 37.4395 - val_loss: 39.2575\n",
      "Epoch 46/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 36.9908 - val_loss: 32.4299\n",
      "Epoch 47/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 38.2161 - val_loss: 29.0424\n",
      "Epoch 48/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 37.1619 - val_loss: 34.7854\n",
      "Epoch 49/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 36.4189 - val_loss: 34.0654\n",
      "Epoch 50/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 37.6231 - val_loss: 40.4258\n",
      "Epoch 51/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 36.8174 - val_loss: 29.7868\n",
      "Epoch 52/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 36.3171 - val_loss: 34.5556\n",
      "Epoch 53/2000\n",
      "10495/10495 [==============================] - 165s 16ms/step - loss: 38.5049 - val_loss: 28.5228\n",
      "Epoch 54/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 36.4277 - val_loss: 30.4360\n",
      "Epoch 55/2000\n",
      "10495/10495 [==============================] - 164s 16ms/step - loss: 37.0162 - val_loss: 35.0496\n",
      "Epoch 56/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 35.5535 - val_loss: 28.8026\n",
      "Epoch 57/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 34.7435 - val_loss: 30.8334\n",
      "Epoch 58/2000\n",
      "10495/10495 [==============================] - 165s 16ms/step - loss: 35.1299 - val_loss: 29.0463\n",
      "Epoch 59/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 35.5662 - val_loss: 30.2752\n",
      "Epoch 60/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 34.8557 - val_loss: 32.5595\n",
      "Epoch 61/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 35.6304 - val_loss: 28.8136\n",
      "Epoch 62/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 37.0003 - val_loss: 53.0966\n",
      "Epoch 63/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 36.8296 - val_loss: 31.3812\n",
      "Epoch 64/2000\n",
      "10495/10495 [==============================] - 165s 16ms/step - loss: 35.5757 - val_loss: 29.5776\n",
      "Epoch 65/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 34.4718 - val_loss: 37.5658\n",
      "Epoch 66/2000\n",
      "10495/10495 [==============================] - 167s 16ms/step - loss: 34.5367 - val_loss: 32.1089\n",
      "Epoch 67/2000\n",
      "10495/10495 [==============================] - 165s 16ms/step - loss: 34.2977 - val_loss: 31.2294\n",
      "Epoch 68/2000\n",
      "10495/10495 [==============================] - 165s 16ms/step - loss: 34.3839 - val_loss: 29.7993\n",
      "Epoch 69/2000\n",
      "10495/10495 [==============================] - 166s 16ms/step - loss: 34.1054 - val_loss: 31.6252\n",
      "Epoch 70/2000\n",
      "10495/10495 [==============================] - 165s 16ms/step - loss: 34.2003 - val_loss: 32.2665\n",
      "Epoch 71/2000\n",
      "10495/10495 [==============================] - 165s 16ms/step - loss: 33.7782 - val_loss: 28.6376\n",
      "Epoch 72/2000\n",
      "10495/10495 [==============================] - 165s 16ms/step - loss: 33.7648 - val_loss: 29.7394\n",
      "Epoch 73/2000\n",
      "10495/10495 [==============================] - 165s 16ms/step - loss: 33.9114 - val_loss: 32.1823\n",
      "Currnent time step:24, lag:6\n",
      "Train on 10494 samples, validate on 3498 samples\n",
      "Epoch 1/2000\n",
      "10494/10494 [==============================] - 161s 15ms/step - loss: 895.1513 - val_loss: 216.5431\n",
      "Epoch 2/2000\n",
      "10494/10494 [==============================] - 120s 11ms/step - loss: 851.5485 - val_loss: 180.2148\n",
      "Epoch 3/2000\n",
      "10494/10494 [==============================] - 118s 11ms/step - loss: 807.9697 - val_loss: 264.2804\n",
      "Epoch 4/2000\n",
      "10494/10494 [==============================] - 116s 11ms/step - loss: 763.0538 - val_loss: 397.7363\n",
      "Epoch 5/2000\n",
      "10494/10494 [==============================] - 116s 11ms/step - loss: 717.9502 - val_loss: 312.2269\n",
      "Epoch 6/2000\n",
      "10494/10494 [==============================] - 118s 11ms/step - loss: 670.1022 - val_loss: 282.2074\n",
      "Epoch 7/2000\n",
      "10494/10494 [==============================] - 118s 11ms/step - loss: 622.3241 - val_loss: 359.4726\n",
      "Epoch 8/2000\n",
      "10494/10494 [==============================] - 119s 11ms/step - loss: 574.3022 - val_loss: 288.0850\n",
      "Epoch 9/2000\n",
      "10494/10494 [==============================] - 119s 11ms/step - loss: 526.2993 - val_loss: 358.0492\n",
      "Epoch 10/2000\n",
      "10494/10494 [==============================] - 116s 11ms/step - loss: 477.8658 - val_loss: 356.5912\n",
      "Epoch 11/2000\n",
      "10494/10494 [==============================] - 117s 11ms/step - loss: 431.8284 - val_loss: 291.9182\n",
      "Epoch 12/2000\n",
      "10494/10494 [==============================] - 118s 11ms/step - loss: 386.8125 - val_loss: 239.5530\n",
      "Epoch 13/2000\n",
      "10494/10494 [==============================] - 119s 11ms/step - loss: 346.6836 - val_loss: 229.5448\n",
      "Epoch 14/2000\n",
      "10494/10494 [==============================] - 117s 11ms/step - loss: 306.7911 - val_loss: 347.0967\n",
      "Epoch 15/2000\n",
      "10494/10494 [==============================] - 118s 11ms/step - loss: 270.4314 - val_loss: 280.0007\n",
      "Epoch 16/2000\n",
      "10494/10494 [==============================] - 117s 11ms/step - loss: 237.4678 - val_loss: 244.3508\n",
      "Epoch 17/2000\n",
      "10494/10494 [==============================] - 119s 11ms/step - loss: 210.2545 - val_loss: 284.2501\n",
      "Epoch 18/2000\n",
      "10494/10494 [==============================] - 120s 11ms/step - loss: 183.9767 - val_loss: 169.9689\n",
      "Epoch 19/2000\n",
      "10494/10494 [==============================] - 120s 11ms/step - loss: 160.1008 - val_loss: 165.9282\n",
      "Epoch 20/2000\n",
      "10494/10494 [==============================] - 121s 11ms/step - loss: 140.4273 - val_loss: 141.7123\n",
      "Epoch 21/2000\n",
      "10494/10494 [==============================] - 120s 11ms/step - loss: 127.9021 - val_loss: 99.2975\n",
      "Epoch 22/2000\n",
      "10494/10494 [==============================] - 119s 11ms/step - loss: 112.2006 - val_loss: 114.4191\n",
      "Epoch 23/2000\n",
      "10494/10494 [==============================] - 121s 12ms/step - loss: 99.8086 - val_loss: 91.5603\n",
      "Epoch 24/2000\n",
      "10494/10494 [==============================] - 121s 12ms/step - loss: 86.7486 - val_loss: 125.7313\n",
      "Epoch 25/2000\n",
      "10494/10494 [==============================] - 123s 12ms/step - loss: 79.5610 - val_loss: 98.4414\n",
      "Epoch 26/2000\n",
      "10494/10494 [==============================] - 121s 12ms/step - loss: 73.8087 - val_loss: 70.3217\n",
      "Epoch 27/2000\n",
      "10494/10494 [==============================] - 122s 12ms/step - loss: 67.5777 - val_loss: 80.9657\n",
      "Epoch 28/2000\n",
      "10494/10494 [==============================] - 120s 11ms/step - loss: 64.0948 - val_loss: 84.8340\n",
      "Epoch 29/2000\n",
      "10494/10494 [==============================] - 122s 12ms/step - loss: 62.5833 - val_loss: 69.1683\n",
      "Epoch 30/2000\n",
      "10494/10494 [==============================] - 121s 12ms/step - loss: 59.3507 - val_loss: 82.5111\n",
      "Epoch 31/2000\n",
      "10494/10494 [==============================] - 122s 12ms/step - loss: 57.4156 - val_loss: 68.4149\n",
      "Epoch 32/2000\n",
      "10494/10494 [==============================] - 123s 12ms/step - loss: 56.9293 - val_loss: 55.6849\n",
      "Epoch 33/2000\n",
      "10494/10494 [==============================] - 123s 12ms/step - loss: 56.4080 - val_loss: 89.2626\n",
      "Epoch 34/2000\n",
      "10494/10494 [==============================] - 123s 12ms/step - loss: 56.5327 - val_loss: 45.9832\n",
      "Epoch 35/2000\n",
      "10494/10494 [==============================] - 122s 12ms/step - loss: 54.6071 - val_loss: 50.6392\n",
      "Epoch 36/2000\n",
      "10494/10494 [==============================] - 126s 12ms/step - loss: 52.7374 - val_loss: 70.3134\n",
      "Epoch 37/2000\n",
      "10494/10494 [==============================] - 126s 12ms/step - loss: 51.4091 - val_loss: 52.5400\n",
      "Epoch 38/2000\n",
      "10494/10494 [==============================] - 126s 12ms/step - loss: 51.6469 - val_loss: 46.1228\n",
      "Epoch 39/2000\n",
      "10494/10494 [==============================] - 125s 12ms/step - loss: 52.0119 - val_loss: 46.3155\n",
      "Epoch 40/2000\n",
      "10494/10494 [==============================] - 127s 12ms/step - loss: 51.3086 - val_loss: 52.5050\n",
      "Epoch 41/2000\n",
      "10494/10494 [==============================] - 127s 12ms/step - loss: 51.8835 - val_loss: 57.8199\n",
      "Epoch 42/2000\n",
      "10494/10494 [==============================] - 127s 12ms/step - loss: 50.4319 - val_loss: 44.9529\n",
      "Epoch 43/2000\n",
      "10494/10494 [==============================] - 127s 12ms/step - loss: 49.9089 - val_loss: 49.6565\n",
      "Epoch 44/2000\n",
      "10494/10494 [==============================] - 125s 12ms/step - loss: 51.3603 - val_loss: 56.6118\n",
      "Epoch 45/2000\n",
      "10494/10494 [==============================] - 126s 12ms/step - loss: 49.8557 - val_loss: 56.1109\n",
      "Epoch 46/2000\n",
      "10494/10494 [==============================] - 126s 12ms/step - loss: 50.0122 - val_loss: 42.8534\n",
      "Epoch 47/2000\n",
      "10494/10494 [==============================] - 123s 12ms/step - loss: 49.7648 - val_loss: 42.7223\n",
      "Epoch 48/2000\n",
      "10494/10494 [==============================] - 124s 12ms/step - loss: 48.5165 - val_loss: 48.1359\n",
      "Epoch 49/2000\n",
      "10494/10494 [==============================] - 123s 12ms/step - loss: 49.0285 - val_loss: 44.7562\n",
      "Epoch 50/2000\n",
      "10494/10494 [==============================] - 122s 12ms/step - loss: 49.0999 - val_loss: 42.4591\n",
      "Epoch 51/2000\n",
      "10494/10494 [==============================] - 123s 12ms/step - loss: 48.4805 - val_loss: 48.6943\n",
      "Epoch 52/2000\n",
      "10494/10494 [==============================] - 123s 12ms/step - loss: 46.9824 - val_loss: 42.6219\n",
      "Epoch 53/2000\n",
      "10494/10494 [==============================] - 125s 12ms/step - loss: 50.7294 - val_loss: 42.8695\n",
      "Epoch 54/2000\n",
      "10494/10494 [==============================] - 125s 12ms/step - loss: 48.6505 - val_loss: 45.4319\n",
      "Epoch 55/2000\n",
      "10494/10494 [==============================] - 125s 12ms/step - loss: 47.0835 - val_loss: 48.4271\n",
      "Epoch 56/2000\n",
      "10494/10494 [==============================] - 125s 12ms/step - loss: 47.6585 - val_loss: 46.5103\n",
      "Epoch 57/2000\n",
      "10494/10494 [==============================] - 126s 12ms/step - loss: 48.0018 - val_loss: 45.7356\n",
      "Epoch 58/2000\n",
      "10494/10494 [==============================] - 127s 12ms/step - loss: 45.5236 - val_loss: 42.6816\n",
      "Epoch 59/2000\n",
      "10494/10494 [==============================] - 126s 12ms/step - loss: 46.5442 - val_loss: 48.5494\n",
      "Epoch 60/2000\n",
      "10494/10494 [==============================] - 124s 12ms/step - loss: 47.9295 - val_loss: 45.2668\n",
      "Epoch 61/2000\n",
      "10494/10494 [==============================] - 126s 12ms/step - loss: 47.9798 - val_loss: 45.1292\n",
      "Epoch 62/2000\n",
      "10494/10494 [==============================] - 126s 12ms/step - loss: 46.3182 - val_loss: 49.5711\n",
      "Epoch 63/2000\n",
      "10494/10494 [==============================] - 126s 12ms/step - loss: 45.9995 - val_loss: 44.2811\n",
      "Epoch 64/2000\n",
      "10494/10494 [==============================] - 125s 12ms/step - loss: 46.2886 - val_loss: 42.4479\n",
      "Epoch 65/2000\n",
      "10494/10494 [==============================] - 126s 12ms/step - loss: 44.2011 - val_loss: 48.0869\n",
      "Epoch 66/2000\n",
      "10494/10494 [==============================] - 129s 12ms/step - loss: 44.6027 - val_loss: 45.4212\n",
      "Epoch 67/2000\n",
      "10494/10494 [==============================] - 126s 12ms/step - loss: 45.3799 - val_loss: 47.3104\n",
      "Epoch 68/2000\n",
      "10494/10494 [==============================] - 126s 12ms/step - loss: 44.7832 - val_loss: 43.6090\n",
      "Epoch 69/2000\n",
      "10494/10494 [==============================] - 125s 12ms/step - loss: 45.2130 - val_loss: 41.9484\n",
      "Epoch 70/2000\n",
      "10494/10494 [==============================] - 123s 12ms/step - loss: 45.1854 - val_loss: 41.9253\n",
      "Epoch 71/2000\n",
      "10494/10494 [==============================] - 123s 12ms/step - loss: 45.0749 - val_loss: 40.9562\n",
      "Epoch 72/2000\n",
      "10494/10494 [==============================] - 126s 12ms/step - loss: 44.6569 - val_loss: 41.7361\n",
      "Epoch 73/2000\n",
      "10494/10494 [==============================] - 128s 12ms/step - loss: 44.6927 - val_loss: 47.3276\n",
      "Epoch 74/2000\n",
      "10494/10494 [==============================] - 126s 12ms/step - loss: 44.2451 - val_loss: 42.7762\n",
      "Epoch 75/2000\n",
      "10494/10494 [==============================] - 126s 12ms/step - loss: 46.0325 - val_loss: 47.4010\n",
      "Epoch 76/2000\n",
      "10494/10494 [==============================] - 123s 12ms/step - loss: 47.9537 - val_loss: 42.5365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/2000\n",
      "10494/10494 [==============================] - 123s 12ms/step - loss: 44.8348 - val_loss: 40.9226\n",
      "Epoch 78/2000\n",
      "10494/10494 [==============================] - 125s 12ms/step - loss: 44.0679 - val_loss: 41.3791\n",
      "Epoch 79/2000\n",
      "10494/10494 [==============================] - 127s 12ms/step - loss: 44.2060 - val_loss: 45.1991\n",
      "Epoch 80/2000\n",
      "10494/10494 [==============================] - 128s 12ms/step - loss: 44.6813 - val_loss: 40.6306\n",
      "Epoch 81/2000\n",
      "10494/10494 [==============================] - 125s 12ms/step - loss: 42.7744 - val_loss: 41.5065\n",
      "Epoch 82/2000\n",
      "10494/10494 [==============================] - 122s 12ms/step - loss: 43.0324 - val_loss: 43.9552\n",
      "Epoch 83/2000\n",
      "10494/10494 [==============================] - 124s 12ms/step - loss: 43.2275 - val_loss: 45.9822\n",
      "Epoch 84/2000\n",
      "10494/10494 [==============================] - 123s 12ms/step - loss: 42.6772 - val_loss: 46.0189\n",
      "Epoch 85/2000\n",
      "10494/10494 [==============================] - 124s 12ms/step - loss: 43.3995 - val_loss: 42.1398\n",
      "Epoch 86/2000\n",
      "10494/10494 [==============================] - 126s 12ms/step - loss: 42.4990 - val_loss: 41.9322\n",
      "Epoch 87/2000\n",
      "10494/10494 [==============================] - 126s 12ms/step - loss: 42.9232 - val_loss: 42.3114\n",
      "Epoch 88/2000\n",
      "10494/10494 [==============================] - 123s 12ms/step - loss: 42.3284 - val_loss: 45.1822\n",
      "Epoch 89/2000\n",
      "10494/10494 [==============================] - 127s 12ms/step - loss: 43.1494 - val_loss: 43.4968\n",
      "Epoch 90/2000\n",
      "10494/10494 [==============================] - 127s 12ms/step - loss: 42.2219 - val_loss: 42.3145\n",
      "Epoch 91/2000\n",
      "10494/10494 [==============================] - 124s 12ms/step - loss: 42.2447 - val_loss: 42.8229\n",
      "Epoch 92/2000\n",
      "10494/10494 [==============================] - 127s 12ms/step - loss: 42.5780 - val_loss: 42.9343\n",
      "Epoch 93/2000\n",
      "10494/10494 [==============================] - 125s 12ms/step - loss: 42.6005 - val_loss: 42.6481\n",
      "Epoch 94/2000\n",
      "10494/10494 [==============================] - 128s 12ms/step - loss: 42.6056 - val_loss: 41.7347\n",
      "Epoch 95/2000\n",
      "10494/10494 [==============================] - 128s 12ms/step - loss: 41.7572 - val_loss: 42.9960\n",
      "Epoch 96/2000\n",
      "10494/10494 [==============================] - 125s 12ms/step - loss: 41.5187 - val_loss: 40.8050\n",
      "Epoch 97/2000\n",
      "10494/10494 [==============================] - 128s 12ms/step - loss: 41.3336 - val_loss: 43.3576\n",
      "Epoch 98/2000\n",
      "10494/10494 [==============================] - 127s 12ms/step - loss: 41.9786 - val_loss: 44.0895\n",
      "Epoch 99/2000\n",
      "10494/10494 [==============================] - 128s 12ms/step - loss: 42.9180 - val_loss: 45.8192\n",
      "Epoch 100/2000\n",
      "10494/10494 [==============================] - 124s 12ms/step - loss: 40.9080 - val_loss: 42.1379\n",
      "Currnent time step:24, lag:12\n",
      "Train on 10491 samples, validate on 3497 samples\n",
      "Epoch 1/2000\n",
      "10491/10491 [==============================] - 143s 14ms/step - loss: 905.0747 - val_loss: 337.6042\n",
      "Epoch 2/2000\n",
      "10491/10491 [==============================] - 125s 12ms/step - loss: 874.5009 - val_loss: 239.9318\n",
      "Epoch 3/2000\n",
      "10491/10491 [==============================] - 106s 10ms/step - loss: 841.1663 - val_loss: 417.9929\n",
      "Epoch 4/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 797.3255 - val_loss: 433.3685\n",
      "Epoch 5/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 744.4019 - val_loss: 367.5134\n",
      "Epoch 6/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 690.3683 - val_loss: 215.0850\n",
      "Epoch 7/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 624.6049 - val_loss: 216.2259\n",
      "Epoch 8/2000\n",
      "10491/10491 [==============================] - 104s 10ms/step - loss: 564.7413 - val_loss: 355.1835\n",
      "Epoch 9/2000\n",
      "10491/10491 [==============================] - 104s 10ms/step - loss: 510.7334 - val_loss: 338.8270\n",
      "Epoch 10/2000\n",
      "10491/10491 [==============================] - 104s 10ms/step - loss: 459.3123 - val_loss: 325.0874\n",
      "Epoch 11/2000\n",
      "10491/10491 [==============================] - 104s 10ms/step - loss: 412.1760 - val_loss: 339.6972\n",
      "Epoch 12/2000\n",
      "10491/10491 [==============================] - 104s 10ms/step - loss: 368.7904 - val_loss: 300.1408\n",
      "Epoch 13/2000\n",
      "10491/10491 [==============================] - 103s 10ms/step - loss: 329.2669 - val_loss: 254.2436\n",
      "Epoch 14/2000\n",
      "10491/10491 [==============================] - 104s 10ms/step - loss: 293.8720 - val_loss: 381.2636\n",
      "Epoch 15/2000\n",
      "10491/10491 [==============================] - 103s 10ms/step - loss: 264.8572 - val_loss: 340.7095\n",
      "Epoch 16/2000\n",
      "10491/10491 [==============================] - 104s 10ms/step - loss: 236.0406 - val_loss: 279.7961\n",
      "Epoch 17/2000\n",
      "10491/10491 [==============================] - 104s 10ms/step - loss: 214.8551 - val_loss: 198.7696\n",
      "Epoch 18/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 192.8291 - val_loss: 231.2068\n",
      "Epoch 19/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 177.8122 - val_loss: 263.1131\n",
      "Epoch 20/2000\n",
      "10491/10491 [==============================] - 104s 10ms/step - loss: 162.3886 - val_loss: 204.7954\n",
      "Epoch 21/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 156.8673 - val_loss: 154.1031\n",
      "Epoch 22/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 147.3259 - val_loss: 170.4711\n",
      "Epoch 23/2000\n",
      "10491/10491 [==============================] - 104s 10ms/step - loss: 135.4353 - val_loss: 160.5197\n",
      "Epoch 24/2000\n",
      "10491/10491 [==============================] - 104s 10ms/step - loss: 127.5191 - val_loss: 145.0322\n",
      "Epoch 25/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 121.1796 - val_loss: 153.1628\n",
      "Epoch 26/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 118.5062 - val_loss: 143.7583\n",
      "Epoch 27/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 114.1760 - val_loss: 133.9125\n",
      "Epoch 28/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 112.7540 - val_loss: 123.0843\n",
      "Epoch 29/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 107.4468 - val_loss: 112.3843\n",
      "Epoch 30/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 102.7654 - val_loss: 128.5262\n",
      "Epoch 31/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 98.8305 - val_loss: 95.9843\n",
      "Epoch 32/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 95.4008 - val_loss: 82.9564\n",
      "Epoch 33/2000\n",
      "10491/10491 [==============================] - 104s 10ms/step - loss: 91.2070 - val_loss: 80.5009\n",
      "Epoch 34/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 89.0032 - val_loss: 80.7968\n",
      "Epoch 35/2000\n",
      "10491/10491 [==============================] - 106s 10ms/step - loss: 88.6704 - val_loss: 78.3694\n",
      "Epoch 36/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 88.0246 - val_loss: 89.1208\n",
      "Epoch 37/2000\n",
      "10491/10491 [==============================] - 104s 10ms/step - loss: 86.8294 - val_loss: 82.5491\n",
      "Epoch 38/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 84.3156 - val_loss: 91.9943\n",
      "Epoch 39/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 88.7205 - val_loss: 81.7442\n",
      "Epoch 40/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 86.3664 - val_loss: 84.8830\n",
      "Epoch 41/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 85.9753 - val_loss: 83.4594\n",
      "Epoch 42/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 85.5426 - val_loss: 81.9718\n",
      "Epoch 43/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 85.7510 - val_loss: 91.4319\n",
      "Epoch 44/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 85.0408 - val_loss: 80.1003\n",
      "Epoch 45/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 83.0930 - val_loss: 78.9210\n",
      "Epoch 46/2000\n",
      "10491/10491 [==============================] - 106s 10ms/step - loss: 81.8896 - val_loss: 86.0273\n",
      "Epoch 47/2000\n",
      "10491/10491 [==============================] - 106s 10ms/step - loss: 79.7608 - val_loss: 84.3944\n",
      "Epoch 48/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 81.1521 - val_loss: 78.8647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/2000\n",
      "10491/10491 [==============================] - 104s 10ms/step - loss: 81.0712 - val_loss: 90.5323\n",
      "Epoch 50/2000\n",
      "10491/10491 [==============================] - 106s 10ms/step - loss: 80.1359 - val_loss: 87.9525\n",
      "Epoch 51/2000\n",
      "10491/10491 [==============================] - 104s 10ms/step - loss: 79.0344 - val_loss: 79.5028\n",
      "Epoch 52/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 81.2202 - val_loss: 80.1032\n",
      "Epoch 53/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 79.4570 - val_loss: 88.3464\n",
      "Epoch 54/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 77.5120 - val_loss: 79.8196\n",
      "Epoch 55/2000\n",
      "10491/10491 [==============================] - 105s 10ms/step - loss: 76.0838 - val_loss: 79.6849\n",
      "Currnent time step:24, lag:24\n",
      "Train on 10483 samples, validate on 3495 samples\n",
      "Epoch 1/2000\n",
      "10483/10483 [==============================] - 111s 11ms/step - loss: 879.0686 - val_loss: 445.0136\n",
      "Epoch 2/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 840.3559 - val_loss: 584.2228\n",
      "Epoch 3/2000\n",
      "10483/10483 [==============================] - 103s 10ms/step - loss: 808.1502 - val_loss: 472.2915\n",
      "Epoch 4/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 771.8216 - val_loss: 363.1444\n",
      "Epoch 5/2000\n",
      "10483/10483 [==============================] - 105s 10ms/step - loss: 730.1570 - val_loss: 344.5451\n",
      "Epoch 6/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 688.4425 - val_loss: 526.1466\n",
      "Epoch 7/2000\n",
      "10483/10483 [==============================] - 103s 10ms/step - loss: 647.6908 - val_loss: 463.6566\n",
      "Epoch 8/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 605.6182 - val_loss: 393.7662\n",
      "Epoch 9/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 565.9611 - val_loss: 402.3108\n",
      "Epoch 10/2000\n",
      "10483/10483 [==============================] - 105s 10ms/step - loss: 527.2691 - val_loss: 395.5126\n",
      "Epoch 11/2000\n",
      "10483/10483 [==============================] - 105s 10ms/step - loss: 490.5704 - val_loss: 359.8682\n",
      "Epoch 12/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 453.1162 - val_loss: 503.9624\n",
      "Epoch 13/2000\n",
      "10483/10483 [==============================] - 105s 10ms/step - loss: 416.5036 - val_loss: 327.1257\n",
      "Epoch 14/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 382.3701 - val_loss: 376.3587\n",
      "Epoch 15/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 352.4439 - val_loss: 454.9636\n",
      "Epoch 16/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 318.5052 - val_loss: 276.7366\n",
      "Epoch 17/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 288.8249 - val_loss: 377.5399\n",
      "Epoch 18/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 262.2104 - val_loss: 217.3958\n",
      "Epoch 19/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 236.3320 - val_loss: 161.3011\n",
      "Epoch 20/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 216.7349 - val_loss: 155.5424\n",
      "Epoch 21/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 194.2627 - val_loss: 213.3227\n",
      "Epoch 22/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 179.8441 - val_loss: 176.9853\n",
      "Epoch 23/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 168.4892 - val_loss: 191.0983\n",
      "Epoch 24/2000\n",
      "10483/10483 [==============================] - 105s 10ms/step - loss: 159.4098 - val_loss: 134.0988\n",
      "Epoch 25/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 153.1598 - val_loss: 135.1852\n",
      "Epoch 26/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 145.8545 - val_loss: 139.2891\n",
      "Epoch 27/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 141.7372 - val_loss: 133.8366\n",
      "Epoch 28/2000\n",
      "10483/10483 [==============================] - 105s 10ms/step - loss: 145.8556 - val_loss: 116.2930\n",
      "Epoch 29/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 144.8097 - val_loss: 133.1038\n",
      "Epoch 30/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 140.5215 - val_loss: 134.9013\n",
      "Epoch 31/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 137.6708 - val_loss: 123.6795\n",
      "Epoch 32/2000\n",
      "10483/10483 [==============================] - 103s 10ms/step - loss: 136.9221 - val_loss: 123.6183\n",
      "Epoch 33/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 135.3705 - val_loss: 125.0767\n",
      "Epoch 34/2000\n",
      "10483/10483 [==============================] - 106s 10ms/step - loss: 137.0265 - val_loss: 116.9846\n",
      "Epoch 35/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 133.1351 - val_loss: 121.2034\n",
      "Epoch 36/2000\n",
      "10483/10483 [==============================] - 105s 10ms/step - loss: 129.8908 - val_loss: 138.0338\n",
      "Epoch 37/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 128.0858 - val_loss: 118.8774\n",
      "Epoch 38/2000\n",
      "10483/10483 [==============================] - 103s 10ms/step - loss: 126.7783 - val_loss: 142.9178\n",
      "Epoch 39/2000\n",
      "10483/10483 [==============================] - 103s 10ms/step - loss: 128.8267 - val_loss: 121.5278\n",
      "Epoch 40/2000\n",
      "10483/10483 [==============================] - 103s 10ms/step - loss: 125.3275 - val_loss: 135.3523\n",
      "Epoch 41/2000\n",
      "10483/10483 [==============================] - 105s 10ms/step - loss: 124.4529 - val_loss: 131.2644\n",
      "Epoch 42/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 125.4681 - val_loss: 135.0759\n",
      "Epoch 43/2000\n",
      "10483/10483 [==============================] - 103s 10ms/step - loss: 122.4420 - val_loss: 121.9631\n",
      "Epoch 44/2000\n",
      "10483/10483 [==============================] - 103s 10ms/step - loss: 121.5268 - val_loss: 119.0064\n",
      "Epoch 45/2000\n",
      "10483/10483 [==============================] - 105s 10ms/step - loss: 122.4682 - val_loss: 138.1840\n",
      "Epoch 46/2000\n",
      "10483/10483 [==============================] - 103s 10ms/step - loss: 121.0381 - val_loss: 138.7149\n",
      "Epoch 47/2000\n",
      "10483/10483 [==============================] - 104s 10ms/step - loss: 119.3037 - val_loss: 124.2868\n",
      "Epoch 48/2000\n",
      "10483/10483 [==============================] - 105s 10ms/step - loss: 117.1189 - val_loss: 127.2505\n"
     ]
    }
   ],
   "source": [
    "for time_step in [1, 12, 24]:\n",
    "    for lag in [1, 2, 4, 6, 12, 24]:\n",
    "        print('Currnent time step:{0}, lag:{1}'.format(str(time_step), str(lag)))\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, mode='min')\n",
    "        mc = ModelCheckpoint('./Result/PM2.5/ICNN-MCDO/Model/timestep{0}_lag{1}.h5'.format(str(time_step), str(lag)), monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "        X_data = data\n",
    "        Y_data = data[:,:,:,:,4] # 3 = PM10, 4 = PM2.5\n",
    "        Y_data = Y_data.reshape((8760*2),1,40,28,1)\n",
    "\n",
    "        X_data, Y_data = look_back(X_data, Y_data, time_step)\n",
    "        if lag > 1:\n",
    "            X_data = X_data[0:len(X_data)-(lag-1)]\n",
    "            Y_data = Y_data[(lag-1):]\n",
    "\n",
    "        X_train, X_val, X_test = division(X_data)\n",
    "        Y_train, Y_val, Y_test = division(Y_data)\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], 40, 28, 1).astype('float32')\n",
    "        Y_val = Y_val.reshape(Y_val.shape[0],  40, 28, 1).astype('float32')\n",
    "        Y_test = Y_test.reshape(Y_test.shape[0], 40, 28, 1).astype('float32')\n",
    "        \n",
    "        model = ICNN_MCDO(time_step)\n",
    "\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        model.fit(X_train, Y_train, epochs=2000, batch_size=512, validation_data=(X_val, Y_val), callbacks=[early_stopping, mc])\n",
    "\n",
    "        del model, X_data, Y_data, X_train, X_val, X_test, Y_train, Y_val, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dsl001/anaconda3/envs/tf115/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/dsl001/anaconda3/envs/tf115/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for time_step in [1, 12, 24]:\n",
    "    for lag in [1, 2, 4, 6, 12, 24]:\n",
    "        model_src = './Result/PM2.5/ICNN-MCDO/Model/timestep{0}_lag{1}.h5'.format(time_step, lag)\n",
    "        model = load_model(model_src, custom_objects={'mish':mish})\n",
    "        \n",
    "        X_data = data\n",
    "        Y_data = data[:,:,:,:,4] # 3 = PM10, 4 = PM2.5\n",
    "        Y_data = Y_data.reshape((8760*2),1,40,28,1)\n",
    "\n",
    "        X_data, Y_data = look_back(X_data, Y_data, time_step)\n",
    "        if lag > 1:\n",
    "            X_data = X_data[0:len(X_data)-(lag-1)]\n",
    "            Y_data = Y_data[(lag-1):]\n",
    "\n",
    "        X_train, X_val, X_test = division(X_data)\n",
    "        Y_train, Y_val, Y_test = division(Y_data)\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], 40, 28, 1).astype('float32')\n",
    "        Y_val = Y_val.reshape(Y_val.shape[0],  40, 28, 1).astype('float32')\n",
    "        Y_test = Y_test.reshape(Y_test.shape[0], 40, 28, 1).astype('float32')\n",
    "        \n",
    "        Y_pred_pm25 = np.zeros((100, Y_test.shape[0],40, 28))\n",
    "        \n",
    "        for i in range(100):\n",
    "            tmp = model.predict(X_test)\n",
    "            tmp = tmp.reshape(-1, 40, 28)\n",
    "            Y_pred_pm25[i, :, :, :] = tmp\n",
    "        \n",
    "        np.save('./Result/PM2.5/ICNN-MCDO/Predict/timestep{0}_lag{1}'.format(time_step, lag), Y_pred_pm25)\n",
    "        \n",
    "        del model, X_data, Y_data, X_train, X_val, X_test, Y_train, Y_val, Y_test, Y_pred_pm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) ICNN-Dropout : PM2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_step in [1, 12, 24]:\n",
    "    for lag in [1, 2, 4, 6, 12, 24]:\n",
    "        print('Currnent time step:{0}, lag:{1}'.format(str(time_step), str(lag)))\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, mode='min')\n",
    "        mc = ModelCheckpoint('./Result/PM2.5/ICNN-Dropout/Model/timestep{0}_lag{1}.h5'.format(str(time_step), str(lag)), monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "        X_data = data\n",
    "        Y_data = data[:,:,:,:,4] # 3 = PM10, 4 = PM2.5\n",
    "        Y_data = Y_data.reshape((8760*2),1,40,28,1)\n",
    "\n",
    "        X_data, Y_data = look_back(X_data, Y_data, time_step)\n",
    "        if lag > 1:\n",
    "            X_data = X_data[0:len(X_data)-(lag-1)]\n",
    "            Y_data = Y_data[(lag-1):]\n",
    "\n",
    "        X_train, X_val, X_test = division(X_data)\n",
    "        Y_train, Y_val, Y_test = division(Y_data)\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], 40, 28, 1).astype('float32')\n",
    "        Y_val = Y_val.reshape(Y_val.shape[0],  40, 28, 1).astype('float32')\n",
    "        Y_test = Y_test.reshape(Y_test.shape[0], 40, 28, 1).astype('float32')\n",
    "        \n",
    "        model = ICNN_Dropout(time_step)\n",
    "\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        model.fit(X_train, Y_train, epochs=2000, batch_size=512, validation_data=(X_val, Y_val), callbacks=[early_stopping, mc])\n",
    "\n",
    "        del model, X_data, Y_data, X_train, X_val, X_test, Y_train, Y_val, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_step in [1, 12, 24]:\n",
    "    for lag in [1, 2, 4, 6, 12, 24]:\n",
    "        model_src = './Result/PM2.5/ICNN-Dropout/Model/timestep{0}_lag{1}.h5'.format(time_step, lag)\n",
    "        model = load_model(model_src, custom_objects={'mish':mish})\n",
    "        \n",
    "        X_data = data\n",
    "        Y_data = data[:,:,:,:,4] # 3 = PM10, 4 = PM2.5\n",
    "        Y_data = Y_data.reshape((8760*2),1,40,28,1)\n",
    "\n",
    "        X_data, Y_data = look_back(X_data, Y_data, time_step)\n",
    "        if lag > 1:\n",
    "            X_data = X_data[0:len(X_data)-(lag-1)]\n",
    "            Y_data = Y_data[(lag-1):]\n",
    "\n",
    "        X_train, X_val, X_test = division(X_data)\n",
    "        Y_train, Y_val, Y_test = division(Y_data)\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], 40, 28, 1).astype('float32')\n",
    "        Y_val = Y_val.reshape(Y_val.shape[0],  40, 28, 1).astype('float32')\n",
    "        Y_test = Y_test.reshape(Y_test.shape[0], 40, 28, 1).astype('float32')\n",
    "        \n",
    "        Y_pred_pm25 = model.predict(X_test).reshape(-1, 40, 28)\n",
    "        \n",
    "        np.save('./Result/PM2.5/ICNN-Dropout/Predict/timestep{0}_lag{1}'.format(time_step, lag), Y_pred_pm25)\n",
    "        \n",
    "        del model, X_data, Y_data, X_train, X_val, X_test, Y_train, Y_val, Y_test, Y_pred_pm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) ICNN : PM2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currnent time step:1, lag:1\n",
      "Train on 10511 samples, validate on 3504 samples\n",
      "Epoch 1/2000\n",
      "10511/10511 [==============================] - 52s 5ms/step - loss: 891.3173 - val_loss: 227.0962\n",
      "Epoch 2/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 847.7753 - val_loss: 166.5910\n",
      "Epoch 3/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 811.2432 - val_loss: 213.1153\n",
      "Epoch 4/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 773.1734 - val_loss: 268.1833\n",
      "Epoch 5/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 733.2273 - val_loss: 294.2687\n",
      "Epoch 6/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 691.7277 - val_loss: 282.2980\n",
      "Epoch 7/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 647.9768 - val_loss: 267.3409\n",
      "Epoch 8/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 600.5761 - val_loss: 243.1777\n",
      "Epoch 9/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 551.2388 - val_loss: 232.7332\n",
      "Epoch 10/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 502.3824 - val_loss: 228.4387\n",
      "Epoch 11/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 453.5894 - val_loss: 224.2946\n",
      "Epoch 12/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 407.1701 - val_loss: 196.4447\n",
      "Epoch 13/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 362.6658 - val_loss: 193.5711\n",
      "Epoch 14/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 320.3082 - val_loss: 182.3603\n",
      "Epoch 15/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 281.0666 - val_loss: 165.5134\n",
      "Epoch 16/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 245.4366 - val_loss: 166.4427\n",
      "Epoch 17/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 210.8465 - val_loss: 147.4292\n",
      "Epoch 18/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 181.4831 - val_loss: 137.8305\n",
      "Epoch 19/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 153.9300 - val_loss: 108.9526\n",
      "Epoch 20/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 130.9737 - val_loss: 110.1600\n",
      "Epoch 21/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 110.8489 - val_loss: 98.1635\n",
      "Epoch 22/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 92.7578 - val_loss: 84.6750\n",
      "Epoch 23/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 77.1736 - val_loss: 88.4897\n",
      "Epoch 24/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 64.5452 - val_loss: 83.4803\n",
      "Epoch 25/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 53.7578 - val_loss: 63.1149\n",
      "Epoch 26/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 45.3082 - val_loss: 61.1817\n",
      "Epoch 27/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 37.7136 - val_loss: 53.0616\n",
      "Epoch 28/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 31.0991 - val_loss: 47.8339\n",
      "Epoch 29/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 27.1348 - val_loss: 39.3474\n",
      "Epoch 30/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 22.4272 - val_loss: 33.2268\n",
      "Epoch 31/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 20.0581 - val_loss: 33.8690\n",
      "Epoch 32/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 18.4492 - val_loss: 28.4205\n",
      "Epoch 33/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 15.9666 - val_loss: 30.7220\n",
      "Epoch 34/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 14.0096 - val_loss: 24.8435\n",
      "Epoch 35/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 13.2363 - val_loss: 23.0085\n",
      "Epoch 36/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 13.5599 - val_loss: 23.5908\n",
      "Epoch 37/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 12.1771 - val_loss: 20.7336\n",
      "Epoch 38/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 11.0876 - val_loss: 15.3712\n",
      "Epoch 39/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 10.0727 - val_loss: 12.7436\n",
      "Epoch 40/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 9.8909 - val_loss: 13.7486\n",
      "Epoch 41/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 10.1688 - val_loss: 11.0371\n",
      "Epoch 42/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 9.2019 - val_loss: 11.5215\n",
      "Epoch 43/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 8.9785 - val_loss: 8.8418\n",
      "Epoch 44/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 9.0352 - val_loss: 8.3998\n",
      "Epoch 45/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 10.6186 - val_loss: 9.4792\n",
      "Epoch 46/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 10.0383 - val_loss: 7.6111\n",
      "Epoch 47/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 9.3352 - val_loss: 6.9107\n",
      "Epoch 48/2000\n",
      "10511/10511 [==============================] - 52s 5ms/step - loss: 8.5508 - val_loss: 7.9864\n",
      "Epoch 49/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 8.7276 - val_loss: 7.3106\n",
      "Epoch 50/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 8.6006 - val_loss: 7.3359\n",
      "Epoch 51/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 8.4691 - val_loss: 6.2640\n",
      "Epoch 52/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 8.1602 - val_loss: 6.2410\n",
      "Epoch 53/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 7.9191 - val_loss: 6.0529\n",
      "Epoch 54/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 8.4765 - val_loss: 5.5807\n",
      "Epoch 55/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 7.6303 - val_loss: 5.5365\n",
      "Epoch 56/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 7.3746 - val_loss: 5.8164\n",
      "Epoch 57/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 7.3674 - val_loss: 5.3862\n",
      "Epoch 58/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 7.4412 - val_loss: 5.7081\n",
      "Epoch 59/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 7.9042 - val_loss: 6.0851\n",
      "Epoch 60/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 7.7415 - val_loss: 5.0787\n",
      "Epoch 61/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 7.7901 - val_loss: 5.2565\n",
      "Epoch 62/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 8.0421 - val_loss: 5.3369\n",
      "Epoch 63/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 8.1620 - val_loss: 5.0641\n",
      "Epoch 64/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 8.3310 - val_loss: 5.2949\n",
      "Epoch 65/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 7.2549 - val_loss: 4.7141\n",
      "Epoch 66/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.8220 - val_loss: 4.8035\n",
      "Epoch 67/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 7.4316 - val_loss: 4.6205\n",
      "Epoch 68/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 7.1883 - val_loss: 5.0539\n",
      "Epoch 69/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.8163 - val_loss: 5.1052\n",
      "Epoch 70/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 7.3911 - val_loss: 4.8452\n",
      "Epoch 71/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.9880 - val_loss: 4.7434\n",
      "Epoch 72/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 7.3160 - val_loss: 4.7511\n",
      "Epoch 73/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.8765 - val_loss: 4.6124\n",
      "Epoch 74/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.8071 - val_loss: 4.6761\n",
      "Epoch 75/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10511/10511 [==============================] - 49s 5ms/step - loss: 7.1840 - val_loss: 4.9897\n",
      "Epoch 76/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.9238 - val_loss: 4.5662\n",
      "Epoch 77/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.9240 - val_loss: 4.7237\n",
      "Epoch 78/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.6184 - val_loss: 4.4303\n",
      "Epoch 79/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.5597 - val_loss: 4.4504\n",
      "Epoch 80/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.4865 - val_loss: 4.5492\n",
      "Epoch 81/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.1124 - val_loss: 4.2688\n",
      "Epoch 82/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.6597 - val_loss: 4.5125\n",
      "Epoch 83/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 6.8944 - val_loss: 4.6637\n",
      "Epoch 84/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.1886 - val_loss: 4.5656\n",
      "Epoch 85/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.5373 - val_loss: 4.2586\n",
      "Epoch 86/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.5593 - val_loss: 4.3000\n",
      "Epoch 87/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.1587 - val_loss: 4.2547\n",
      "Epoch 88/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.7652 - val_loss: 4.3447\n",
      "Epoch 89/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.7989 - val_loss: 4.2682\n",
      "Epoch 90/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.8068 - val_loss: 4.4596\n",
      "Epoch 91/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.6306 - val_loss: 4.6685\n",
      "Epoch 92/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.3052 - val_loss: 4.8043\n",
      "Epoch 93/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 6.3973 - val_loss: 4.2213\n",
      "Epoch 94/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 6.4571 - val_loss: 4.1661\n",
      "Epoch 95/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 7.0034 - val_loss: 4.1771\n",
      "Epoch 96/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 6.2947 - val_loss: 4.1393\n",
      "Epoch 97/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 6.7029 - val_loss: 4.2355\n",
      "Epoch 98/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 6.8753 - val_loss: 4.9359\n",
      "Epoch 99/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 6.8578 - val_loss: 5.1461\n",
      "Epoch 100/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 6.3113 - val_loss: 4.3757\n",
      "Epoch 101/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.0661 - val_loss: 5.4099\n",
      "Epoch 102/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.4327 - val_loss: 4.7982\n",
      "Epoch 103/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 6.2744 - val_loss: 4.1495\n",
      "Epoch 104/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 6.2711 - val_loss: 4.1329\n",
      "Epoch 105/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.2407 - val_loss: 4.2298\n",
      "Epoch 106/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.0936 - val_loss: 4.0761\n",
      "Epoch 107/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.1332 - val_loss: 4.3600\n",
      "Epoch 108/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 5.6806 - val_loss: 4.0266\n",
      "Epoch 109/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 5.9451 - val_loss: 4.3603\n",
      "Epoch 110/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 5.6430 - val_loss: 4.1348\n",
      "Epoch 111/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 5.9531 - val_loss: 5.2687\n",
      "Epoch 112/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.7149 - val_loss: 4.7547\n",
      "Epoch 113/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.0978 - val_loss: 4.1772\n",
      "Epoch 114/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 5.9544 - val_loss: 4.2298\n",
      "Epoch 115/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 6.4197 - val_loss: 4.3649\n",
      "Epoch 116/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.1687 - val_loss: 4.3646\n",
      "Epoch 117/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 6.1607 - val_loss: 4.3294\n",
      "Epoch 118/2000\n",
      "10511/10511 [==============================] - 52s 5ms/step - loss: 6.8373 - val_loss: 4.2561\n",
      "Epoch 119/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 6.4459 - val_loss: 4.3343\n",
      "Epoch 120/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 6.0437 - val_loss: 4.1917\n",
      "Epoch 121/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 5.7337 - val_loss: 4.2123\n",
      "Epoch 122/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 6.0921 - val_loss: 4.0673\n",
      "Epoch 123/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 6.0915 - val_loss: 4.0738\n",
      "Epoch 124/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 6.1922 - val_loss: 4.2117\n",
      "Epoch 125/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 6.8866 - val_loss: 4.2966\n",
      "Epoch 126/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 6.3427 - val_loss: 4.4037\n",
      "Epoch 127/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 5.8258 - val_loss: 3.9125\n",
      "Epoch 128/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.1408 - val_loss: 4.9419\n",
      "Epoch 129/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.3545 - val_loss: 4.2642\n",
      "Epoch 130/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.6380 - val_loss: 4.1656\n",
      "Epoch 131/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.7275 - val_loss: 3.9507\n",
      "Epoch 132/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 6.2711 - val_loss: 3.9843\n",
      "Epoch 133/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 5.9008 - val_loss: 4.2430\n",
      "Epoch 134/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.2288 - val_loss: 4.5657\n",
      "Epoch 135/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 6.3810 - val_loss: 6.1417\n",
      "Epoch 136/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.3430 - val_loss: 4.0812\n",
      "Epoch 137/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.6886 - val_loss: 6.6074\n",
      "Epoch 138/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.1599 - val_loss: 4.4507\n",
      "Epoch 139/2000\n",
      "10511/10511 [==============================] - 48s 5ms/step - loss: 6.2032 - val_loss: 4.6650\n",
      "Epoch 140/2000\n",
      "10511/10511 [==============================] - 49s 5ms/step - loss: 6.1751 - val_loss: 4.9914\n",
      "Epoch 141/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 5.9195 - val_loss: 4.1929\n",
      "Epoch 142/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 5.8330 - val_loss: 4.0853\n",
      "Epoch 143/2000\n",
      "10511/10511 [==============================] - 50s 5ms/step - loss: 6.5489 - val_loss: 4.1084\n",
      "Epoch 144/2000\n",
      "10511/10511 [==============================] - 52s 5ms/step - loss: 5.9653 - val_loss: 4.5014\n",
      "Epoch 145/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 5.8073 - val_loss: 4.9183\n",
      "Epoch 146/2000\n",
      "10511/10511 [==============================] - 51s 5ms/step - loss: 6.2332 - val_loss: 4.3444\n",
      "Epoch 147/2000\n",
      "10511/10511 [==============================] - 52s 5ms/step - loss: 6.6177 - val_loss: 4.0979\n",
      "Currnent time step:1, lag:2\n",
      "Train on 10510 samples, validate on 3504 samples\n",
      "Epoch 1/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 869.7475 - val_loss: 203.4827\n",
      "Epoch 2/2000\n",
      "10510/10510 [==============================] - 50s 5ms/step - loss: 824.7626 - val_loss: 169.9818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 781.2723 - val_loss: 196.4615\n",
      "Epoch 4/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 724.8336 - val_loss: 199.0042\n",
      "Epoch 5/2000\n",
      "10510/10510 [==============================] - 50s 5ms/step - loss: 669.0435 - val_loss: 189.7806\n",
      "Epoch 6/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 614.1861 - val_loss: 185.2826\n",
      "Epoch 7/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 559.8756 - val_loss: 194.9053\n",
      "Epoch 8/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 506.5583 - val_loss: 204.4629\n",
      "Epoch 9/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 455.2544 - val_loss: 208.3913\n",
      "Epoch 10/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 405.0406 - val_loss: 196.4585\n",
      "Epoch 11/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 357.8384 - val_loss: 219.5232\n",
      "Epoch 12/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 314.9834 - val_loss: 217.1742\n",
      "Epoch 13/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 274.2008 - val_loss: 201.8478\n",
      "Epoch 14/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 238.0203 - val_loss: 191.3391\n",
      "Epoch 15/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 202.5692 - val_loss: 196.9719\n",
      "Epoch 16/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 172.6041 - val_loss: 195.9845\n",
      "Epoch 17/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 146.6250 - val_loss: 185.4881\n",
      "Epoch 18/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 121.1942 - val_loss: 152.7843\n",
      "Epoch 19/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 100.0627 - val_loss: 123.5848\n",
      "Epoch 20/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 81.9530 - val_loss: 111.5684\n",
      "Epoch 21/2000\n",
      "10510/10510 [==============================] - 53s 5ms/step - loss: 67.7884 - val_loss: 103.5884\n",
      "Epoch 22/2000\n",
      "10510/10510 [==============================] - 50s 5ms/step - loss: 57.0910 - val_loss: 94.0206\n",
      "Epoch 23/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 47.4692 - val_loss: 85.4503\n",
      "Epoch 24/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 41.0724 - val_loss: 71.2558\n",
      "Epoch 25/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 36.6394 - val_loss: 53.8061\n",
      "Epoch 26/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 31.6699 - val_loss: 52.7441\n",
      "Epoch 27/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 28.4422 - val_loss: 43.2412\n",
      "Epoch 28/2000\n",
      "10510/10510 [==============================] - 50s 5ms/step - loss: 26.5741 - val_loss: 36.7951\n",
      "Epoch 29/2000\n",
      "10510/10510 [==============================] - 50s 5ms/step - loss: 25.2918 - val_loss: 34.1829\n",
      "Epoch 30/2000\n",
      "10510/10510 [==============================] - 50s 5ms/step - loss: 22.7295 - val_loss: 26.0504\n",
      "Epoch 31/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 22.2037 - val_loss: 28.2545\n",
      "Epoch 32/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 21.7179 - val_loss: 25.7256\n",
      "Epoch 33/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 20.2796 - val_loss: 20.4680\n",
      "Epoch 34/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 19.9155 - val_loss: 20.6294\n",
      "Epoch 35/2000\n",
      "10510/10510 [==============================] - 50s 5ms/step - loss: 19.0321 - val_loss: 18.2348\n",
      "Epoch 36/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 18.9532 - val_loss: 17.0538\n",
      "Epoch 37/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 18.3179 - val_loss: 16.4539\n",
      "Epoch 38/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 19.1996 - val_loss: 16.4089\n",
      "Epoch 39/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 17.7137 - val_loss: 14.4397\n",
      "Epoch 40/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 17.7206 - val_loss: 15.0360\n",
      "Epoch 41/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 18.4146 - val_loss: 13.8961\n",
      "Epoch 42/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 17.7836 - val_loss: 14.3868\n",
      "Epoch 43/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 17.4849 - val_loss: 14.3770\n",
      "Epoch 44/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 17.1144 - val_loss: 12.9603\n",
      "Epoch 45/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 16.5473 - val_loss: 12.5282\n",
      "Epoch 46/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 17.9288 - val_loss: 11.4540\n",
      "Epoch 47/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 17.6674 - val_loss: 12.5579\n",
      "Epoch 48/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 17.2706 - val_loss: 11.9938\n",
      "Epoch 49/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 17.3078 - val_loss: 11.6945\n",
      "Epoch 50/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 16.9603 - val_loss: 11.3875\n",
      "Epoch 51/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 17.2485 - val_loss: 11.6129\n",
      "Epoch 52/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 16.1670 - val_loss: 11.1876\n",
      "Epoch 53/2000\n",
      "10510/10510 [==============================] - 53s 5ms/step - loss: 16.7491 - val_loss: 12.2546\n",
      "Epoch 54/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 15.8590 - val_loss: 11.5084\n",
      "Epoch 55/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 15.7359 - val_loss: 13.5945\n",
      "Epoch 56/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 16.5099 - val_loss: 14.9114\n",
      "Epoch 57/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 16.6344 - val_loss: 13.9991\n",
      "Epoch 58/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 16.9599 - val_loss: 12.8768\n",
      "Epoch 59/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 17.2321 - val_loss: 11.4576\n",
      "Epoch 60/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 16.1083 - val_loss: 10.7641\n",
      "Epoch 61/2000\n",
      "10510/10510 [==============================] - 50s 5ms/step - loss: 16.5019 - val_loss: 12.1315\n",
      "Epoch 62/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 16.1676 - val_loss: 11.3626\n",
      "Epoch 63/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 16.2064 - val_loss: 11.1782\n",
      "Epoch 64/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 17.1496 - val_loss: 11.3711\n",
      "Epoch 65/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 16.8473 - val_loss: 12.5691\n",
      "Epoch 66/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 16.0608 - val_loss: 10.8217\n",
      "Epoch 67/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 16.0375 - val_loss: 10.8106\n",
      "Epoch 68/2000\n",
      "10510/10510 [==============================] - 50s 5ms/step - loss: 16.2429 - val_loss: 12.4073\n",
      "Epoch 69/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 16.4330 - val_loss: 10.9644\n",
      "Epoch 70/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 15.7132 - val_loss: 12.0043\n",
      "Epoch 71/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 15.1429 - val_loss: 10.6036\n",
      "Epoch 72/2000\n",
      "10510/10510 [==============================] - 50s 5ms/step - loss: 15.2567 - val_loss: 10.7594\n",
      "Epoch 73/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 15.2692 - val_loss: 10.7123\n",
      "Epoch 74/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 15.1921 - val_loss: 10.7856\n",
      "Epoch 75/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 15.5120 - val_loss: 10.7377\n",
      "Epoch 76/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 15.5566 - val_loss: 11.2297\n",
      "Epoch 77/2000\n",
      "10510/10510 [==============================] - 50s 5ms/step - loss: 15.5637 - val_loss: 10.9064\n",
      "Epoch 78/2000\n",
      "10510/10510 [==============================] - 50s 5ms/step - loss: 15.5766 - val_loss: 10.3793\n",
      "Epoch 79/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 14.8720 - val_loss: 11.4751\n",
      "Epoch 80/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 15.6861 - val_loss: 10.6493\n",
      "Epoch 81/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 15.6503 - val_loss: 13.7317\n",
      "Epoch 82/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 15.7415 - val_loss: 11.7752\n",
      "Epoch 83/2000\n",
      "10510/10510 [==============================] - 50s 5ms/step - loss: 15.8170 - val_loss: 10.3231\n",
      "Epoch 84/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 16.5490 - val_loss: 11.7558\n",
      "Epoch 85/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 15.3986 - val_loss: 11.6178\n",
      "Epoch 86/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 15.0612 - val_loss: 10.1147\n",
      "Epoch 87/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 14.5499 - val_loss: 10.3607\n",
      "Epoch 88/2000\n",
      "10510/10510 [==============================] - 50s 5ms/step - loss: 15.0534 - val_loss: 10.5084\n",
      "Epoch 89/2000\n",
      "10510/10510 [==============================] - 48s 5ms/step - loss: 15.4061 - val_loss: 12.4921\n",
      "Epoch 90/2000\n",
      "10510/10510 [==============================] - 49s 5ms/step - loss: 14.5260 - val_loss: 10.2387\n",
      "Epoch 91/2000\n",
      "10510/10510 [==============================] - 49s 5ms/step - loss: 14.6257 - val_loss: 10.4084\n",
      "Epoch 92/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 14.4853 - val_loss: 10.6913\n",
      "Epoch 93/2000\n",
      "10510/10510 [==============================] - 52s 5ms/step - loss: 14.6403 - val_loss: 10.3479\n",
      "Epoch 94/2000\n",
      "10510/10510 [==============================] - 51s 5ms/step - loss: 15.1330 - val_loss: 10.1938\n",
      "Epoch 95/2000\n",
      "10510/10510 [==============================] - 49s 5ms/step - loss: 14.4737 - val_loss: 10.9810\n",
      "Epoch 96/2000\n",
      "10510/10510 [==============================] - 49s 5ms/step - loss: 15.0077 - val_loss: 11.0201\n",
      "Epoch 97/2000\n",
      " 7680/10510 [====================>.........] - ETA: 11s - loss: 14.7046"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f3367d4b270d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf115/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf115/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf115/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/tf115/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for time_step in [1, 12, 24]:\n",
    "    for lag in [1, 2, 4, 6, 12, 24]:\n",
    "        print('Currnent time step:{0}, lag:{1}'.format(str(time_step), str(lag)))\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, mode='min')\n",
    "        mc = ModelCheckpoint('./Result/PM2.5/ICNN/Model/timestep{0}_lag{1}.h5'.format(str(time_step), str(lag)), monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "        X_data = data\n",
    "        Y_data = data[:,:,:,:,4] # 3 = PM10, 4 = PM2.5\n",
    "        Y_data = Y_data.reshape((8760*2),1,40,28,1)\n",
    "\n",
    "        X_data, Y_data = look_back(X_data, Y_data, time_step)\n",
    "        if lag > 1:\n",
    "            X_data = X_data[0:len(X_data)-(lag-1)]\n",
    "            Y_data = Y_data[(lag-1):]\n",
    "\n",
    "        X_train, X_val, X_test = division(X_data)\n",
    "        Y_train, Y_val, Y_test = division(Y_data)\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], 40, 28, 1).astype('float32')\n",
    "        Y_val = Y_val.reshape(Y_val.shape[0],  40, 28, 1).astype('float32')\n",
    "        Y_test = Y_test.reshape(Y_test.shape[0], 40, 28, 1).astype('float32')\n",
    "        \n",
    "        model = ICNN(time_step)\n",
    "\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        model.fit(X_train, Y_train, epochs=2000, batch_size=512, validation_data=(X_val, Y_val), callbacks=[early_stopping, mc])\n",
    "\n",
    "        del model, X_data, Y_data, X_train, X_val, X_test, Y_train, Y_val, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_step in [1, 12, 24]:\n",
    "    for lag in [1, 2, 4, 6, 12, 24]:\n",
    "        model_src = './Result/PM2.5/ICNN/Model/timestep{0}_lag{1}.h5'.format(time_step, lag)\n",
    "        model = load_model(model_src, custom_objects={'mish':mish})\n",
    "        \n",
    "        X_data = data\n",
    "        Y_data = data[:,:,:,:,4] # 3 = PM10, 4 = PM2.5\n",
    "        Y_data = Y_data.reshape((8760*2),1,40,28,1)\n",
    "\n",
    "        X_data, Y_data = look_back(X_data, Y_data, time_step)\n",
    "        if lag > 1:\n",
    "            X_data = X_data[0:len(X_data)-(lag-1)]\n",
    "            Y_data = Y_data[(lag-1):]\n",
    "\n",
    "        X_train, X_val, X_test = division(X_data)\n",
    "        Y_train, Y_val, Y_test = division(Y_data)\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], 40, 28, 1).astype('float32')\n",
    "        Y_val = Y_val.reshape(Y_val.shape[0],  40, 28, 1).astype('float32')\n",
    "        Y_test = Y_test.reshape(Y_test.shape[0], 40, 28, 1).astype('float32')\n",
    "        \n",
    "        Y_pred_pm25 = model.predict(X_test).reshape(-1, 40, 28)\n",
    "        \n",
    "        np.save('./Result/PM2.5/ICNN/Predict/timestep{0}_lag{1}'.format(time_step, lag), Y_pred_pm25)\n",
    "        \n",
    "        del model, X_data, Y_data, X_train, X_val, X_test, Y_train, Y_val, Y_test, Y_pred_pm25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf115] *",
   "language": "python",
   "name": "conda-env-tf115-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
